# Day 3: å¼ é‡æ“ä½œä¸è®¡ç®—å›¾

## è¯¾ç¨‹ç›®æ ‡

ä»Šå¤©æˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ ï¼š
- GGML çš„æ ¸å¿ƒå¼ é‡æ“ä½œ
- è®¡ç®—å›¾ï¼ˆComputation Graphï¼‰çš„æ¦‚å¿µä¸å®ç°
- å¦‚ä½•æ„å»ºå’Œæ‰§è¡Œè®¡ç®—å›¾
- å¸¸è§å¼ é‡æ“ä½œçš„å®ç°ç»†èŠ‚

## 1. æ ¸å¿ƒå¼ é‡æ“ä½œ

### 1.1 å¼ é‡åˆ›å»ºæ“ä½œ

GGML æä¾›äº†ä¸°å¯Œçš„å¼ é‡åˆ›å»ºå‡½æ•°ï¼Œä½äº `ggml/src/ggml.c`ï¼š

```c
// åˆ›å»º1Då¼ é‡
struct ggml_tensor * ggml_new_tensor_1d(
    struct ggml_context * ctx,
    enum ggml_type type,
    int64_t ne0);  // å…ƒç´ æ•°é‡

// åˆ›å»º2Då¼ é‡ï¼ˆæœ€å¸¸ç”¨ï¼šçŸ©é˜µï¼‰
struct ggml_tensor * ggml_new_tensor_2d(
    struct ggml_context * ctx,
    enum ggml_type type,
    int64_t ne0,   // åˆ—æ•°ï¼ˆç¬¬ä¸€ç»´ï¼‰
    int64_t ne1);  // è¡Œæ•°ï¼ˆç¬¬äºŒç»´ï¼‰

// åˆ›å»º3Då¼ é‡
struct ggml_tensor * ggml_new_tensor_3d(
    struct ggml_context * ctx,
    enum ggml_type type,
    int64_t ne0, int64_t ne1, int64_t ne2);

// åˆ›å»º4Då¼ é‡
struct ggml_tensor * ggml_new_tensor_4d(
    struct ggml_context * ctx,
    enum ggml_type type,
    int64_t ne0, int64_t ne1, int64_t ne2, int64_t ne3);
```

**é‡è¦æ¦‚å¿µ**ï¼šGGML ä½¿ç”¨è¡Œä¸»åºï¼ˆrow-majorï¼‰å­˜å‚¨ï¼š
- `ne[0]` = åˆ—æ•°ï¼ˆæœ€å¿«å˜åŒ–ç»´åº¦ï¼‰
- `ne[1]` = è¡Œæ•°
- `ne[2]` = æ·±åº¦
- `ne[3]` = æ‰¹æ¬¡

### 1.2 åŸºç¡€ç®—æœ¯æ“ä½œ

#### åŠ æ³•æ“ä½œ

```c
// ä½ç½®ï¼šggml/src/ggml.c:4523
struct ggml_tensor * ggml_add(
    struct ggml_context * ctx,
    struct ggml_tensor * a,
    struct ggml_tensor * b) {
    // åˆ›å»ºç»“æœå¼ é‡
    struct ggml_tensor * result = ggml_dup_tensor(ctx, a);
    result->op = GGML_OP_ADD;
    result->src[0] = a;
    result->src[1] = b;
    return result;
}

// å¯¹åº”çš„å‰å‘è®¡ç®—å‡½æ•°
// ä½ç½®ï¼šggml/src/ggml.c:10234
static void ggml_compute_forward_add_f32(
    const struct ggml_compute_params * params,
    struct ggml_tensor * dst) {

    const struct ggml_tensor * src0 = dst->src[0];
    const struct ggml_tensor * src1 = dst->src[1];

    // è·å–ç»´åº¦
    const int64_t ne00 = src0->ne[0];
    const int64_t ne01 = src0->ne[1];
    // ... å…¶ä»–ç»´åº¦

    // æ‰§è¡Œé€å…ƒç´ åŠ æ³•
    for (int64_t i03 = 0; i03 < ne03; i03++) {
        for (int64_t i02 = 0; i02 < ne02; i02++) {
            for (int64_t i01 = 0; i01 < ne01; i01++) {
                // ä½¿ç”¨SIMDä¼˜åŒ–çš„å‘é‡åŠ æ³•
                ggml_vec_add_f32(ne00,
                    (float *) dst_ptr,
                    (float *) src0_ptr,
                    (float *) src1_ptr);
            }
        }
    }
}
```

#### ä¹˜æ³•æ“ä½œ

```c
// é€å…ƒç´ ä¹˜æ³•
struct ggml_tensor * ggml_mul(
    struct ggml_context * ctx,
    struct ggml_tensor * a,
    struct ggml_tensor * b);

// æ ‡é‡ä¹˜æ³•
struct ggml_tensor * ggml_scale(
    struct ggml_context * ctx,
    struct ggml_tensor * a,
    float s);
```

### 1.3 çŸ©é˜µä¹˜æ³• - æœ€å…³é”®çš„æ“ä½œ

çŸ©é˜µä¹˜æ³•æ˜¯ LLM æ¨ç†ä¸­æœ€è€—æ—¶çš„æ“ä½œï¼Œå æ€»è®¡ç®—é‡çš„ 80%+ã€‚

```c
// ä½ç½®ï¼šggml/src/ggml.c:5247
// C = ggml_mul_mat(ctx, A, B) è®¡ç®—: C^T = AB^T âŸº C = BA^T
struct ggml_tensor * ggml_mul_mat(
    struct ggml_context * ctx,
    struct ggml_tensor * a,    // [K, M] æƒé‡çŸ©é˜µ
    struct ggml_tensor * b) {  // [K, N] è¾“å…¥çŸ©é˜µ

    // æ–­è¨€ç»´åº¦å…¼å®¹
    GGML_ASSERT(a->ne[0] == b->ne[0]);  // K ç»´åº¦å¿…é¡»åŒ¹é…

    // åˆ›å»ºè¾“å‡ºå¼ é‡ [M, N]
    struct ggml_tensor * result = ggml_new_tensor_2d(
        ctx, GGML_TYPE_F32,
        a->ne[1],  // M (Açš„åˆ—æ•°)
        b->ne[1]); // N (Bçš„åˆ—æ•°)

    result->op = GGML_OP_MUL_MAT;
    result->src[0] = a;
    result->src[1] = b;

    return result;
}
```

**çŸ©é˜µä¹˜æ³•çš„ä¼˜åŒ–**ï¼š
- CPUï¼šä½¿ç”¨ AVX2/NEON SIMD æŒ‡ä»¤
- GPUï¼šä½¿ç”¨ cuBLASï¼ˆCUDAï¼‰æˆ– Metal Performance Shaders
- é‡åŒ–ï¼šç‰¹æ®Šçš„é‡åŒ–çŸ©é˜µä¹˜æ³•å†…æ ¸

#### é‡åŒ–çŸ©é˜µä¹˜æ³•ç¤ºä¾‹ï¼ˆCUDAï¼‰

```cuda
// ä½ç½®ï¼šggml/src/ggml-cuda/mmq.cuh
// é’ˆå¯¹ Q4_K é‡åŒ–çš„ä¼˜åŒ–çŸ©é˜µä¹˜æ³•
template<int qk, int qr, dequantize_kernel_t dequantize_kernel, int ncols>
static __global__ void dequantize_mul_mat_vec_q4_k(
    const void * __restrict__ vx,
    const float * __restrict__ y,
    float * __restrict__ dst,
    const int ncols_x, const int nrows_x) {

    // æ¯ä¸ª warp å¤„ç†ä¸€è¡Œ
    const int row = blockIdx.x;

    // åŠ è½½é‡åŒ–æ•°æ®åˆ°å…±äº«å†…å­˜
    __shared__ float tmp[WARP_SIZE];

    // åé‡åŒ–å¹¶æ‰§è¡Œç‚¹ç§¯
    float sum = 0.0f;
    for (int i = threadIdx.x; i < ncols_x; i += WARP_SIZE) {
        const float xi = dequantize_q4_k(vx, i);  // åé‡åŒ–
        sum += xi * y[i];  // ç‚¹ç§¯ç´¯åŠ 
    }

    // Warp å†…å½’çº¦
    sum = warp_reduce_sum(sum);

    if (threadIdx.x == 0) {
        dst[row] = sum;
    }
}
```

### 1.4 RoPE ä½ç½®ç¼–ç 

RoPEï¼ˆRotary Position Embeddingï¼‰æ˜¯ç°ä»£ LLM çš„æ ¸å¿ƒæŠ€æœ¯ã€‚

```c
// ä½ç½®ï¼šggml/src/ggml.c:5819
struct ggml_tensor * ggml_rope(
    struct ggml_context * ctx,
    struct ggml_tensor * a,     // è¾“å…¥å¼ é‡ [n_embd, n_tokens, ...]
    struct ggml_tensor * b,     // ä½ç½®ç´¢å¼• [n_tokens]
    int n_dims,                 // RoPE ç»´åº¦
    int mode,                   // RoPE æ¨¡å¼ï¼ˆNORM/NEOX/...ï¼‰
    int n_ctx) {                // ä¸Šä¸‹æ–‡é•¿åº¦

    struct ggml_tensor * result = ggml_dup_tensor(ctx, a);

    // è®¾ç½®æ“ä½œå‚æ•°
    int32_t params[11] = {
        /*n_dims=*/ n_dims,
        /*mode=*/   mode,
        /*n_ctx=*/  n_ctx,
        // ... å…¶ä»–å‚æ•°
    };
    ggml_set_op_params(result, params, sizeof(params));

    result->op = GGML_OP_ROPE;
    result->src[0] = a;
    result->src[1] = b;

    return result;
}
```

**RoPE å‰å‘è®¡ç®—**ï¼ˆä½ç½®ï¼š`ggml/src/ggml.c:12456`ï¼‰ï¼š

```c
static void ggml_compute_forward_rope_f32(
    const struct ggml_compute_params * params,
    struct ggml_tensor * dst) {

    const struct ggml_tensor * src0 = dst->src[0];  // è¾“å…¥
    const struct ggml_tensor * src1 = dst->src[1];  // ä½ç½®

    // è·å– RoPE å‚æ•°
    const int n_dims     = ((int32_t *) dst->op_params)[0];
    const int mode       = ((int32_t *) dst->op_params)[1];
    const float freq_base = ((float *) dst->op_params)[4];

    // å¯¹æ¯ä¸ª token åº”ç”¨æ—‹è½¬
    for (int64_t i3 = 0; i3 < ne3; i3++) {
        for (int64_t i2 = 0; i2 < ne2; i2++) {
            const int64_t p = positions[i2];  // å½“å‰ä½ç½®

            for (int64_t i1 = 0; i1 < ne1; i1++) {
                // å¯¹æ¯å¯¹ç»´åº¦åº”ç”¨æ—‹è½¬
                for (int64_t i0 = 0; i0 < n_dims; i0 += 2) {
                    // è®¡ç®—æ—‹è½¬è§’åº¦
                    const float theta = powf(freq_base, -(float)i0 / n_dims);
                    const float cos_theta = cosf(p * theta);
                    const float sin_theta = sinf(p * theta);

                    // æ—‹è½¬æ“ä½œ
                    const float x0 = src[i0];
                    const float x1 = src[i0 + 1];

                    dst[i0]     = x0 * cos_theta - x1 * sin_theta;
                    dst[i0 + 1] = x0 * sin_theta + x1 * cos_theta;
                }
            }
        }
    }
}
```

### 1.5 å…¶ä»–é‡è¦æ“ä½œ

#### å½’ä¸€åŒ–æ“ä½œ

```c
// RMS Normï¼ˆLLaMAä½¿ç”¨ï¼‰
struct ggml_tensor * ggml_rms_norm(
    struct ggml_context * ctx,
    struct ggml_tensor * a,
    float eps);

// Layer Normï¼ˆGPTä½¿ç”¨ï¼‰
struct ggml_tensor * ggml_norm(
    struct ggml_context * ctx,
    struct ggml_tensor * a,
    float eps);
```

#### æ¿€æ´»å‡½æ•°

```c
// SiLU / Swish (LLaMA FFN)
struct ggml_tensor * ggml_silu(
    struct ggml_context * ctx,
    struct ggml_tensor * a);

// GELU (GPT FFN)
struct ggml_tensor * ggml_gelu(
    struct ggml_context * ctx,
    struct ggml_tensor * a);

// ReLU
struct ggml_tensor * ggml_relu(
    struct ggml_context * ctx,
    struct ggml_tensor * a);
```

#### æ³¨æ„åŠ›ç›¸å…³æ“ä½œ

```c
// Softmax
struct ggml_tensor * ggml_soft_max(
    struct ggml_context * ctx,
    struct ggml_tensor * a);

// Flash Attentionï¼ˆä¼˜åŒ–çš„æ³¨æ„åŠ›ï¼‰
struct ggml_tensor * ggml_flash_attn_ext(
    struct ggml_context * ctx,
    struct ggml_tensor * q,
    struct ggml_tensor * k,
    struct ggml_tensor * v,
    struct ggml_tensor * mask,
    float scale);
```

## 2. è®¡ç®—å›¾ï¼ˆComputation Graphï¼‰

### 2.1 è®¡ç®—å›¾æ¦‚å¿µ

è®¡ç®—å›¾æ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œè¡¨ç¤ºå¼ é‡æ“ä½œçš„ä¾èµ–å…³ç³»ï¼š
- **èŠ‚ç‚¹**ï¼šå¼ é‡ï¼ˆå˜é‡ï¼‰
- **è¾¹**ï¼šæ“ä½œï¼ˆå‡½æ•°ï¼‰

```
ç¤ºä¾‹ï¼šf(x) = Ïƒ(Wx + b)

      [x]  [W]  [b]
       â”‚    â”‚    â”‚
       â””â”€â†’[mul]â†â”€â”˜
           â”‚
         [add]
           â”‚
        [sigmoid]
           â”‚
          [y]
```

### 2.2 è®¡ç®—å›¾ç»“æ„

```c
// ä½ç½®ï¼šggml/include/ggml.h:625
struct ggml_cgraph {
    int size;          // æœ€å¤§èŠ‚ç‚¹æ•°
    int n_nodes;       // å½“å‰èŠ‚ç‚¹æ•°
    int n_leafs;       // å¶å­èŠ‚ç‚¹æ•°

    struct ggml_tensor ** nodes;   // æ‰€æœ‰æ“ä½œèŠ‚ç‚¹
    struct ggml_tensor ** grads;   // æ¢¯åº¦ï¼ˆåå‘ä¼ æ’­ç”¨ï¼‰
    struct ggml_tensor ** leafs;   // è¾“å…¥èŠ‚ç‚¹

    struct ggml_hash_set visited_hash_set;  // å·²è®¿é—®èŠ‚ç‚¹

    // æ‰§è¡Œé¡ºåºï¼ˆæ‹“æ‰‘æ’åºåï¼‰
    enum ggml_cgraph_eval_order order;
};
```

### 2.3 æ„å»ºè®¡ç®—å›¾

```c
// åˆ›å»ºè®¡ç®—å›¾
struct ggml_cgraph * ggml_new_graph(struct ggml_context * ctx) {
    return ggml_new_graph_custom(ctx, GGML_DEFAULT_GRAPH_SIZE, false);
}

// æ·»åŠ èŠ‚ç‚¹åˆ°å›¾
void ggml_build_forward_expand(
    struct ggml_cgraph * cgraph,
    struct ggml_tensor * tensor) {

    // é€’å½’æ·»åŠ æ‰€æœ‰ä¾èµ–èŠ‚ç‚¹
    if (tensor->op != GGML_OP_NONE) {
        // å…ˆæ·»åŠ æºèŠ‚ç‚¹
        for (int i = 0; i < GGML_MAX_SRC; i++) {
            if (tensor->src[i]) {
                ggml_build_forward_expand(cgraph, tensor->src[i]);
            }
        }
    }

    // æ·»åŠ å½“å‰èŠ‚ç‚¹
    if (!ggml_hash_contains(&cgraph->visited_hash_set, tensor)) {
        ggml_hash_insert(&cgraph->visited_hash_set, tensor);
        cgraph->nodes[cgraph->n_nodes++] = tensor;
    }
}
```

### 2.4 æ‰§è¡Œè®¡ç®—å›¾

```c
// ä½ç½®ï¼šggml/src/ggml-backend.cpp:823
enum ggml_status ggml_backend_graph_compute(
    ggml_backend_t backend,
    struct ggml_cgraph * cgraph) {

    // ä¸ºæ¯ä¸ªèŠ‚ç‚¹åˆ†é…åç«¯èµ„æº
    for (int i = 0; i < cgraph->n_nodes; i++) {
        struct ggml_tensor * node = cgraph->nodes[i];

        // åˆ†é…è¾“å‡ºç¼“å†²åŒº
        if (!node->data) {
            ggml_backend_tensor_alloc(backend, node);
        }
    }

    // æŒ‰é¡ºåºæ‰§è¡Œæ¯ä¸ªæ“ä½œ
    for (int i = 0; i < cgraph->n_nodes; i++) {
        struct ggml_tensor * node = cgraph->nodes[i];

        // æ ¹æ®æ“ä½œç±»å‹è°ƒç”¨å¯¹åº”çš„è®¡ç®—å‡½æ•°
        switch (node->op) {
            case GGML_OP_ADD:
                ggml_compute_forward_add(params, node);
                break;
            case GGML_OP_MUL_MAT:
                ggml_compute_forward_mul_mat(params, node);
                break;
            case GGML_OP_ROPE:
                ggml_compute_forward_rope(params, node);
                break;
            // ... 200+ ç§æ“ä½œ
        }
    }

    return GGML_STATUS_SUCCESS;
}
```

## 3. å®æˆ˜ç¤ºä¾‹ï¼šæ„å»ºç®€å•çš„ FFN

è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªå‰é¦ˆç¥ç»ç½‘ç»œå±‚ï¼š`y = GELU(xW1) W2`

```c
#include "ggml.h"
#include <stdio.h>

int main() {
    // 1. åˆå§‹åŒ–ä¸Šä¸‹æ–‡
    struct ggml_init_params params = {
        .mem_size   = 128*1024*1024,  // 128MB
        .mem_buffer = NULL,
        .no_alloc   = false,
    };
    struct ggml_context * ctx = ggml_init(params);

    // 2. åˆ›å»ºè¾“å…¥å¼ é‡ [batch_size=1, d_model=512]
    struct ggml_tensor * x = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, 512, 1);
    ggml_set_name(x, "input");

    // 3. åˆ›å»ºæƒé‡çŸ©é˜µ
    // W1: [d_model=512, d_ff=2048]
    struct ggml_tensor * w1 = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, 512, 2048);
    ggml_set_name(w1, "ffn.w1");

    // W2: [d_ff=2048, d_model=512]
    struct ggml_tensor * w2 = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, 2048, 512);
    ggml_set_name(w2, "ffn.w2");

    // 4. æ„å»ºè®¡ç®—å›¾
    // hidden = x @ W1
    struct ggml_tensor * hidden = ggml_mul_mat(ctx, w1, x);
    ggml_set_name(hidden, "hidden");

    // activated = GELU(hidden)
    struct ggml_tensor * activated = ggml_gelu(ctx, hidden);
    ggml_set_name(activated, "activated");

    // output = activated @ W2
    struct ggml_tensor * output = ggml_mul_mat(ctx, w2, activated);
    ggml_set_name(output, "output");

    // 5. åˆ›å»ºè®¡ç®—å›¾
    struct ggml_cgraph * gf = ggml_new_graph(ctx);
    ggml_build_forward_expand(gf, output);

    // 6. åˆå§‹åŒ–è¾“å…¥æ•°æ®ï¼ˆç¤ºä¾‹ï¼‰
    float * x_data = (float *) x->data;
    for (int i = 0; i < 512; i++) {
        x_data[i] = ((float)rand() / RAND_MAX) * 2.0f - 1.0f;
    }

    // 7. æ‰§è¡Œè®¡ç®—å›¾
    int n_threads = 4;
    ggml_graph_compute_with_ctx(ctx, gf, n_threads);

    // 8. è·å–ç»“æœ
    float * output_data = (float *) output->data;
    printf("Output[0] = %f\n", output_data[0]);

    // 9. æ¸…ç†
    ggml_free(ctx);

    return 0;
}
```

ç¼–è¯‘è¿è¡Œï¼š
```bash
gcc -O3 -o ffn_example ffn_example.c \
    -I./ggml/include \
    -L./build/ggml/src -lggml \
    -lm -lpthread

./ffn_example
```

## 4. è®¡ç®—å›¾å¯è§†åŒ–

GGML æä¾›äº†è®¡ç®—å›¾å¯¼å‡ºåŠŸèƒ½ï¼š

```c
// å¯¼å‡ºä¸º DOT æ ¼å¼ï¼ˆGraphvizï¼‰
ggml_graph_dump_dot(gf, NULL, "ffn_graph.dot");
```

ç”Ÿæˆå¯è§†åŒ–ï¼š
```bash
dot -Tpng ffn_graph.dot -o ffn_graph.png
```

## 5. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 5.1 æ“ä½œèåˆ

å°†å¤šä¸ªæ“ä½œèåˆä¸ºä¸€ä¸ªï¼Œå‡å°‘å†…å­˜è®¿é—®ï¼š

```c
// ä¸èåˆï¼š3æ¬¡å†…å­˜è¯»å†™
y = ggml_add(ctx, x, bias);    // è¯»x, å†™y
y = ggml_mul(ctx, y, scale);   // è¯»y, å†™y
y = ggml_relu(ctx, y);         // è¯»y, å†™y

// èåˆï¼š1æ¬¡å†…å­˜è¯»å†™
y = ggml_add_mul_relu(ctx, x, bias, scale);  // ä¸€æ¬¡å®Œæˆ
```

### 5.2 In-place æ“ä½œ

æŸäº›æ“ä½œå¯ä»¥åŸåœ°ä¿®æ”¹ï¼ŒèŠ‚çœå†…å­˜ï¼š

```c
// åŸåœ° ReLU
struct ggml_tensor * ggml_relu_inplace(
    struct ggml_context * ctx,
    struct ggml_tensor * a);

// åŸåœ° Scale
struct ggml_tensor * ggml_scale_inplace(
    struct ggml_context * ctx,
    struct ggml_tensor * a,
    float s);
```

### 5.3 ä½¿ç”¨åˆé€‚çš„æ•°æ®ç±»å‹

```c
// å¯¹äºæƒé‡ï¼Œä½¿ç”¨é‡åŒ–ç±»å‹
struct ggml_tensor * w = ggml_new_tensor_2d(ctx, GGML_TYPE_Q4_K, 512, 2048);

// å¯¹äºæ¿€æ´»å€¼ï¼Œä½¿ç”¨ FP16
struct ggml_tensor * hidden = ggml_new_tensor_2d(ctx, GGML_TYPE_F16, 2048, 1);
```

## 6. è°ƒè¯•æŠ€å·§

### 6.1 æ‰“å°å¼ é‡ä¿¡æ¯

```c
void print_tensor_info(struct ggml_tensor * t) {
    printf("Tensor: %s\n", t->name);
    printf("  Type: %s\n", ggml_type_name(t->type));
    printf("  Shape: [%ld, %ld, %ld, %ld]\n",
           t->ne[0], t->ne[1], t->ne[2], t->ne[3]);
    printf("  Stride: [%ld, %ld, %ld, %ld]\n",
           t->nb[0], t->nb[1], t->nb[2], t->nb[3]);
    printf("  Op: %s\n", ggml_op_name(t->op));
}
```

### 6.2 æ£€æŸ¥è®¡ç®—å›¾ç»“æ„

```c
void print_graph_info(struct ggml_cgraph * gf) {
    printf("Graph nodes: %d\n", gf->n_nodes);

    for (int i = 0; i < gf->n_nodes; i++) {
        struct ggml_tensor * node = gf->nodes[i];
        printf("  [%d] %s: %s -> [%ld, %ld]\n",
               i, node->name,
               ggml_op_name(node->op),
               node->ne[0], node->ne[1]);
    }
}
```

## 7. å®è·µç»ƒä¹ 

### ç»ƒä¹  1ï¼šå®ç° LayerNorm
ç¼–å†™ä»£ç å®ç° LayerNorm å±‚ï¼š
```
y = (x - mean) / sqrt(variance + eps) * gamma + beta
```

### ç»ƒä¹  2ï¼šæ„å»ºæ³¨æ„åŠ›å±‚
æ„å»ºå•å¤´è‡ªæ³¨æ„åŠ›ï¼š
```
Q = xWq, K = xWk, V = xWv
scores = QK^T / sqrt(d_k)
attn = softmax(scores)
output = attn @ V
```

### ç»ƒä¹  3ï¼šåˆ†ææ“ä½œæ€§èƒ½
ä½¿ç”¨ `ggml_graph_compute` çš„æ€§èƒ½ç»Ÿè®¡åŠŸèƒ½ï¼Œåˆ†ææ¯ä¸ªæ“ä½œçš„è€—æ—¶ã€‚

## 8. æ€»ç»“

ä»Šå¤©æˆ‘ä»¬å­¦ä¹ äº†ï¼š

âœ… **å¼ é‡æ“ä½œ**ï¼šåˆ›å»ºã€ç®—æœ¯ã€çŸ©é˜µä¹˜æ³•ã€RoPE ç­‰æ ¸å¿ƒæ“ä½œ
âœ… **è®¡ç®—å›¾**ï¼šç†è§£è®¡ç®—å›¾çš„æ¦‚å¿µã€æ„å»ºå’Œæ‰§è¡Œæµç¨‹
âœ… **å®æˆ˜ç¤ºä¾‹**ï¼šæ„å»ºç®€å•çš„ FFN å±‚
âœ… **ä¼˜åŒ–æŠ€å·§**ï¼šæ“ä½œèåˆã€in-place æ“ä½œã€æ•°æ®ç±»å‹é€‰æ‹©
âœ… **è°ƒè¯•æ–¹æ³•**ï¼šæ‰“å°ä¿¡æ¯ã€å¯è§†åŒ–è®¡ç®—å›¾

### å…³é”®è¦ç‚¹

1. **çŸ©é˜µä¹˜æ³•æ˜¯æ€§èƒ½ç“¶é¢ˆ**ï¼šå æ® 80%+ è®¡ç®—æ—¶é—´
2. **è®¡ç®—å›¾å®ç°å»¶è¿Ÿæ‰§è¡Œ**ï¼šå®šä¹‰æ—¶ä¸è®¡ç®—ï¼Œæ‰§è¡Œæ—¶ç»Ÿä¸€è®¡ç®—
3. **æ“ä½œå®ç°åˆ†ä¸ºä¸¤éƒ¨åˆ†**ï¼šå›¾æ„å»ºå‡½æ•° + å‰å‘è®¡ç®—å‡½æ•°
4. **SIMD ä¼˜åŒ–è‡³å…³é‡è¦**ï¼šAVX2/NEON å¯æå‡ 4-8 å€æ€§èƒ½

## ä¸‹ä¸€æ­¥

æ˜å¤©æˆ‘ä»¬å°†å­¦ä¹  **Day 4: GGML å†…å­˜ç®¡ç†æœºåˆ¶**ï¼Œæ·±å…¥ç†è§£ï¼š
- å†…å­˜åˆ†é…ç­–ç•¥
- mmap æ–‡ä»¶æ˜ å°„
- åç«¯ç¼“å†²åŒºç®¡ç†
- å†…å­˜ä¼˜åŒ–æŠ€å·§

---

**æ€è€ƒé¢˜**ï¼š
1. ä¸ºä»€ä¹ˆ `ggml_mul_mat(A, B)` è®¡ç®—çš„æ˜¯ `BA^T` è€Œä¸æ˜¯ `AB`ï¼Ÿ
2. RoPE ç›¸æ¯”ä¼ ç»Ÿä½ç½®ç¼–ç æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
3. å¦‚ä½•ä¼˜åŒ–å¤§æ‰¹é‡çš„çŸ©é˜µä¹˜æ³•ï¼Ÿ

ğŸ“š ç»§ç»­å­¦ä¹ ï¼š[Day 4: GGML å†…å­˜ç®¡ç†æœºåˆ¶](day04-ggml-memory.md)
