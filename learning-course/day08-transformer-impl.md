# Day 8: Transformer æ¶æ„å®ç°

## è¯¾ç¨‹ç›®æ ‡

æ·±å…¥ç†è§£ Transformer æ¶æ„åœ¨ llama.cpp ä¸­çš„å®ç°ï¼š
- LLaMA æ¶æ„è¯¦è§£
- Self-Attention å®ç°ç»†èŠ‚
- FFN (Feed-Forward Network) å®ç°
- ä¸åŒæ¨¡å‹æ¶æ„çš„å˜ä½“
- æ¶æ„ç‰¹å®šçš„ä¼˜åŒ–

## 1. LLaMA æ¶æ„æ¦‚è§ˆ

### 1.1 æ¨¡å‹ç»“æ„

```
LLaMA æ¨¡å‹ç»“æ„ï¼š

Input: token IDs [n_tokens]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token Embedding [n_vocab, n_embd]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Transformer     â”‚ Ã— n_layer (é€šå¸¸ 32 å±‚)
    â”‚  Block           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â•”â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•—
    â•‘ 1. RMS Norm    â•‘
    â•šâ•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•
             â†“
    â•”â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘ 2. Self-Attention           â•‘
    â•‘   â€¢ Q, K, V æŠ•å½±             â•‘
    â•‘   â€¢ RoPE ä½ç½®ç¼–ç            â•‘
    â•‘   â€¢ ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›           â•‘
    â•‘   â€¢ KV ç¼“å­˜                 â•‘
    â•‘   â€¢ è¾“å‡ºæŠ•å½±                â•‘
    â•šâ•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
             â†“
    â•”â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•—
    â•‘ 3. Residual    â•‘ +
    â•šâ•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•
             â†“
    â•”â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•—
    â•‘ 4. RMS Norm    â•‘
    â•šâ•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•
             â†“
    â•”â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘ 5. FFN (SwiGLU)             â•‘
    â•‘   â€¢ Gate æŠ•å½±                â•‘
    â•‘   â€¢ Up æŠ•å½±                  â•‘
    â•‘   â€¢ SiLU æ¿€æ´»                â•‘
    â•‘   â€¢ Element-wise ä¹˜æ³•        â•‘
    â•‘   â€¢ Down æŠ•å½±                â•‘
    â•šâ•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
             â†“
    â•”â•â•â•â•â•â•â•â•â–¼â•â•â•â•â•â•â•â•—
    â•‘ 6. Residual    â•‘ +
    â•šâ•â•â•â•â•â•â•â•â•¤â•â•â•â•â•â•â•â•
             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Final RMS Norm                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Projection [n_embd, n_vocab]â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
Output: logits [n_vocab]
```

### 1.2 å…³é”®å‚æ•°

```cpp
// LLaMA-7B çš„è¶…å‚æ•°
n_vocab     = 32000    // è¯è¡¨å¤§å°
n_embd      = 4096     // åµŒå…¥ç»´åº¦ (éšè—å¤§å°)
n_layer     = 32       // å±‚æ•°
n_head      = 32       // æ³¨æ„åŠ›å¤´æ•°
n_head_kv   = 32       // KV å¤´æ•° (GQA: å¯ä»¥æ›´å°‘)
n_ff        = 11008    // FFN éšè—å¤§å°
n_ctx_train = 2048     // è®­ç»ƒä¸Šä¸‹æ–‡é•¿åº¦
rope_freq_base = 10000.0  // RoPE é¢‘ç‡åŸºæ•°
```

## 2. Self-Attention å®ç°è¯¦è§£

### 2.1 å®Œæ•´çš„æ³¨æ„åŠ›è®¡ç®—

```cpp
// ä½ç½®ï¼šsrc/llama-graph.cpp:456
struct ggml_tensor * llm_build_kqv(
    struct llama_context & lctx,
    struct ggml_tensor * cur,      // è¾“å…¥ [n_embd, n_tokens]
    struct ggml_tensor * wq,       // Q æƒé‡ [n_embd, n_embd]
    struct ggml_tensor * wk,       // K æƒé‡ [n_embd, n_embd_gqa]
    struct ggml_tensor * wv,       // V æƒé‡ [n_embd, n_embd_gqa]
    struct ggml_tensor * wo,       // è¾“å‡ºæƒé‡ [n_embd, n_embd]
    int n_head,
    int n_head_kv,
    int il) {                      // å±‚ç´¢å¼•

    struct ggml_context * ctx = lctx.ctx_compute.get();
    const int n_embd_head = lctx.model.hparams.n_embd_head_k;
    const int n_embd = lctx.model.hparams.n_embd;

    // === Step 1: Q, K, V æŠ•å½± ===
    struct ggml_tensor * Qcur = ggml_mul_mat(ctx, wq, cur);
    struct ggml_tensor * Kcur = ggml_mul_mat(ctx, wk, cur);
    struct ggml_tensor * Vcur = ggml_mul_mat(ctx, wv, cur);

    // === Step 2: Reshape ä¸ºå¤šå¤´ ===
    // Q: [n_embd, n_tokens] â†’ [n_embd_head, n_head, n_tokens]
    Qcur = ggml_reshape_3d(ctx, Qcur, n_embd_head, n_head, n_tokens);

    // K, V: [n_embd_gqa, n_tokens] â†’ [n_embd_head, n_head_kv, n_tokens]
    Kcur = ggml_reshape_3d(ctx, Kcur, n_embd_head, n_head_kv, n_tokens);
    Vcur = ggml_reshape_3d(ctx, Vcur, n_embd_head, n_head_kv, n_tokens);

    // === Step 3: åº”ç”¨ RoPE ===
    Qcur = ggml_rope_ext(ctx, Qcur, inp_pos,
                         n_rot, rope_type, freq_base, freq_scale);
    Kcur = ggml_rope_ext(ctx, Kcur, inp_pos,
                         n_rot, rope_type, freq_base, freq_scale);

    // === Step 4: å­˜å‚¨ K, V åˆ°ç¼“å­˜ ===
    struct ggml_tensor * k = kv_cache.k_l[il];  // [n_embd_head, n_head_kv, kv_size]
    struct ggml_tensor * v = kv_cache.v_l[il];

    // å°† Kcur, Vcur æ‹·è´åˆ° KV ç¼“å­˜çš„ç›¸åº”ä½ç½®
    ggml_build_forward_expand(gf,
        ggml_cpy(ctx, Kcur,
                 ggml_view_1d(ctx, k, n_tokens * n_embd_head * n_head_kv,
                              (kv_head) * n_embd_head * n_head_kv * ggml_element_size(k))));
    ggml_build_forward_expand(gf,
        ggml_cpy(ctx, Vcur,
                 ggml_view_1d(ctx, v, n_tokens * n_embd_head * n_head_kv,
                              (kv_head) * n_embd_head * n_head_kv * ggml_element_size(v))));

    // === Step 5: Flash Attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰===
    struct ggml_tensor * kqv;

    if (use_flash_attn) {
        // Flash Attention: èåˆçš„ä¼˜åŒ–å®ç°
        kqv = ggml_flash_attn_ext(ctx, Qcur, k, v, kq_mask,
                                   1.0f / sqrtf(n_embd_head),  // scale
                                   0.0f);  // max_bias
    } else {
        // === Step 5a: æ ‡å‡†æ³¨æ„åŠ› - è®¡ç®— QK^T ===
        // scores = Q @ K^T  [n_head, n_tokens, kv_size]
        struct ggml_tensor * kq = ggml_mul_mat(ctx, k, Qcur);

        // === Step 5b: ç¼©æ”¾ ===
        kq = ggml_scale(ctx, kq, 1.0f / sqrtf(n_embd_head));

        // === Step 5c: æ·»åŠ æ³¨æ„åŠ›æ©ç  ===
        kq = ggml_add(ctx, kq, kq_mask);

        // === Step 5d: Softmax ===
        kq = ggml_soft_max_ext(ctx, kq, kq_mask, 1.0f, hparams.f_max_alibi_bias);

        // === Step 5e: åº”ç”¨åˆ° V ===
        // kqv = softmax(scores) @ V  [n_head, n_tokens, n_embd_head]
        kqv = ggml_mul_mat(ctx, v, kq);
    }

    // === Step 6: Reshape å›åŸå§‹å½¢çŠ¶ ===
    // [n_head, n_tokens, n_embd_head] â†’ [n_embd, n_tokens]
    kqv = ggml_reshape_2d(ctx, kqv, n_embd, n_tokens);

    // === Step 7: è¾“å‡ºæŠ•å½± ===
    struct ggml_tensor * attn_out = ggml_mul_mat(ctx, wo, kqv);

    return attn_out;
}
```

### 2.2 åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰

```
æ ‡å‡† Multi-Head Attention (MHA):
    Q: 32 ä¸ªå¤´
    K: 32 ä¸ªå¤´  â† æ¯ä¸ªå¤´éƒ½æœ‰ç‹¬ç«‹çš„ K
    V: 32 ä¸ªå¤´  â† æ¯ä¸ªå¤´éƒ½æœ‰ç‹¬ç«‹çš„ V

åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA):
    Q: 32 ä¸ªå¤´
    K: 4 ä¸ªå¤´   â† å…±äº«ï¼å¤šä¸ª Q å¤´å…±äº«ä¸€ä¸ª KV å¤´
    V: 4 ä¸ªå¤´

    Q[0-7]   å…±äº« â†’ K[0], V[0]
    Q[8-15]  å…±äº« â†’ K[1], V[1]
    Q[16-23] å…±äº« â†’ K[2], V[2]
    Q[24-31] å…±äº« â†’ K[3], V[3]

ä¼˜åŠ¿ï¼š
    â€¢ KV ç¼“å­˜å‡å°‘ 8 å€
    â€¢ å†…å­˜ä½¿ç”¨å¤§å¹…é™ä½
    â€¢ æ¨ç†é€Ÿåº¦æå‡
    â€¢ è´¨é‡æŸå¤±å¾ˆå°
```

**å®ç°**ï¼š
```cpp
const int n_head = 32;       // Q å¤´æ•°
const int n_head_kv = 4;     // KV å¤´æ•°
const int n_gqa = n_head / n_head_kv;  // 8 (æ¯ç»„ Q å¤´æ•°)

// K, V åªæœ‰ 4 ä¸ªå¤´
Kcur = ggml_reshape_3d(ctx, Kcur, n_embd_head, n_head_kv, n_tokens);

// å¹¿æ’­ KV åˆ°æ‰€æœ‰ Q å¤´
K = ggml_repeat(ctx, K, n_gqa);  // [n_embd_head, 4, ...] â†’ [n_embd_head, 32, ...]
```

## 3. FFN (Feed-Forward Network) å®ç°

### 3.1 SwiGLU æ¿€æ´»

LLaMA ä½¿ç”¨ SwiGLU è€Œéä¼ ç»Ÿçš„ ReLUï¼š

```
ä¼ ç»Ÿ FFN (GPT-2 é£æ ¼):
    hidden = ReLU(x @ W1 + b1)
    output = hidden @ W2 + b2

SwiGLU FFN (LLaMA é£æ ¼):
    gate = x @ W_gate
    up = x @ W_up
    hidden = SiLU(gate) âŠ™ up    # âŠ™ æ˜¯é€å…ƒç´ ä¹˜æ³•
    output = hidden @ W_down

å…¬å¼ï¼š
    SwiGLU(x, W) = SiLU(x @ W_gate) âŠ™ (x @ W_up)
    å…¶ä¸­ SiLU(x) = x * sigmoid(x)
```

### 3.2 å®ç°ä»£ç 

```cpp
// ä½ç½®ï¼šsrc/llama-graph.cpp:1123
struct ggml_tensor * llm_build_ffn(
    struct ggml_context * ctx,
    struct ggml_tensor * cur,      // è¾“å…¥ [n_embd, n_tokens]
    struct ggml_tensor * ffn_gate, // Gate æƒé‡ [n_embd, n_ff]
    struct ggml_tensor * ffn_up,   // Up æƒé‡ [n_embd, n_ff]
    struct ggml_tensor * ffn_down) // Down æƒé‡ [n_ff, n_embd]
{
    // === Step 1: Gate å’Œ Up æŠ•å½± ===
    struct ggml_tensor * gate = ggml_mul_mat(ctx, ffn_gate, cur);
    struct ggml_tensor * up   = ggml_mul_mat(ctx, ffn_up, cur);

    // === Step 2: SiLU æ¿€æ´» ===
    // SiLU(x) = x * sigmoid(x) = x / (1 + exp(-x))
    gate = ggml_silu(ctx, gate);

    // === Step 3: é€å…ƒç´ ä¹˜æ³• ===
    struct ggml_tensor * hidden = ggml_mul(ctx, gate, up);

    // === Step 4: Down æŠ•å½± ===
    struct ggml_tensor * output = ggml_mul_mat(ctx, ffn_down, hidden);

    return output;
}
```

**ä¸ºä»€ä¹ˆä½¿ç”¨ SwiGLUï¼Ÿ**
- **æ€§èƒ½æ›´å¥½**ï¼šåœ¨å¤šä¸ªåŸºå‡†æµ‹è¯•ä¸Šä¼˜äº ReLU/GELU
- **é—¨æ§æœºåˆ¶**ï¼šå¯ä»¥åŠ¨æ€æ§åˆ¶ä¿¡æ¯æµ
- **å¹³æ»‘æ¢¯åº¦**ï¼šæ¯” ReLU æ›´åˆ©äºè®­ç»ƒ

## 4. RMS Normï¼ˆRoot Mean Square Normalizationï¼‰

LLaMA ä½¿ç”¨ RMS Norm è€Œé LayerNormï¼š

```cpp
// ä½ç½®ï¼šggml/src/ggml.c:12678
static void ggml_compute_forward_rms_norm_f32(
    const struct ggml_compute_params * params,
    struct ggml_tensor * dst) {

    const struct ggml_tensor * src = dst->src[0];
    const float eps = ((float *) dst->op_params)[0];

    const int n_elements = src->ne[0];  // å½’ä¸€åŒ–ç»´åº¦

    for (int64_t i = 0; i < n_rows; i++) {
        const float * x = (float *)((char *) src->data + i * src->nb[1]);
        float * y = (float *)((char *) dst->data + i * dst->nb[1]);

        // 1. è®¡ç®—å¹³æ–¹å’Œ
        float sum = 0.0f;
        for (int j = 0; j < n_elements; j++) {
            sum += x[j] * x[j];
        }

        // 2. è®¡ç®— RMS
        float rms = sqrtf(sum / n_elements + eps);

        // 3. å½’ä¸€åŒ–
        for (int j = 0; j < n_elements; j++) {
            y[j] = x[j] / rms;
        }
    }
}
```

**RMS Norm vs LayerNorm**ï¼š
```
LayerNorm:
    y = (x - mean(x)) / sqrt(var(x) + eps) * gamma + beta

RMS Norm:
    y = x / RMS(x) * gamma
    å…¶ä¸­ RMS(x) = sqrt(mean(xÂ²) + eps)

å·®å¼‚ï¼š
    â€¢ ä¸å‡å»å‡å€¼
    â€¢ ä¸éœ€è¦ bias é¡¹
    â€¢ è®¡ç®—æ›´å¿«
    â€¢ æ•ˆæœç›¸å½“
```

## 5. ä¸åŒæ¨¡å‹æ¶æ„çš„å˜ä½“

### 5.1 ä¸»è¦ LLM æ¶æ„å¯¹æ¯”

| æ¨¡å‹ | æ³¨æ„åŠ› | FFN æ¿€æ´» | å½’ä¸€åŒ– | ä½ç½®ç¼–ç  | ç‰¹æ®Šä¹‹å¤„ |
|------|--------|---------|--------|---------|---------|
| **LLaMA** | MHA/GQA | SwiGLU | RMS Norm | RoPE | ç»å…¸æ¶æ„ |
| **Mistral** | GQA | SwiGLU | RMS Norm | RoPE | SWA (æ»‘çª—) |
| **Qwen** | GQA | SwiGLU | RMS Norm | RoPE | YARN æ‰©å±• |
| **Gemma** | MQA | GeGLU | RMS Norm | RoPE | Logit è½¯ä¸Šé™ |
| **Phi** | MHA | GELU | LayerNorm | Learned | å°æ¨¡å‹ä¼˜åŒ– |
| **GPT-2** | MHA | GELU | LayerNorm | Learned | ç»å…¸ GPT |

### 5.2 Mistral çš„ Sliding Window Attention

```cpp
// Mistral ç‰¹æœ‰ï¼šæ»‘åŠ¨çª—å£æ³¨æ„åŠ›
// ä½ç½®ï¼šsrc/llama-graph.cpp:678

if (hparams.n_swa > 0) {  // Sliding Window Attention å¯ç”¨
    // åªæ³¨æ„æœ€è¿‘ n_swa ä¸ª token
    for (int i = 0; i < n_tokens; i++) {
        int window_start = max(0, i - n_swa);
        int window_end = i + 1;

        // æ©ç åªå…è®¸çª—å£å†…çš„ token
        for (int j = 0; j < window_start; j++) {
            mask[i * kv_size + j] = -INFINITY;  // å±è”½çª—å£å¤–
        }
        for (int j = window_start; j < window_end; j++) {
            mask[i * kv_size + j] = 0.0f;  // çª—å£å†…å¯è§
        }
        for (int j = window_end; j < kv_size; j++) {
            mask[i * kv_size + j] = -INFINITY;  // æœªæ¥ä¸å¯è§
        }
    }
}
```

**SWA ä¼˜åŠ¿**ï¼š
- å‡å°‘ KV ç¼“å­˜ä½¿ç”¨
- å¯ä»¥å¤„ç†è¶…é•¿ä¸Šä¸‹æ–‡
- æ¨ç†é€Ÿåº¦æ›´å¿«

### 5.3 Gemma çš„ Logit è½¯ä¸Šé™

```cpp
// Gemma ç‰¹æœ‰ï¼šé™åˆ¶ logit èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼ä¸ç¨³å®š
// ä½ç½®ï¼šsrc/llama-graph.cpp:1789

if (hparams.f_attn_logit_softcapping > 0.0f) {
    // logits = tanh(logits / cap) * cap
    float cap = hparams.f_attn_logit_softcapping;

    logits = ggml_scale(ctx, logits, 1.0f / cap);
    logits = ggml_tanh(ctx, logits);
    logits = ggml_scale(ctx, logits, cap);
}
```

## 6. å®Œæ•´çš„ Transformer Block å®ç°

```cpp
// ä½ç½®ï¼šsrc/llama-graph.cpp:2341
struct ggml_tensor * llm_build_llama_layer(
    struct llama_context & lctx,
    int il,
    struct ggml_tensor * inpL,
    struct ggml_tensor * attn_mask) {

    const auto & model = lctx.model;
    const auto & hparams = model.hparams;
    const auto & layer = model.layers[il];
    struct ggml_context * ctx = lctx.ctx_compute.get();

    // ========== æ³¨æ„åŠ›éƒ¨åˆ† ==========

    // 1. RMS Norm
    struct ggml_tensor * attn_norm_out = ggml_rms_norm(ctx, inpL, hparams.f_norm_rms_eps);
    attn_norm_out = ggml_mul(ctx, attn_norm_out, layer.attn_norm);

    // 2. Self-Attention
    struct ggml_tensor * attn_out = llm_build_kqv(
        lctx, attn_norm_out,
        layer.wq, layer.wk, layer.wv, layer.wo,
        hparams.n_head, hparams.n_head_kv, il);

    // 3. æ®‹å·®è¿æ¥
    inpL = ggml_add(ctx, inpL, attn_out);

    // ========== FFN éƒ¨åˆ† ==========

    // 4. RMS Norm
    struct ggml_tensor * ffn_norm_out = ggml_rms_norm(ctx, inpL, hparams.f_norm_rms_eps);
    ffn_norm_out = ggml_mul(ctx, ffn_norm_out, layer.ffn_norm);

    // 5. FFN (SwiGLU)
    struct ggml_tensor * ffn_out = llm_build_ffn(
        ctx, ffn_norm_out,
        layer.ffn_gate, layer.ffn_up, layer.ffn_down);

    // 6. æ®‹å·®è¿æ¥
    inpL = ggml_add(ctx, inpL, ffn_out);

    return inpL;
}
```

## 7. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 7.1 KV ç¼“å­˜ä¼˜åŒ–

```cpp
// ä½¿ç”¨ FP16 å­˜å‚¨ KV ç¼“å­˜
ctx_params.type_k = GGML_TYPE_F16;  // è€Œé F32
ctx_params.type_v = GGML_TYPE_F16;

// èŠ‚çœ 50% çš„ KV ç¼“å­˜å†…å­˜
// 7B æ¨¡å‹ï¼š~1GB (F16) vs ~2GB (F32)
```

### 7.2 æ‰¹å¤„ç†ä¼˜åŒ–

```cpp
// æ‰¹é‡å¤„ç†å¤šä¸ª token
llama_batch batch = llama_batch_init(512, 0, 1);

// æ·»åŠ å¤šä¸ª token
for (int i = 0; i < n_tokens; i++) {
    llama_batch_add(batch, tokens[i], i, {0}, i == n_tokens - 1);
}

// ä¸€æ¬¡æ¨ç†å¤„ç†æ•´ä¸ªæ‰¹æ¬¡
llama_decode(ctx, batch);
```

### 7.3 Flash Attention

```cpp
// å¯ç”¨ Flash Attentionï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰
ctx_params.flash_attn_type = LLAMA_FLASH_ATTN_TYPE_ENABLED;

// ä¼˜åŠ¿ï¼š
// â€¢ å‡å°‘å†…å­˜è®¿é—®
// â€¢ 2-4x æ³¨æ„åŠ›åŠ é€Ÿ
// â€¢ æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡
```

## 8. æ€»ç»“

ä»Šå¤©æˆ‘ä»¬æ·±å…¥å­¦ä¹ äº† Transformer æ¶æ„ï¼š

âœ… **LLaMA æ¶æ„**ï¼šå®Œæ•´çš„æ¨¡å‹ç»“æ„
âœ… **Self-Attention**ï¼šå¤šå¤´æ³¨æ„åŠ›ã€GQAã€Flash Attention
âœ… **FFN**ï¼šSwiGLU æ¿€æ´»å‡½æ•°
âœ… **å½’ä¸€åŒ–**ï¼šRMS Norm å®ç°
âœ… **æ¶æ„å˜ä½“**ï¼šMistral SWAã€Gemma è½¯ä¸Šé™

### å…³é”®è¦ç‚¹

1. **GQA**ï¼šå‡å°‘ KV ç¼“å­˜ï¼Œæå‡æ¨ç†é€Ÿåº¦
2. **SwiGLU**ï¼šé—¨æ§ FFNï¼Œæ€§èƒ½æ›´å¥½
3. **RMS Norm**ï¼šæ›´å¿«çš„å½’ä¸€åŒ–
4. **RoPE**ï¼šæ—‹è½¬ä½ç½®ç¼–ç ï¼Œæ”¯æŒå¤–æ¨

## ä¸‹ä¸€æ­¥

æ˜å¤©æˆ‘ä»¬å°†å­¦ä¹  **Day 10: æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–**ï¼š
- Flash Attention åŸç†
- å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰
- Sliding Window Attention
- ç¨€ç–æ³¨æ„åŠ›æŠ€æœ¯

---

**ç»ƒä¹ **ï¼š
1. å¯¹æ¯” MHAã€GQAã€MQA çš„ KV ç¼“å­˜å¤§å°
2. å®ç°ä¸€ä¸ªç®€å•çš„ Transformer Block
3. åˆ†æä¸åŒæ¿€æ´»å‡½æ•°çš„æ€§èƒ½å·®å¼‚

ğŸ“š [Day 10: æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–](day10-attention-optimization.md)
