# Day 10: æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–

## è¯¾ç¨‹ç›®æ ‡

æ·±å…¥ç†è§£æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–æŠ€æœ¯ï¼š
- Flash Attention åŸç†ä¸å®ç°
- å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰
- Sliding Window Attentionï¼ˆSWAï¼‰
- ç¨€ç–æ³¨æ„åŠ›æŠ€æœ¯
- é•¿ä¸Šä¸‹æ–‡ä¼˜åŒ–

## 1. Flash Attention

### 1.1 æ ‡å‡†æ³¨æ„åŠ›çš„é—®é¢˜

```
æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—æµç¨‹ï¼š

1. Q @ K^T  â†’  [n_tokens, kv_size] (å†™å…¥ HBM)
2. softmax   â†’  [n_tokens, kv_size] (è¯»å†™ HBM)
3. @ V       â†’  [n_tokens, n_embd]  (è¯»å†™ HBM)

é—®é¢˜ï¼š
â€¢ ä¸­é—´ç»“æœ (QK^T) éœ€è¦å¤§é‡å†…å­˜
â€¢ å¤šæ¬¡ HBM è®¿é—®ï¼ˆGPU çš„ç“¶é¢ˆï¼‰
â€¢ æ—¶é—´å¤æ‚åº¦: O(NÂ²)
â€¢ ç©ºé—´å¤æ‚åº¦: O(NÂ²)

å¯¹äº n_tokens=2048, éœ€è¦:
    2048Â² Ã— 4 bytes = 16MB æ¯å±‚
    16MB Ã— 32 å±‚ = 512MB ä»…ç”¨äºæ³¨æ„åŠ›çŸ©é˜µï¼
```

### 1.2 Flash Attention åŸç†

**æ ¸å¿ƒæ€æƒ³**ï¼šåˆ†å—è®¡ç®— + åœ¨çº¿ Softmax

```cpp
// ä¼ªä»£ç 
FlashAttention(Q, K, V, block_size):
    // åˆ†å—
    Q_blocks = split(Q, block_size)
    K_blocks = split(K, block_size)
    V_blocks = split(V, block_size)

    O = zeros_like(Q)  // è¾“å‡º
    l = zeros(Q.rows)  // ç´¯ç§¯å½’ä¸€åŒ–å› å­
    m = -inf(Q.rows)   // ç´¯ç§¯æœ€å¤§å€¼

    // å¤–å¾ªç¯ï¼šQ å—
    for Q_block in Q_blocks:
        // å†…å¾ªç¯ï¼šK, V å—
        for K_block, V_block in zip(K_blocks, V_blocks):
            // 1. è®¡ç®—å½“å‰å—çš„æ³¨æ„åŠ›åˆ†æ•°
            S_block = Q_block @ K_block.T

            // 2. åœ¨çº¿æ›´æ–°æœ€å¤§å€¼å’Œå½’ä¸€åŒ–å› å­
            m_new = max(m, max(S_block, axis=1))
            l_new = exp(m - m_new) * l + exp(S_block - m_new).sum(axis=1)

            // 3. æ›´æ–°è¾“å‡º
            O = O * (l / l_new) + (S_block @ V_block) * (exp(S_block - m_new) / l_new)

            m = m_new
            l = l_new

    return O
```

**ä¼˜åŠ¿**ï¼š
- âœ… å†…å­˜ä½¿ç”¨ï¼šO(N) è€Œé O(NÂ²)
- âœ… HBM è®¿é—®å‡å°‘ï¼š~5x åŠ é€Ÿ
- âœ… æ”¯æŒæ›´é•¿ä¸Šä¸‹æ–‡
- âœ… æ•°å€¼ç¨³å®š

### 1.3 CUDA å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰

```cuda
// ä½ç½®ï¼šggml/src/ggml-cuda/fattn.cuh
template<int D, int ncols>
__global__ void flash_attn_kernel(
    const float * Q,    // [n_head, n_tokens, d_head]
    const float * K,    // [n_head, kv_size, d_head]
    const float * V,    // [n_head, kv_size, d_head]
    float * O,          // [n_head, n_tokens, d_head]
    const float scale,
    const int kv_size) {

    const int tid = threadIdx.x;
    const int head_idx = blockIdx.y;
    const int token_idx = blockIdx.x;

    // å…±äº«å†…å­˜ï¼šå‡å°‘ HBM è®¿é—®
    __shared__ float Q_shared[D];
    __shared__ float K_shared[D];
    __shared__ float V_shared[D];
    __shared__ float scores_shared[32];  // å—å†…åˆ†æ•°

    // åŠ è½½ Q åˆ°å…±äº«å†…å­˜
    if (tid < D) {
        Q_shared[tid] = Q[head_idx * n_tokens * D + token_idx * D + tid];
    }
    __syncthreads();

    float max_score = -INFINITY;
    float sum_exp = 0.0f;
    float output[D] = {0};

    // åˆ†å—å¤„ç† K, V
    const int n_blocks = (kv_size + 31) / 32;
    for (int block = 0; block < n_blocks; block++) {
        int kv_idx = block * 32 + tid;

        // åŠ è½½ K å—
        if (kv_idx < kv_size && tid < D) {
            K_shared[tid] = K[head_idx * kv_size * D + kv_idx * D + tid];
        }
        __syncthreads();

        // è®¡ç®— Q @ K^T
        if (kv_idx < kv_size) {
            float score = 0.0f;
            for (int i = 0; i < D; i++) {
                score += Q_shared[i] * K_shared[i];
            }
            score *= scale;
            scores_shared[tid] = score;

            // åœ¨çº¿æ›´æ–°æœ€å¤§å€¼
            max_score = fmaxf(max_score, score);
        }
        __syncthreads();

        // è®¡ç®— exp å¹¶ç´¯ç§¯
        if (kv_idx < kv_size) {
            float exp_score = expf(scores_shared[tid] - max_score);
            sum_exp += exp_score;

            // åŠ è½½ V å¹¶ç´¯ç§¯åˆ°è¾“å‡º
            if (tid < D) {
                V_shared[tid] = V[head_idx * kv_size * D + kv_idx * D + tid];
            }
            __syncthreads();

            for (int i = 0; i < D; i++) {
                output[i] += exp_score * V_shared[i];
            }
        }
        __syncthreads();
    }

    // å½’ä¸€åŒ–å¹¶å†™å›
    if (tid < D) {
        O[head_idx * n_tokens * D + token_idx * D + tid] = output[tid] / sum_exp;
    }
}
```

### 1.4 ä½¿ç”¨ Flash Attention

```cpp
// å¯ç”¨ Flash Attention
llama_context_params ctx_params = llama_context_default_params();
ctx_params.flash_attn_type = LLAMA_FLASH_ATTN_TYPE_ENABLED;

llama_context * ctx = llama_new_context_with_model(model, ctx_params);

// è‡ªåŠ¨ä½¿ç”¨ Flash Attentionï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰
// æ€§èƒ½æå‡ï¼š2-4xï¼ˆå–å†³äºåºåˆ—é•¿åº¦ï¼‰
```

## 2. å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆMQAï¼‰

### 2.1 MQA vs GQA vs MHA

```
MHA (Multi-Head Attention):
    Q: [n_head=32, n_tokens, d_head]
    K: [n_head=32, kv_size, d_head]
    V: [n_head=32, kv_size, d_head]
    KV ç¼“å­˜: 32 Ã— 2 Ã— kv_size Ã— d_head

GQA (Grouped-Query Attention):
    Q: [n_head=32, n_tokens, d_head]
    K: [n_head_kv=4, kv_size, d_head]  â† å…±äº«
    V: [n_head_kv=4, kv_size, d_head]  â† å…±äº«
    KV ç¼“å­˜: 4 Ã— 2 Ã— kv_size Ã— d_head (8x å‡å°‘)

MQA (Multi-Query Attention):
    Q: [n_head=32, n_tokens, d_head]
    K: [n_head_kv=1, kv_size, d_head]  â† å…¨å±€å…±äº«
    V: [n_head_kv=1, kv_size, d_head]  â† å…¨å±€å…±äº«
    KV ç¼“å­˜: 1 Ã— 2 Ã— kv_size Ã— d_head (32x å‡å°‘)
```

### 2.2 MQA å®ç°

```cpp
// ä½ç½®ï¼šsrc/llama-graph.cpp:589
struct ggml_tensor * llm_build_mqa(
    struct ggml_context * ctx,
    struct ggml_tensor * Q,    // [n_head, n_tokens, d_head]
    struct ggml_tensor * K,    // [1, kv_size, d_head]
    struct ggml_tensor * V) {  // [1, kv_size, d_head]

    const int n_head = Q->ne[1];

    // å¹¿æ’­ K, V åˆ°æ‰€æœ‰å¤´
    K = ggml_repeat(ctx, K, ggml_new_tensor_3d(ctx, K->type, d_head, n_head, kv_size));
    V = ggml_repeat(ctx, V, ggml_new_tensor_3d(ctx, V->type, d_head, n_head, kv_size));

    // æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
    struct ggml_tensor * kq = ggml_mul_mat(ctx, K, Q);
    kq = ggml_scale(ctx, kq, 1.0f / sqrtf(d_head));
    kq = ggml_soft_max(ctx, kq);

    struct ggml_tensor * output = ggml_mul_mat(ctx, V, kq);

    return output;
}
```

**Trade-off**ï¼š
- MQAï¼šæœ€çœå†…å­˜ï¼Œä½†è´¨é‡ç•¥é™
- GQAï¼šå¹³è¡¡å†…å­˜å’Œè´¨é‡ï¼ˆæ¨èï¼‰
- MHAï¼šæœ€é«˜è´¨é‡ï¼Œä½†å†…å­˜æœ€å¤š

## 3. Sliding Window Attention

### 3.1 åŸç†

```
æ ‡å‡†å› æœæ³¨æ„åŠ›ï¼š
    Token 0: å¯ä»¥çœ‹åˆ° [0]
    Token 1: å¯ä»¥çœ‹åˆ° [0, 1]
    Token 2: å¯ä»¥çœ‹åˆ° [0, 1, 2]
    ...
    Token N: å¯ä»¥çœ‹åˆ° [0, 1, 2, ..., N]

    âŒ é—®é¢˜ï¼šKV ç¼“å­˜éš N çº¿æ€§å¢é•¿

SWA (window_size=1024)ï¼š
    Token 1000: å¯ä»¥çœ‹åˆ° [0, 1, ..., 1000]        (å…¨éƒ¨)
    Token 1500: å¯ä»¥çœ‹åˆ° [476, 477, ..., 1500]    (æœ€è¿‘ 1024)
    Token 2000: å¯ä»¥çœ‹åˆ° [976, 977, ..., 2000]    (æœ€è¿‘ 1024)

    âœ… å¥½å¤„ï¼šKV ç¼“å­˜å›ºå®šä¸º window_size
```

### 3.2 å®ç°

```cpp
// ä½ç½®ï¼šsrc/llama-kv-cache.cpp:456
void llama_kv_cache_update_swa(
    struct llama_kv_cache & cache,
    int n_tokens_new,
    int window_size) {

    if (cache.head + n_tokens_new <= window_size) {
        // è¿˜åœ¨çª—å£å†…ï¼Œæ­£å¸¸æ·»åŠ 
        cache.head += n_tokens_new;
        return;
    }

    // è¶…å‡ºçª—å£ï¼Œéœ€è¦æ»‘åŠ¨
    const int n_discard = cache.head + n_tokens_new - window_size;

    // ç§»åŠ¨ KV ç¼“å­˜
    for (int il = 0; il < cache.n_layer; il++) {
        ggml_tensor * k = cache.k_l[il];
        ggml_tensor * v = cache.v_l[il];

        // ä¸¢å¼ƒæœ€æ—§çš„ n_discard ä¸ª token
        // K: [d_head, kv_size] â†’ shift left by n_discard
        memmove(k->data,
                (char*)k->data + n_discard * k->nb[1],
                (window_size - n_discard) * k->nb[1]);

        // ç±»ä¼¼åœ°ç§»åŠ¨ V
        memmove(v->data,
                (char*)v->data + n_discard * v->nb[1],
                (window_size - n_discard) * v->nb[1]);
    }

    cache.head = window_size - n_discard;
}
```

**ä½¿ç”¨åœºæ™¯**ï¼š
- Mistral 7Bï¼šwindow_size=4096
- é•¿æ–‡æ¡£ç”Ÿæˆ
- å¯¹è¯ç³»ç»Ÿ

## 4. é•¿ä¸Šä¸‹æ–‡ä¼˜åŒ–

### 4.1 YaRNï¼ˆYet another RoPE extensioNï¼‰

**é—®é¢˜**ï¼šRoPE å¤–æ¨æ€§èƒ½å·®
```
è®­ç»ƒæ—¶: max_pos = 2048
æ¨ç†æ—¶: pos = 4096  â† è¶…å‡ºè®­ç»ƒèŒƒå›´
â†’ æ¨¡å‹æ€§èƒ½å¤§å¹…ä¸‹é™
```

**YaRN è§£å†³æ–¹æ¡ˆ**ï¼š
```cpp
// ä½ç½®ï¼šsrc/llama-graph.cpp:234
float yarn_get_scaling_factor(float pos, float base_freq, float scale_factor) {
    // æ ¹æ®ä½ç½®åŠ¨æ€è°ƒæ•´é¢‘ç‡
    if (pos <= ctx_train) {
        return 1.0f;  // è®­ç»ƒèŒƒå›´å†…ï¼Œä¸ç¼©æ”¾
    } else {
        // å¤–æ¨èŒƒå›´ï¼Œåº”ç”¨ç¼©æ”¾
        float alpha = scale_factor * (pos / ctx_train);
        return 1.0f / powf(alpha, 2.0f / dim);
    }
}
```

### 4.2 ä¸Šä¸‹æ–‡å‹ç¼©

```cpp
// ä¿ç•™é‡è¦ tokenï¼Œä¸¢å¼ƒä¸é‡è¦çš„
std::vector<int> compress_context(
    const std::vector<llama_token> & tokens,
    const std::vector<float> & importance_scores,
    int target_len) {

    // æŒ‰é‡è¦æ€§æ’åº
    std::vector<int> indices(tokens.size());
    std::iota(indices.begin(), indices.end(), 0);
    std::sort(indices.begin(), indices.end(),
        [&](int a, int b) {
            return importance_scores[a] > importance_scores[b];
        });

    // ä¿ç•™å‰ target_len ä¸ª
    indices.resize(target_len);
    std::sort(indices.begin(), indices.end());

    std::vector<int> compressed;
    for (int idx : indices) {
        compressed.push_back(tokens[idx]);
    }

    return compressed;
}
```

## 5. æ€§èƒ½å¯¹æ¯”

| æŠ€æœ¯ | å†…å­˜èŠ‚çœ | é€Ÿåº¦æå‡ | è´¨é‡å½±å“ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|---------|
| **Flash Attention** | 50% | 2-4x | æ—  | æ‰€æœ‰ |
| **GQA** | 75% | 1.5x | è½»å¾® | æ¨ç† |
| **MQA** | 97% | 2x | ä¸­ç­‰ | é€Ÿåº¦ä¼˜å…ˆ |
| **SWA** | å›ºå®š | 1.2x | å–å†³äºçª—å£ | é•¿æ–‡æœ¬ |
| **YaRN** | æ—  | æ—  | æ”¹å–„å¤–æ¨ | é•¿ä¸Šä¸‹æ–‡ |

## 6. æ€»ç»“

ä»Šå¤©æˆ‘ä»¬å­¦ä¹ äº†æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–ï¼š

âœ… **Flash Attention**ï¼šå‡å°‘å†…å­˜å’Œè®¡ç®—
âœ… **MQA/GQA**ï¼šå…±äº« KV å¤´
âœ… **SWA**ï¼šå›ºå®šçª—å£å¤§å°
âœ… **é•¿ä¸Šä¸‹æ–‡**ï¼šYaRNã€å‹ç¼©æŠ€æœ¯

### å…³é”®è¦ç‚¹

1. **Flash Attention æ˜¯å¿…å¤‡**ï¼š2-4x åŠ é€Ÿ
2. **GQA æœ€å¹³è¡¡**ï¼šè´¨é‡å’Œé€Ÿåº¦å…¼é¡¾
3. **SWA é€‚åˆé•¿æ–‡æœ¬**ï¼šå›ºå®šå†…å­˜
4. **ç»„åˆä½¿ç”¨**ï¼šå åŠ ä¼˜åŒ–æ•ˆæœ

## ä¸‹ä¸€æ­¥

æ˜å¤©æˆ‘ä»¬å°†å­¦ä¹  **Day 11: CPU åç«¯ä¸ SIMD ä¼˜åŒ–**ï¼š
- AVX2/AVX-512 ä¼˜åŒ–
- ARM NEON ä¼˜åŒ–
- çŸ©é˜µä¹˜æ³•å†…æ ¸
- é‡åŒ–åŠ é€Ÿ

---

ğŸ“š [Day 11: CPU åç«¯ä¸ SIMD ä¼˜åŒ–](day11-cpu-backend.md)
