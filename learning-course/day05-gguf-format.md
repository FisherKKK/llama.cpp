# Day 5: GGUF æ–‡ä»¶æ ¼å¼è¯¦è§£

## è¯¾ç¨‹ç›®æ ‡

æ·±å…¥ç†è§£ GGUFï¼ˆGGML Universal Fileï¼‰æ ¼å¼ï¼š
- GGUF æ–‡ä»¶ç»“æ„
- å…ƒæ•°æ®ç³»ç»Ÿ
- å¼ é‡å­˜å‚¨æ ¼å¼
- æ–‡ä»¶è§£æå®ç°
- å®æˆ˜ï¼šè§£æ GGUF æ–‡ä»¶

## 1. GGUF æ ¼å¼æ¦‚è¿°

GGUF æ˜¯ llama.cpp ä½¿ç”¨çš„æ¨¡å‹æ–‡ä»¶æ ¼å¼ï¼Œè®¾è®¡ç›®æ ‡ï¼š
- **è‡ªæè¿°**ï¼šæ–‡ä»¶åŒ…å«å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯
- **å¯æ‰©å±•**ï¼šæ”¯æŒæ·»åŠ æ–°çš„å…ƒæ•°æ®
- **é«˜æ•ˆ**ï¼šæ”¯æŒ mmap ç›´æ¥æ˜ å°„
- **è·¨å¹³å°**ï¼šç»Ÿä¸€çš„äºŒè¿›åˆ¶æ ¼å¼

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ GGUFï¼Ÿ

æ—§æ ¼å¼ï¼ˆGGMLï¼‰çš„é—®é¢˜ï¼š
- ç¼ºå°‘ç‰ˆæœ¬ä¿¡æ¯
- å…ƒæ•°æ®ä¸å®Œæ•´
- éš¾ä»¥æ‰©å±•

GGUF çš„æ”¹è¿›ï¼š
- âœ… å®Œæ•´çš„å…ƒæ•°æ®ç³»ç»Ÿ
- âœ… æ¸…æ™°çš„ç‰ˆæœ¬ç®¡ç†
- âœ… çµæ´»çš„æ‰©å±•æœºåˆ¶
- âœ… æ›´å¥½çš„å·¥å…·æ”¯æŒ

## 2. GGUF æ–‡ä»¶ç»“æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GGUF File Structure                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Header (å›ºå®šå¤§å°)                 â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ â€¢ Magic: 0x46554747 ("GGUF")    â”‚ â”‚
â”‚  â”‚ â€¢ Version: 3                     â”‚ â”‚
â”‚  â”‚ â€¢ Tensor Count: N                â”‚ â”‚
â”‚  â”‚ â€¢ Metadata KV Count: M           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Metadata (å¯å˜å¤§å°)               â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ KV Pair 1:                       â”‚ â”‚
â”‚  â”‚   key: "general.architecture"    â”‚ â”‚
â”‚  â”‚   type: STRING                   â”‚ â”‚
â”‚  â”‚   value: "llama"                 â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ KV Pair 2:                       â”‚ â”‚
â”‚  â”‚   key: "llama.context_length"    â”‚ â”‚
â”‚  â”‚   type: UINT32                   â”‚ â”‚
â”‚  â”‚   value: 4096                    â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ ... (M-2 more pairs)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Tensor Info (Nä¸ªå¼ é‡çš„å…ƒä¿¡æ¯)     â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Tensor 1:                        â”‚ â”‚
â”‚  â”‚   name: "token_embd.weight"      â”‚ â”‚
â”‚  â”‚   n_dims: 2                      â”‚ â”‚
â”‚  â”‚   dims: [4096, 32000]            â”‚ â”‚
â”‚  â”‚   type: Q4_K                     â”‚ â”‚
â”‚  â”‚   offset: 1048576                â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Tensor 2:                        â”‚ â”‚
â”‚  â”‚   name: "blk.0.attn_q.weight"    â”‚ â”‚
â”‚  â”‚   ...                            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Alignment Padding                â”‚ â”‚
â”‚  â”‚ (å¯¹é½åˆ° 32/64 å­—èŠ‚)               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Tensor Data (å®é™…æƒé‡æ•°æ®)        â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ [Token Embedding Data]           â”‚ â”‚
â”‚  â”‚ Size: 4096 * 32000 * 0.547å­—èŠ‚   â”‚ â”‚
â”‚  â”‚ (Q4_K æ ¼å¼)                      â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ [Layer 0 Q Weight Data]          â”‚ â”‚
â”‚  â”‚ ...                              â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ [More tensor data...]            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. æ–‡ä»¶å¤´è§£æ

### 3.1 å¤´éƒ¨ç»“æ„

```c
// ä½ç½®ï¼šggml/include/gguf.h:48
#define GGUF_MAGIC 0x46554747  // "GGUF" in little endian
#define GGUF_VERSION 3

struct gguf_header {
    uint32_t magic;             // é­”æ•° GGUF_MAGIC
    uint32_t version;           // ç‰ˆæœ¬å·ï¼ˆå½“å‰ä¸º 3ï¼‰
    uint64_t n_tensors;         // å¼ é‡æ•°é‡
    uint64_t n_kv;              // å…ƒæ•°æ®é”®å€¼å¯¹æ•°é‡
};
```

### 3.2 è¯»å–æ–‡ä»¶å¤´

```c
// ä½ç½®ï¼šggml/src/gguf.c:1523
struct gguf_context * gguf_init_from_file(const char * fname, struct gguf_init_params params) {
    FILE * file = fopen(fname, "rb");
    if (!file) {
        return NULL;
    }

    // è¯»å–æ–‡ä»¶å¤´
    struct gguf_header header;
    fread(&header, sizeof(header), 1, file);

    // éªŒè¯é­”æ•°
    if (header.magic != GGUF_MAGIC) {
        fprintf(stderr, "Invalid GGUF magic: 0x%08x\n", header.magic);
        fclose(file);
        return NULL;
    }

    // æ£€æŸ¥ç‰ˆæœ¬
    if (header.version != GGUF_VERSION) {
        fprintf(stderr, "Unsupported GGUF version: %u\n", header.version);
        fclose(file);
        return NULL;
    }

    printf("GGUF file: %llu tensors, %llu metadata\n",
           header.n_tensors, header.n_kv);

    // ... ç»§ç»­è§£æ
}
```

## 4. å…ƒæ•°æ®ç³»ç»Ÿ

### 4.1 å…ƒæ•°æ®ç±»å‹

```c
// ä½ç½®ï¼šggml/include/gguf.h:28
enum gguf_type {
    GGUF_TYPE_UINT8   = 0,
    GGUF_TYPE_INT8    = 1,
    GGUF_TYPE_UINT16  = 2,
    GGUF_TYPE_INT16   = 3,
    GGUF_TYPE_UINT32  = 4,
    GGUF_TYPE_INT32   = 5,
    GGUF_TYPE_FLOAT32 = 6,
    GGUF_TYPE_BOOL    = 7,
    GGUF_TYPE_STRING  = 8,
    GGUF_TYPE_ARRAY   = 9,
    GGUF_TYPE_UINT64  = 10,
    GGUF_TYPE_INT64   = 11,
    GGUF_TYPE_FLOAT64 = 12,
};
```

### 4.2 é”®å€¼å¯¹ç»“æ„

```c
// æ¯ä¸ªå…ƒæ•°æ®æ¡ç›®çš„ç»“æ„ï¼ˆå†…å­˜ä¸­ï¼‰
struct gguf_kv {
    char * key;              // é”®åï¼ˆå¦‚ "llama.context_length"ï¼‰
    enum gguf_type type;     // å€¼ç±»å‹
    union {
        uint8_t   uint8;
        int8_t    int8;
        uint16_t  uint16;
        int16_t   int16;
        uint32_t  uint32;
        int32_t   int32;
        float     float32;
        uint64_t  uint64;
        int64_t   int64;
        double    float64;
        bool      bool_;
        struct {
            char * data;
            uint64_t len;
        } str;
        struct {
            enum gguf_type type;
            uint64_t n;
            void * data;
        } arr;
    } value;
};
```

### 4.3 å¸¸è§å…ƒæ•°æ®é”®

```python
# é€šç”¨å…ƒæ•°æ®
"general.architecture"       # æ¨¡å‹æ¶æ„ ("llama", "mistral", ...)
"general.name"               # æ¨¡å‹åç§°
"general.author"             # ä½œè€…
"general.version"            # ç‰ˆæœ¬
"general.file_type"          # æ–‡ä»¶ç±»å‹ï¼ˆé‡åŒ–çº§åˆ«ï¼‰
"general.quantization_version" # é‡åŒ–ç‰ˆæœ¬

# LLaMA ç‰¹å®šå…ƒæ•°æ®
"llama.context_length"       # ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆå¦‚ 4096ï¼‰
"llama.embedding_length"     # åµŒå…¥ç»´åº¦ï¼ˆå¦‚ 4096ï¼‰
"llama.block_count"          # å±‚æ•°ï¼ˆå¦‚ 32ï¼‰
"llama.feed_forward_length"  # FFN éšè—å¤§å°ï¼ˆå¦‚ 11008ï¼‰
"llama.attention.head_count" # æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¦‚ 32ï¼‰
"llama.attention.head_count_kv" # KV å¤´æ•°ï¼ˆMQA/GQAï¼‰
"llama.rope.dimension_count" # RoPE ç»´åº¦
"llama.rope.freq_base"       # RoPE é¢‘ç‡åŸºæ•°ï¼ˆå¦‚ 10000.0ï¼‰

# è¯è¡¨å…ƒæ•°æ®
"tokenizer.ggml.model"       # tokenizer ç±»å‹
"tokenizer.ggml.tokens"      # token åˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰
"tokenizer.ggml.scores"      # token å¾—åˆ†ï¼ˆæ•°ç»„ï¼‰
"tokenizer.ggml.token_type"  # token ç±»å‹ï¼ˆæ•°ç»„ï¼‰
```

### 4.4 è§£æå…ƒæ•°æ®

```c
// ä½ç½®ï¼šggml/src/gguf.c:1623
static void gguf_read_kv(struct gguf_context * ctx, FILE * file) {
    for (uint64_t i = 0; i < ctx->header.n_kv; i++) {
        struct gguf_kv * kv = &ctx->kv[i];

        // è¯»å–é”®å
        uint64_t key_len;
        fread(&key_len, sizeof(key_len), 1, file);
        kv->key = malloc(key_len + 1);
        fread(kv->key, 1, key_len, file);
        kv->key[key_len] = '\0';

        // è¯»å–å€¼ç±»å‹
        fread(&kv->type, sizeof(kv->type), 1, file);

        // æ ¹æ®ç±»å‹è¯»å–å€¼
        switch (kv->type) {
            case GGUF_TYPE_UINT32:
                fread(&kv->value.uint32, sizeof(uint32_t), 1, file);
                break;

            case GGUF_TYPE_FLOAT32:
                fread(&kv->value.float32, sizeof(float), 1, file);
                break;

            case GGUF_TYPE_STRING: {
                uint64_t str_len;
                fread(&str_len, sizeof(str_len), 1, file);
                kv->value.str.data = malloc(str_len + 1);
                fread(kv->value.str.data, 1, str_len, file);
                kv->value.str.data[str_len] = '\0';
                kv->value.str.len = str_len;
                break;
            }

            case GGUF_TYPE_ARRAY: {
                fread(&kv->value.arr.type, sizeof(enum gguf_type), 1, file);
                fread(&kv->value.arr.n, sizeof(uint64_t), 1, file);
                // è¯»å–æ•°ç»„æ•°æ®...
                break;
            }

            // ... å…¶ä»–ç±»å‹
        }
    }
}
```

## 5. å¼ é‡ä¿¡æ¯è§£æ

### 5.1 å¼ é‡å…ƒä¿¡æ¯ç»“æ„

```c
// ä½ç½®ï¼šggml/src/gguf.c
struct gguf_tensor_info {
    char * name;                    // å¼ é‡åç§°
    uint32_t n_dims;                // ç»´åº¦æ•°ï¼ˆ1-4ï¼‰
    uint64_t ne[GGML_MAX_DIMS];     // æ¯ä¸ªç»´åº¦çš„å¤§å°
    enum ggml_type type;            // æ•°æ®ç±»å‹ï¼ˆF32/Q4_K/...ï¼‰
    uint64_t offset;                // åœ¨æ–‡ä»¶ä¸­çš„åç§»é‡
    void * data;                    // æŒ‡å‘å®é™…æ•°æ®ï¼ˆmmapåï¼‰
    size_t size;                    // æ•°æ®å¤§å°ï¼ˆå­—èŠ‚ï¼‰
};
```

### 5.2 è§£æå¼ é‡ä¿¡æ¯

```c
// ä½ç½®ï¼šggml/src/gguf.c:1712
static void gguf_read_tensor_info(struct gguf_context * ctx, FILE * file) {
    for (uint64_t i = 0; i < ctx->header.n_tensors; i++) {
        struct gguf_tensor_info * info = &ctx->infos[i];

        // è¯»å–å¼ é‡åç§°
        uint64_t name_len;
        fread(&name_len, sizeof(name_len), 1, file);
        info->name = malloc(name_len + 1);
        fread(info->name, 1, name_len, file);
        info->name[name_len] = '\0';

        // è¯»å–ç»´åº¦æ•°
        fread(&info->n_dims, sizeof(uint32_t), 1, file);

        // è¯»å–æ¯ä¸ªç»´åº¦çš„å¤§å°
        for (uint32_t j = 0; j < info->n_dims; j++) {
            fread(&info->ne[j], sizeof(uint64_t), 1, file);
        }

        // è¯»å–æ•°æ®ç±»å‹
        fread(&info->type, sizeof(enum ggml_type), 1, file);

        // è¯»å–æ•°æ®åç§»
        fread(&info->offset, sizeof(uint64_t), 1, file);

        // è®¡ç®—æ•°æ®å¤§å°
        size_t type_size = ggml_type_size(info->type);
        info->size = ggml_row_size(info->type, info->ne[0]);
        for (uint32_t j = 1; j < info->n_dims; j++) {
            info->size *= info->ne[j];
        }

        printf("Tensor %llu: %s [", i, info->name);
        for (uint32_t j = 0; j < info->n_dims; j++) {
            printf("%llu%s", info->ne[j], j < info->n_dims-1 ? ", " : "");
        }
        printf("], type=%s, size=%zu bytes\n",
               ggml_type_name(info->type), info->size);
    }
}
```

## 6. å¼ é‡æ•°æ®åŠ è½½

### 6.1 æ•°æ®å¯¹é½

GGUF è¦æ±‚å¼ é‡æ•°æ®å¯¹é½åˆ° 32 å­—èŠ‚ï¼ˆæŸäº›å¹³å° 64 å­—èŠ‚ï¼‰ï¼š

```c
// ä½ç½®ï¼šsrc/llama-model-loader.cpp:234
static const size_t GGUF_DEFAULT_ALIGNMENT = 32;

// è®¡ç®—å¯¹é½åçš„åç§»
size_t tensor_data_offset = /* å¤´éƒ¨ + å…ƒæ•°æ® + å¼ é‡ä¿¡æ¯å¤§å° */;
tensor_data_offset = (tensor_data_offset + GGUF_DEFAULT_ALIGNMENT - 1)
                     & ~(GGUF_DEFAULT_ALIGNMENT - 1);
```

### 6.2 ä½¿ç”¨ mmap åŠ è½½

```c
// ä½ç½®ï¼šsrc/llama-mmap.cpp:347
struct llama_mmap {
    void * addr;      // æ˜ å°„åœ°å€
    size_t size;      // æ˜ å°„å¤§å°

    llama_mmap(const char * fname, size_t prefetch = 0) {
        int fd = open(fname, O_RDONLY);
        if (fd < 0) {
            throw std::runtime_error("cannot open file");
        }

        struct stat st;
        fstat(fd, &st);
        size = st.st_size;

        // å†…å­˜æ˜ å°„
        addr = mmap(NULL, size, PROT_READ, MAP_SHARED, fd, 0);
        if (addr == MAP_FAILED) {
            close(fd);
            throw std::runtime_error("mmap failed");
        }

        // å»ºè®®é¢„å–ï¼ˆå¯é€‰ï¼‰
        if (prefetch > 0) {
            madvise(addr, size, MADV_WILLNEED);
        }

        close(fd);
    }

    ~llama_mmap() {
        munmap(addr, size);
    }
};
```

### 6.3 è®¿é—®å¼ é‡æ•°æ®

```c
// ä½ç½®ï¼šsrc/llama-model-loader.cpp:587
void * llama_model_loader::get_tensor_data(const char * name) {
    // æŸ¥æ‰¾å¼ é‡ä¿¡æ¯
    struct gguf_tensor_info * info = gguf_find_tensor(ctx_gguf, name);
    if (!info) {
        return NULL;
    }

    // è®¡ç®—åœ¨æ–‡ä»¶ä¸­çš„å®é™…åœ°å€
    uint8_t * data = (uint8_t *) mmap->addr + tensor_data_offset + info->offset;

    return data;
}
```

## 7. å®æˆ˜ï¼šè§£æ GGUF æ–‡ä»¶

### 7.1 ç®€å•çš„ GGUF è¯»å–å·¥å…·

```c
#include "ggml.h"
#include "gguf.h"
#include <stdio.h>

void print_gguf_info(const char * fname) {
    // åˆå§‹åŒ– GGUF ä¸Šä¸‹æ–‡
    struct gguf_init_params params = {
        .no_alloc = true,   // ä¸åˆ†é…å†…å­˜ï¼Œåªè¯»å–å…ƒæ•°æ®
    };
    struct gguf_context * ctx = gguf_init_from_file(fname, params);
    if (!ctx) {
        printf("Failed to open %s\n", fname);
        return;
    }

    // æ‰“å°æ–‡ä»¶ä¿¡æ¯
    printf("=== GGUF File Info ===\n");
    printf("Version: %u\n", gguf_get_version(ctx));
    printf("Tensors: %zu\n", gguf_get_n_tensors(ctx));
    printf("Metadata pairs: %zu\n", gguf_get_n_kv(ctx));
    printf("\n");

    // æ‰“å°å…³é”®å…ƒæ•°æ®
    printf("=== Key Metadata ===\n");
    const char * arch = gguf_get_val_str(ctx, gguf_find_key(ctx, "general.architecture"));
    printf("Architecture: %s\n", arch);

    int n_layers = gguf_get_val_u32(ctx, gguf_find_key(ctx, "llama.block_count"));
    int n_embd = gguf_get_val_u32(ctx, gguf_find_key(ctx, "llama.embedding_length"));
    int n_heads = gguf_get_val_u32(ctx, gguf_find_key(ctx, "llama.attention.head_count"));
    printf("Layers: %d\n", n_layers);
    printf("Embedding dim: %d\n", n_embd);
    printf("Attention heads: %d\n", n_heads);
    printf("\n");

    // æ‰“å°å¼ é‡åˆ—è¡¨ï¼ˆå‰10ä¸ªï¼‰
    printf("=== Tensor List (first 10) ===\n");
    int n_tensors = gguf_get_n_tensors(ctx);
    for (int i = 0; i < n_tensors && i < 10; i++) {
        const char * name = gguf_get_tensor_name(ctx, i);
        enum ggml_type type = gguf_get_tensor_type(ctx, i);

        int n_dims = gguf_get_tensor_n_dims(ctx, i);
        printf("[%d] %s: ", i, name);

        for (int j = 0; j < n_dims; j++) {
            int64_t dim = gguf_get_tensor_size(ctx, i, j);
            printf("%lld%s", dim, j < n_dims-1 ? " x " : "");
        }
        printf(", type=%s\n", ggml_type_name(type));
    }
    if (n_tensors > 10) {
        printf("... (%d more tensors)\n", n_tensors - 10);
    }

    // æ¸…ç†
    gguf_free(ctx);
}

int main(int argc, char ** argv) {
    if (argc < 2) {
        printf("Usage: %s <model.gguf>\n", argv[0]);
        return 1;
    }

    print_gguf_info(argv[1]);
    return 0;
}
```

ç¼–è¯‘è¿è¡Œï¼š
```bash
gcc -O3 -o gguf_info gguf_info.c \
    -I./ggml/include \
    -L./build/ggml/src -lggml

./gguf_info model.gguf
```

### 7.2 ä½¿ç”¨ Python è§£æ

llama.cpp æä¾›äº† Python å·¥å…·ï¼š

```bash
# æ‰“å° GGUF æ–‡ä»¶ä¿¡æ¯
python gguf-py/scripts/gguf_dump.py model.gguf

# ä¿®æ”¹å…ƒæ•°æ®
python gguf-py/scripts/gguf_set_metadata.py \
    model.gguf \
    --kv "general.name=My Model"

# è½¬æ¢å­—èŠ‚åº
python gguf-py/scripts/gguf_convert_endian.py \
    model.gguf model-be.gguf
```

## 8. GGUF vs å…¶ä»–æ ¼å¼å¯¹æ¯”

| ç‰¹æ€§ | GGUF | PyTorch (.pt) | SafeTensors | HF (.bin) |
|------|------|---------------|-------------|-----------|
| **è‡ªæè¿°** | âœ… å®Œæ•´ | âŒ éœ€è¦é…ç½®æ–‡ä»¶ | âš ï¸ éƒ¨åˆ† | âŒ éœ€è¦é…ç½®æ–‡ä»¶ |
| **mmap æ”¯æŒ** | âœ… åŸç”Ÿ | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒ | âŒ ä¸æ”¯æŒ |
| **é‡åŒ–æ”¯æŒ** | âœ… åŸç”Ÿ | âŒ éœ€è¦é¢å¤–å·¥å…· | âŒ éœ€è¦é¢å¤–å·¥å…· | âŒ éœ€è¦é¢å¤–å·¥å…· |
| **è·¨å¹³å°** | âœ… ç»Ÿä¸€æ ¼å¼ | âš ï¸ Pythonä¾èµ– | âœ… å¥½ | âš ï¸ Pythonä¾èµ– |
| **æ–‡ä»¶å¤§å°** | å°ï¼ˆé‡åŒ–ï¼‰ | å¤§ï¼ˆå…¨ç²¾åº¦ï¼‰ | å¤§ï¼ˆå…¨ç²¾åº¦ï¼‰ | å¤§ï¼ˆå…¨ç²¾åº¦ï¼‰ |
| **åŠ è½½é€Ÿåº¦** | å¿«ï¼ˆmmapï¼‰ | æ…¢ï¼ˆååºåˆ—åŒ–ï¼‰ | ä¸­ç­‰ | æ…¢ï¼ˆååºåˆ—åŒ–ï¼‰ |

## 9. æ€»ç»“

ä»Šå¤©æˆ‘ä»¬æ·±å…¥å­¦ä¹ äº† GGUF æ–‡ä»¶æ ¼å¼ï¼š

âœ… **æ–‡ä»¶ç»“æ„**ï¼šå¤´éƒ¨ â†’ å…ƒæ•°æ® â†’ å¼ é‡ä¿¡æ¯ â†’ å¼ é‡æ•°æ®
âœ… **å…ƒæ•°æ®ç³»ç»Ÿ**ï¼šçµæ´»çš„é”®å€¼å¯¹ç³»ç»Ÿ
âœ… **å¼ é‡å­˜å‚¨**ï¼šé«˜æ•ˆçš„æ•°æ®å¯¹é½å’Œ mmap æ”¯æŒ
âœ… **å®æˆ˜å·¥å…·**ï¼šC å’Œ Python çš„è§£æç¤ºä¾‹

### å…³é”®è¦ç‚¹

1. **GGUF æ˜¯è‡ªæè¿°çš„**ï¼šåŒ…å«å®Œæ•´çš„æ¨¡å‹ä¿¡æ¯
2. **æ”¯æŒ mmap**ï¼šå¿«é€ŸåŠ è½½ï¼ŒèŠ‚çœå†…å­˜
3. **å…ƒæ•°æ®é©±åŠ¨**ï¼šæ¶æ„ä¿¡æ¯ä»æ–‡ä»¶ä¸­è¯»å–
4. **é‡åŒ–å‹å¥½**ï¼šç›´æ¥å­˜å‚¨é‡åŒ–æƒé‡

## ä¸‹ä¸€æ­¥

æ˜å¤©æˆ‘ä»¬å°†å­¦ä¹  **Day 6: æ¨¡å‹åŠ è½½ä¸æƒé‡ç®¡ç†**ï¼š
- ä» GGUF åˆ° llama_model
- æƒé‡æ˜ å°„ç­–ç•¥
- å¤š GPU åˆ†é…
- å†…å­˜ä¼˜åŒ–æŠ€å·§

---

**ç»ƒä¹ **ï¼š
1. ä½¿ç”¨ç¤ºä¾‹ä»£ç è§£æä¸€ä¸ª GGUF æ–‡ä»¶
2. ç»Ÿè®¡ä¸åŒé‡åŒ–ç±»å‹çš„å¼ é‡å æ¯”
3. è®¡ç®—æ¨¡å‹çš„æ€»å‚æ•°é‡å’Œå†…å­˜å ç”¨

ğŸ“š [Day 6: æ¨¡å‹åŠ è½½ä¸æƒé‡ç®¡ç†](day06-model-loading.md)
