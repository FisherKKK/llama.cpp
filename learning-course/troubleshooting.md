# æ•…éšœæ’æŸ¥æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©ä½ å¿«é€Ÿå®šä½å’Œè§£å†³ llama.cpp ä½¿ç”¨ä¸­çš„å¸¸è§é—®é¢˜ã€‚

---

## ğŸ” è¯Šæ–­æµç¨‹

### ç¬¬ä¸€æ­¥ï¼šç¡®å®šé—®é¢˜ç±»åˆ«

```
â”Œâ”€ ç¼–è¯‘/å®‰è£…é—®é¢˜ â†’ ç¬¬ 1 èŠ‚
â”œâ”€ åŠ è½½/å¯åŠ¨é—®é¢˜ â†’ ç¬¬ 2 èŠ‚
â”œâ”€ æ€§èƒ½é—®é¢˜ â†’ ç¬¬ 3 èŠ‚
â”œâ”€ å†…å­˜é—®é¢˜ â†’ ç¬¬ 4 èŠ‚
â”œâ”€ è¾“å‡ºè´¨é‡é—®é¢˜ â†’ ç¬¬ 5 èŠ‚
â””â”€ GPU ç›¸å…³é—®é¢˜ â†’ ç¬¬ 6 èŠ‚
```

---

## 1. ç¼–è¯‘/å®‰è£…é—®é¢˜

### é—®é¢˜ï¼šæ‰¾ä¸åˆ° CUDA

**ç—‡çŠ¶**ï¼š
```
CMake Error: CUDA not found
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥ CUDA å®‰è£…
nvcc --version
which nvcc

# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $CUDA_HOME
echo $PATH | grep cuda
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å®‰è£… CUDA Toolkit
# Ubuntu:
sudo apt install nvidia-cuda-toolkit

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_HOME=/usr/local/cuda
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# é‡æ–°ç¼–è¯‘
rm -rf build
cmake -B build -DGGML_CUDA=ON
cmake --build build --config Release
```

### é—®é¢˜ï¼šç¼–è¯‘é”™è¯¯ "undefined reference"

**ç—‡çŠ¶**ï¼š
```
undefined reference to `llama_model_load_from_file'
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥é“¾æ¥åº“
ldd ./build/bin/llama-cli | grep llama
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å®Œå…¨æ¸…ç†é‡å»º
rm -rf build
cmake -B build
cmake --build build --config Release -j

# å¦‚æœä»ç„¶å¤±è´¥ï¼Œæ£€æŸ¥ CMakeLists.txt
```

### é—®é¢˜ï¼šMetal åœ¨ macOS ä¸Šä¸å·¥ä½œ

**ç—‡çŠ¶**ï¼š
```
ggml_metal_init: error: failed to load Metal library
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥ Xcode Command Line Tools
xcode-select -p

# æ£€æŸ¥ Metal æ–‡ä»¶
ls ggml/src/ggml-metal/ggml-metal.metal
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å®‰è£… Xcode Command Line Tools
xcode-select --install

# é‡æ–°ç¼–è¯‘
cmake -B build -DGGML_METAL=ON
cmake --build build --config Release
```

---

## 2. åŠ è½½/å¯åŠ¨é—®é¢˜

### é—®é¢˜ï¼šæ¨¡å‹åŠ è½½å¤±è´¥

**ç—‡çŠ¶**ï¼š
```
error: failed to load model from 'model.gguf'
llama_model_load: failed to load model
```

**è¯Šæ–­æ­¥éª¤**ï¼š

1. **æ£€æŸ¥æ–‡ä»¶å­˜åœ¨**
```bash
ls -lh model.gguf
file model.gguf
```

2. **éªŒè¯æ–‡ä»¶å®Œæ•´æ€§**
```bash
# æ£€æŸ¥æ–‡ä»¶å¤´
hexdump -C model.gguf | head -20
# åº”è¯¥çœ‹åˆ° "GGUF" (47 47 55 46)

# æˆ–ä½¿ç”¨ Python å·¥å…·
python gguf-py/scripts/gguf_dump.py model.gguf | head -50
```

3. **æ£€æŸ¥æƒé™**
```bash
chmod 644 model.gguf
```

**å¸¸è§åŸå› ä¸è§£å†³**ï¼š

| åŸå›  | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|---------|
| æ–‡ä»¶æŸå | åŠ è½½åˆ°ä¸€åŠå¤±è´¥ | é‡æ–°ä¸‹è½½ |
| æ ¼å¼ä¸å…¼å®¹ | é­”æ•°é”™è¯¯ | ä½¿ç”¨æ–°ç‰ˆ llama.cpp |
| æƒé™ä¸è¶³ | Permission denied | `chmod 644` |
| ç£ç›˜å·²æ»¡ | mmap å¤±è´¥ | æ¸…ç†ç©ºé—´ |

### é—®é¢˜ï¼šä¸Šä¸‹æ–‡åˆ›å»ºå¤±è´¥

**ç—‡çŠ¶**ï¼š
```
llama_new_context_with_model: failed to create context
```

**è¯Šæ–­**ï¼š
```cpp
// æ£€æŸ¥è¿”å›å€¼
llama_context * ctx = llama_new_context_with_model(model, ctx_params);
if (ctx == NULL) {
    fprintf(stderr, "Failed to create context\n");
    // æ£€æŸ¥å…·ä½“åŸå› 
}
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å‡å°‘å†…å­˜éœ€æ±‚
./llama-cli -m model.gguf -c 1024  # å‡å°ä¸Šä¸‹æ–‡
./llama-cli -m model.gguf -ngl 16  # éƒ¨åˆ† GPU
```

---

## 3. æ€§èƒ½é—®é¢˜

### é—®é¢˜ï¼šæ¨ç†é€Ÿåº¦æ…¢

**ç—‡çŠ¶**ï¼š
```
ç”Ÿæˆé€Ÿåº¦ < 5 tokens/sï¼ˆ7B æ¨¡å‹åœ¨ç°ä»£ç¡¬ä»¶ä¸Šï¼‰
```

**è¯Šæ–­æ¸…å•**ï¼š

```bash
# 1. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ GPU
./llama-cli -m model.gguf -ngl 1 -p "test" -n 10 2>&1 | grep -i "cuda\|metal"

# 2. æ£€æŸ¥ GPU åˆ©ç”¨ç‡
nvidia-smi  # æˆ– sudo powermetricsï¼ˆmacOSï¼‰

# 3. è¿è¡ŒåŸºå‡†æµ‹è¯•
./llama-bench -m model.gguf -p 512 -n 128 -ngl 32

# 4. æ£€æŸ¥ CPU åˆ©ç”¨ç‡
htop  # æŸ¥çœ‹çº¿ç¨‹æ˜¯å¦å……åˆ†åˆ©ç”¨
```

**ä¼˜åŒ–æ­¥éª¤**ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š

1. **å¯ç”¨ GPU å…¨å±‚ offload**
```bash
./llama-cli -m model.gguf -ngl -1
```

2. **ä½¿ç”¨é‡åŒ–æ¨¡å‹**
```bash
./llama-quantize model-f16.gguf model-q4.gguf Q4_K_M
./llama-cli -m model-q4.gguf
```

3. **å¢åŠ æ‰¹å¤§å°**
```bash
./llama-cli -m model.gguf -b 512 -ub 512
```

4. **è°ƒæ•´çº¿ç¨‹æ•°**
```bash
# è®¾ç½®ä¸ºç‰©ç†æ ¸å¿ƒæ•°
./llama-cli -m model.gguf -t $(nproc)
```

5. **å¯ç”¨ç¼–è¯‘ä¼˜åŒ–**
```bash
cmake -B build -DCMAKE_BUILD_TYPE=Release -DGGML_NATIVE=ON
```

### é—®é¢˜ï¼šæç¤ºå¤„ç†æ…¢

**ç—‡çŠ¶**ï¼š
```
Prompt processing: 5 tokens/sï¼ˆåº”è¯¥ > 100 t/sï¼‰
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥æ‰¹å¤§å°
./llama-cli -m model.gguf -p "long prompt..." -n 1 --verbose-prompt
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å¢åŠ æ‰¹å¤§å°
./llama-cli -m model.gguf -b 2048 -ub 512

# æˆ–åœ¨ä»£ç ä¸­
ctx_params.n_batch = 2048;
ctx_params.n_ubatch = 512;
```

### é—®é¢˜ï¼šGPU æœªå……åˆ†åˆ©ç”¨

**ç—‡çŠ¶**ï¼š
```
nvidia-smi æ˜¾ç¤º GPU åˆ©ç”¨ç‡ < 50%
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥é…ç½®
./llama-cli -m model.gguf -ngl -1 --verbose-prompt 2>&1 | grep "offload"
```

**å¯èƒ½åŸå› **ï¼š

1. **æœªå…¨å±‚ offload**
```bash
# è§£å†³ï¼šä½¿ç”¨ -ngl -1
./llama-cli -m model.gguf -ngl -1
```

2. **æ‰¹å¤§å°å¤ªå°**
```bash
# è§£å†³ï¼šå¢åŠ æ‰¹å¤§å°
./llama-cli -m model.gguf -ngl -1 -b 512
```

3. **æœªå¯ç”¨ Flash Attention**
```cpp
// è§£å†³ï¼šåœ¨ä»£ç ä¸­å¯ç”¨
ctx_params.flash_attn_type = LLAMA_FLASH_ATTN_TYPE_ENABLED;
```

---

## 4. å†…å­˜é—®é¢˜

### é—®é¢˜ï¼šCUDA Out of Memory

**ç—‡çŠ¶**ï¼š
```
CUDA error: out of memory
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥å¯ç”¨å†…å­˜
nvidia-smi --query-gpu=memory.free --format=csv

# ä¼°ç®—éœ€æ±‚
# 7B Q4_K: ~4GB æ¨¡å‹ + ~500MB KVç¼“å­˜(ctx=2048) = ~4.5GB
```

**è§£å†³æ–¹æ¡ˆï¼ˆä¼˜å…ˆçº§æ’åºï¼‰**ï¼š

1. **å‡å°‘ GPU å±‚æ•°**
```bash
./llama-cli -m model.gguf -ngl 24  # è€Œä¸æ˜¯ -1
```

2. **å‡å°‘ä¸Šä¸‹æ–‡**
```bash
./llama-cli -m model.gguf -ngl -1 -c 1024  # è€Œä¸æ˜¯ 4096
```

3. **ä½¿ç”¨æ›´æ¿€è¿›çš„é‡åŒ–**
```bash
# Q3_K_M æ¯” Q4_K_M çœ 25%
./llama-quantize model.gguf model-q3.gguf Q3_K_M
```

4. **ä½¿ç”¨ Q8_0 KV ç¼“å­˜**ï¼ˆæŸå¤±è´¨é‡ï¼‰
```cpp
ctx_params.type_k = GGML_TYPE_Q8_0;
ctx_params.type_v = GGML_TYPE_Q8_0;
```

### é—®é¢˜ï¼šç³»ç»Ÿå†…å­˜ä¸è¶³

**ç—‡çŠ¶**ï¼š
```
std::bad_alloc
æˆ–ç³»ç»Ÿ OOM killer æ€æ­»è¿›ç¨‹
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥å¯ç”¨å†…å­˜
free -h

# ç›‘æ§å†…å­˜ä½¿ç”¨
watch -n 1 'free -h; ps aux | grep llama'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

1. **å¯ç”¨ mmap**ï¼ˆé»˜è®¤ï¼‰
```cpp
model_params.use_mmap = true;
```

2. **ä¸ä½¿ç”¨ mlock**ï¼ˆé»˜è®¤ï¼‰
```cpp
model_params.use_mlock = false;
```

3. **éƒ¨åˆ† offload åˆ° GPU**
```bash
./llama-cli -m model.gguf -ngl 16  # å°†éƒ¨åˆ†è½¬ç§»åˆ° GPU
```

### é—®é¢˜ï¼šå†…å­˜æ³„æ¼

**ç—‡çŠ¶**ï¼š
```
å†…å­˜ä½¿ç”¨æŒç»­å¢é•¿
```

**è¯Šæ–­**ï¼š
```bash
# ä½¿ç”¨ valgrind
valgrind --leak-check=full --show-leak-kinds=all \
  ./build/bin/llama-cli -m model.gguf -p "test" -n 10
```

**å¸¸è§åŸå› **ï¼š
- æœªé‡Šæ”¾ context
- æœªé‡Šæ”¾ model
- æœªé‡Šæ”¾ sampler
- æœªé‡Šæ”¾ batch

**è§£å†³æ–¹æ¡ˆ**ï¼š
```cpp
// ç¡®ä¿æ¸…ç†èµ„æº
llama_sampler_free(sampler);
llama_free(ctx);
llama_free_model(model);
llama_backend_free();
```

---

## 5. è¾“å‡ºè´¨é‡é—®é¢˜

### é—®é¢˜ï¼šè¾“å‡ºé‡å¤

**ç—‡çŠ¶**ï¼š
```
"The cat is cute. The cat is cute. The cat is cute..."
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å¢åŠ é‡å¤æƒ©ç½š
./llama-cli -m model.gguf --repeat-penalty 1.2

# è°ƒæ•´é‡‡æ ·å‚æ•°
./llama-cli -m model.gguf \
  --repeat-penalty 1.2 \
  --frequency-penalty 0.5 \
  --presence-penalty 0.5
```

### é—®é¢˜ï¼šè¾“å‡ºæ··ä¹±/æ— æ„ä¹‰

**ç—‡çŠ¶**ï¼š
```
"asdf jkl; qwer..."ï¼ˆä¹±ç ï¼‰
```

**å¯èƒ½åŸå› **ï¼š

1. **æ¸©åº¦å¤ªé«˜**
```bash
# è§£å†³ï¼šé™ä½æ¸©åº¦
./llama-cli -m model.gguf --temp 0.7  # æˆ–æ›´ä½
```

2. **é‡åŒ–è¿‡åº¦**
```bash
# è§£å†³ï¼šä½¿ç”¨æ›´é«˜è´¨é‡é‡åŒ–
# Q2_K â†’ Q4_K_M â†’ Q6_K
```

3. **tokenizer é”™è¯¯**
```bash
# è¯Šæ–­ï¼šæ£€æŸ¥æ¨¡å‹å…ƒæ•°æ®
python gguf-py/scripts/gguf_dump.py model.gguf | grep tokenizer
```

### é—®é¢˜ï¼šè¾“å‡ºæˆªæ–­

**ç—‡çŠ¶**ï¼š
```
è¾“å‡ºæå‰ç»“æŸï¼Œæœªè¾¾åˆ° max_tokens
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥ EOS token
./llama-cli -m model.gguf -p "test" -n 100 --log-disable 2>&1 | grep -i "eos"
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```cpp
// æ£€æŸ¥æ˜¯å¦é‡åˆ° EOS
if (llama_token_is_eog(model, next_token)) {
    fprintf(stderr, "Hit EOS token\n");
    break;
}

// æˆ–å¿½ç•¥ EOSï¼ˆå°å¿ƒä½¿ç”¨ï¼‰
if (next_token == llama_token_eos(model) && i < max_tokens - 1) {
    continue;  // è·³è¿‡ EOS
}
```

---

## 6. GPU ç›¸å…³é—®é¢˜

### é—®é¢˜ï¼šGPU æœªè¢«è¯†åˆ«

**ç—‡çŠ¶**ï¼š
```
no CUDA-capable device is detected
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥ GPU
nvidia-smi
lspci | grep -i nvidia

# æ£€æŸ¥é©±åŠ¨
cat /proc/driver/nvidia/version

# æ£€æŸ¥ CUDA
nvcc --version
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# å®‰è£…/æ›´æ–°é©±åŠ¨
sudo ubuntu-drivers autoinstall

# æˆ–æ‰‹åŠ¨å®‰è£… NVIDIA é©±åŠ¨
# https://www.nvidia.com/Download/index.aspx
```

### é—®é¢˜ï¼šCUDA ç‰ˆæœ¬ä¸åŒ¹é…

**ç—‡çŠ¶**ï¼š
```
CUDA driver version is insufficient for CUDA runtime version
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥ç‰ˆæœ¬
nvidia-smi  # Driver version
nvcc --version  # Runtime version
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
- å‡çº§é©±åŠ¨ä»¥åŒ¹é… CUDA runtime
- æˆ–é™çº§ CUDA runtime ä»¥åŒ¹é…é©±åŠ¨

### é—®é¢˜ï¼šå¤š GPU ä¸å·¥ä½œ

**ç—‡çŠ¶**ï¼š
```
åªæœ‰ä¸€ä¸ª GPU è¢«ä½¿ç”¨
```

**è¯Šæ–­**ï¼š
```bash
# æ£€æŸ¥å¯è§ GPU
echo $CUDA_VISIBLE_DEVICES
nvidia-smi -L

# ç›‘æ§æ‰€æœ‰ GPU
watch -n 1 nvidia-smi
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```bash
# è®¾ç½®å¯è§è®¾å¤‡
export CUDA_VISIBLE_DEVICES=0,1,2,3

# ä½¿ç”¨åˆ†å±‚æ¨¡å¼
./llama-cli -m model.gguf -ngl -1 -sm layer

# éªŒè¯
./llama-cli -m model.gguf -ngl -1 --verbose-prompt 2>&1 | grep "split"
```

---

## 7. è°ƒè¯•æŠ€å·§

### å¯ç”¨è¯¦ç»†æ—¥å¿—

```bash
# ç¯å¢ƒå˜é‡
export LLAMA_LOG_LEVEL=debug
export LLAMA_LOG_FILE=debug.log

# è¿è¡Œ
./llama-cli -m model.gguf -p "test" -n 10

# æŸ¥çœ‹æ—¥å¿—
cat debug.log
```

### ä½¿ç”¨ GDB è°ƒè¯•

```bash
# ç¼–è¯‘è°ƒè¯•ç‰ˆæœ¬
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build

# è¿è¡Œ GDB
gdb --args ./build/bin/llama-cli -m model.gguf -p "test"

# GDB å‘½ä»¤
(gdb) run
(gdb) backtrace  # æŸ¥çœ‹è°ƒç”¨æ ˆ
(gdb) print variable  # æ‰“å°å˜é‡
(gdb) continue
```

### æ€§èƒ½åˆ†æ

```bash
# Linux: perf
perf record -g ./llama-cli -m model.gguf -p "test" -n 100
perf report

# macOS: Instruments
xcrun xctrace record --template 'Time Profiler' \
  --launch ./llama-cli -- -m model.gguf -p "test" -n 100
```

### å†…å­˜åˆ†æ

```bash
# Valgrind
valgrind --leak-check=full \
  --track-origins=yes \
  ./llama-cli -m model.gguf -p "test" -n 10

# AddressSanitizer
cmake -B build -DCMAKE_BUILD_TYPE=Debug \
  -DCMAKE_CXX_FLAGS="-fsanitize=address"
cmake --build build
./build/bin/llama-cli -m model.gguf -p "test"
```

---

## 8. å¸¸è§é”™è¯¯ä¿¡æ¯

| é”™è¯¯ä¿¡æ¯ | å«ä¹‰ | è§£å†³æ–¹æ¡ˆ |
|---------|------|---------|
| `failed to load model` | æ¨¡å‹æ–‡ä»¶é—®é¢˜ | æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œå®Œæ•´æ€§ |
| `out of memory` | å†…å­˜ä¸è¶³ | å‡å°‘ n_ctx æˆ–ä½¿ç”¨é‡åŒ– |
| `CUDA error` | GPU é”™è¯¯ | æ£€æŸ¥é©±åŠ¨å’Œ CUDA ç‰ˆæœ¬ |
| `invalid model file` | æ ¼å¼ä¸æ”¯æŒ | ä½¿ç”¨å…¼å®¹ç‰ˆæœ¬æˆ–è½¬æ¢ |
| `context size exceeded` | è¶…å‡ºä¸Šä¸‹æ–‡é™åˆ¶ | å¢åŠ  n_ctx æˆ–æˆªæ–­è¾“å…¥ |
| `batch size exceeded` | æ‰¹æ¬¡å¤ªå¤§ | å‡å°‘ batch size |
| `tensor not found` | æƒé‡ç¼ºå¤± | é‡æ–°ä¸‹è½½/è½¬æ¢æ¨¡å‹ |

---

## 9. æ±‚åŠ©æ¸…å•

å½“éœ€è¦å¯»æ±‚å¸®åŠ©æ—¶ï¼Œè¯·æä¾›ï¼š

```markdown
**ç¯å¢ƒä¿¡æ¯**:
- OS: [Ubuntu 22.04 / macOS 14.0 / Windows 11]
- CPU: [i9-13900K]
- GPU: [RTX 4090 24GB / M2 Max / None]
- RAM: [32GB]
- llama.cpp ç‰ˆæœ¬: [commit hash æˆ– release tag]
- CUDAç‰ˆæœ¬: [12.2 / N/A]

**ç¼–è¯‘é€‰é¡¹**:
```bash
cmake -B build -DGGML_CUDA=ON
```

**å‘½ä»¤/ä»£ç **:
```bash
./llama-cli -m model.gguf -ngl -1 -p "test" -n 10
```

**å®Œæ•´é”™è¯¯è¾“å‡º**:
```
[ç²˜è´´å®Œæ•´é”™è¯¯ä¿¡æ¯]
```

**å·²å°è¯•çš„è§£å†³æ–¹æ¡ˆ**:
1. ...
2. ...

**å…¶ä»–ä¿¡æ¯**:
- æ¨¡å‹æ–‡ä»¶: [model.gguf, 4.2GB, Q4_K_M]
- é—®é¢˜æ˜¯å¦å¯å¤ç°: [æ˜¯/å¦]
```

---

## 10. å¿«é€Ÿè¯Šæ–­è„šæœ¬

```bash
#!/bin/bash
# diagnose.sh - llama.cpp è¯Šæ–­è„šæœ¬

echo "=== System Info ==="
uname -a
cat /etc/os-release | grep PRETTY_NAME

echo -e "\n=== CPU Info ==="
lscpu | grep "Model name"
lscpu | grep "CPU(s):"

echo -e "\n=== Memory ==="
free -h

echo -e "\n=== GPU Info ==="
if command -v nvidia-smi &> /dev/null; then
    nvidia-smi --query-gpu=name,driver_version,memory.total --format=csv
else
    echo "No NVIDIA GPU detected"
fi

echo -e "\n=== CUDA Info ==="
if command -v nvcc &> /dev/null; then
    nvcc --version
else
    echo "CUDA not found"
fi

echo -e "\n=== llama.cpp Build Info ==="
./build/bin/llama-cli --version 2>&1 || echo "llama-cli not found"

echo -e "\n=== Model Info ==="
if [ -f "$1" ]; then
    ls -lh "$1"
    python gguf-py/scripts/gguf_dump.py "$1" 2>&1 | head -30
else
    echo "Usage: $0 <model.gguf>"
fi
```

ä½¿ç”¨ï¼š
```bash
chmod +x diagnose.sh
./diagnose.sh model.gguf > diagnostic_report.txt
```

---

**ç›¸å…³èµ„æº**ï¼š
- [FAQ](FAQ.md)
- [å¿«é€Ÿå‚è€ƒ](quick-reference.md)
- [GitHub Issues](https://github.com/ggml-org/llama.cpp/issues)

**æç¤º**ï¼š90% çš„é—®é¢˜éƒ½å¯ä»¥é€šè¿‡æœ¬æŒ‡å—è§£å†³ã€‚å¦‚æœä»æœ‰é—®é¢˜ï¼Œè¯·åœ¨ GitHub æ Issueã€‚
