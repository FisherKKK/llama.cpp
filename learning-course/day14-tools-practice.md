# Day 14: å®æˆ˜é¡¹ç›®ä¸æ€»ç»“

## è¯¾ç¨‹ç›®æ ‡

ç»¼åˆåº”ç”¨æ‰€å­¦çŸ¥è¯†ï¼š
- ä»é›¶æ­å»ºæ¨ç†æœåŠ¡
- llama-server æºç è§£æ
- OpenAI API å…¼å®¹å®ç°
- æ€§èƒ½ä¼˜åŒ–å®è·µ
- è¯¾ç¨‹æ€»ç»“ä¸è¿›é˜¶è·¯çº¿

## 1. ä»é›¶æ­å»ºæ¨ç†æœåŠ¡

### 1.1 æœ€ç®€å•çš„æ¨ç†ç¨‹åº

è®©æˆ‘ä»¬ä»æœ€åŸºç¡€çš„å¼€å§‹ï¼š

```cpp
// simple_inference.cpp
#include "llama.h"
#include <iostream>
#include <string>
#include <vector>

int main(int argc, char ** argv) {
    if (argc < 2) {
        std::cerr << "Usage: " << argv[0] << " <model_path>" << std::endl;
        return 1;
    }

    // 1. åˆå§‹åŒ–åç«¯
    llama_backend_init();

    // 2. åŠ è½½æ¨¡å‹
    llama_model_params model_params = llama_model_default_params();
    model_params.n_gpu_layers = 32;  // ä½¿ç”¨ GPU åŠ é€Ÿ

    llama_model * model = llama_model_load_from_file(argv[1], model_params);
    if (!model) {
        std::cerr << "Failed to load model" << std::endl;
        return 1;
    }

    // 3. åˆ›å»ºä¸Šä¸‹æ–‡
    llama_context_params ctx_params = llama_context_default_params();
    ctx_params.n_ctx = 2048;         // ä¸Šä¸‹æ–‡å¤§å°
    ctx_params.n_batch = 512;        // æ‰¹å¤§å°
    ctx_params.n_threads = 8;        // CPU çº¿ç¨‹æ•°

    llama_context * ctx = llama_new_context_with_model(model, ctx_params);
    if (!ctx) {
        std::cerr << "Failed to create context" << std::endl;
        llama_free_model(model);
        return 1;
    }

    // 4. å‡†å¤‡è¾“å…¥
    std::string prompt = "The capital of France is";
    std::cout << "Prompt: " << prompt << std::endl;

    // 5. Tokenize
    std::vector<llama_token> tokens(prompt.size() + 1);
    int n_tokens = llama_tokenize(
        model,
        prompt.c_str(),
        prompt.size(),
        tokens.data(),
        tokens.size(),
        true,   // add_bos
        false   // special tokens
    );
    tokens.resize(n_tokens);

    std::cout << "Tokens: " << n_tokens << std::endl;

    // 6. åˆ›å»ºé‡‡æ ·å™¨
    llama_sampler_chain_params sampler_params = llama_sampler_chain_default_params();
    sampler_params.temp = 0.7f;
    sampler_params.top_p = 0.9f;
    sampler_params.top_k = 40;

    llama_sampler * sampler = llama_sampler_chain_init(sampler_params);

    // 7. ç”Ÿæˆå¾ªç¯
    std::cout << "Output: " << prompt;

    for (int i = 0; i < 50; i++) {  // ç”Ÿæˆ 50 ä¸ª token
        // å‡†å¤‡ batch
        llama_batch batch = llama_batch_get_one(
            tokens.data(),
            tokens.size(),
            0,  // position
            0   // sequence id
        );

        // å‰å‘ä¼ æ’­
        if (llama_decode(ctx, batch) != 0) {
            std::cerr << "Failed to decode" << std::endl;
            break;
        }

        // è·å– logits
        float * logits = llama_get_logits_ith(ctx, batch.n_tokens - 1);

        // é‡‡æ ·
        llama_token next_token = llama_sampler_sample(sampler, ctx, -1);

        // æ£€æŸ¥ç»“æŸ
        if (llama_token_is_eog(model, next_token)) {
            break;
        }

        // è¾“å‡º
        char piece[256];
        int n = llama_token_to_piece(model, next_token, piece, sizeof(piece), 0, false);
        if (n > 0) {
            std::cout << std::string(piece, n);
            std::cout.flush();
        }

        // æ›´æ–° tokens
        tokens.clear();
        tokens.push_back(next_token);
    }

    std::cout << std::endl;

    // 8. æ¸…ç†
    llama_sampler_free(sampler);
    llama_free(ctx);
    llama_free_model(model);
    llama_backend_free();

    return 0;
}
```

ç¼–è¯‘è¿è¡Œï¼š
```bash
g++ -O3 -o simple_inference simple_inference.cpp \
    -I./include \
    -L./build/src -lllama \
    -lpthread -lm

./simple_inference model.gguf
```

### 1.2 æ·»åŠ æµå¼è¾“å‡º

```cpp
// stream_inference.cpp
// ... (å‰é¢ç›¸åŒ)

// å›è°ƒå‡½æ•°
void on_token_generated(const char * token_str, int len, void * user_data) {
    std::cout << std::string(token_str, len);
    std::cout.flush();
}

// ç”Ÿæˆå¾ªç¯ï¼ˆæµå¼ç‰ˆæœ¬ï¼‰
llama_generate_stream(
    ctx,
    model,
    sampler,
    tokens.data(),
    tokens.size(),
    50,  // max_tokens
    on_token_generated,
    nullptr  // user_data
);
```

## 2. llama-server æºç è§£æ

### 2.1 æœåŠ¡å™¨æ¶æ„

llama-server æ˜¯ä¸€ä¸ªå®Œæ•´çš„ HTTP æœåŠ¡å™¨ï¼Œæä¾› OpenAI å…¼å®¹çš„ APIã€‚

```
llama-server æ¶æ„ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HTTP æœåŠ¡å™¨ (httplib)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è·¯ç”±å±‚                                   â”‚
â”‚  â€¢ /v1/chat/completions                 â”‚
â”‚  â€¢ /v1/completions                      â”‚
â”‚  â€¢ /v1/embeddings                       â”‚
â”‚  â€¢ /health                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¯·æ±‚å¤„ç†å±‚                               â”‚
â”‚  â€¢ JSON è§£æ                            â”‚
â”‚  â€¢ å‚æ•°éªŒè¯                             â”‚
â”‚  â€¢ æµå¼å“åº”ç®¡ç†                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ¨ç†é˜Ÿåˆ—                                 â”‚
â”‚  â€¢ å¤šè¯·æ±‚å¹¶å‘                           â”‚
â”‚  â€¢ æ‰¹å¤„ç†ä¼˜åŒ–                           â”‚
â”‚  â€¢ è¿ç»­æ‰¹å¤„ç† (continuous batching)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llama.cpp æ ¸å¿ƒ                          â”‚
â”‚  â€¢ æ¨¡å‹åŠ è½½                             â”‚
â”‚  â€¢ æ¨ç†æ‰§è¡Œ                             â”‚
â”‚  â€¢ KV ç¼“å­˜ç®¡ç†                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 å…³é”®ä»£ç ç‰‡æ®µ

#### ä¸»å¾ªç¯ï¼ˆä½ç½®ï¼š`tools/server/server.cpp:3421`ï¼‰

```cpp
int main(int argc, char ** argv) {
    // 1. è§£æå‚æ•°
    gpt_params params = parse_server_params(argc, argv);

    // 2. åˆå§‹åŒ–æœåŠ¡å™¨
    llama_server_context ctx_server;
    ctx_server.load_model(params);

    // 3. è®¾ç½®è·¯ç”±
    httplib::Server svr;

    // POST /v1/chat/completions
    svr.Post("/v1/chat/completions", [&](const httplib::Request & req, httplib::Response & res) {
        handle_chat_completions(ctx_server, req, res);
    });

    // POST /v1/completions
    svr.Post("/v1/completions", [&](const httplib::Request & req, httplib::Response & res) {
        handle_completions(ctx_server, req, res);
    });

    // GET /health
    svr.Get("/health", [&](const httplib::Request &, httplib::Response & res) {
        res.set_content(R"({"status":"ok"})", "application/json");
    });

    // 4. å¯åŠ¨æœåŠ¡å™¨
    std::cout << "Server listening on http://localhost:" << params.port << std::endl;
    svr.listen("0.0.0.0", params.port);

    return 0;
}
```

#### Chat Completions å¤„ç†

```cpp
void handle_chat_completions(
    llama_server_context & ctx,
    const httplib::Request & req,
    httplib::Response & res) {

    // 1. è§£æè¯·æ±‚
    json body = json::parse(req.body);

    std::string model = body["model"];
    json messages = body["messages"];
    float temperature = body.value("temperature", 0.7f);
    float top_p = body.value("top_p", 0.9f);
    int max_tokens = body.value("max_tokens", 512);
    bool stream = body.value("stream", false);

    // 2. æ„å»º prompt (apply chat template)
    std::string prompt = ctx.apply_chat_template(messages);

    // 3. Tokenize
    std::vector<llama_token> tokens = ctx.tokenize(prompt, true);

    // 4. åˆ›å»ºä»»åŠ¡
    server_task task;
    task.type = TASK_TYPE_COMPLETION;
    task.prompt_tokens = tokens;
    task.params.temp = temperature;
    task.params.top_p = top_p;
    task.params.n_predict = max_tokens;
    task.stream = stream;

    // 5. æäº¤åˆ°é˜Ÿåˆ—
    ctx.queue_task(task);

    // 6. å¤„ç†å“åº”
    if (stream) {
        // æµå¼å“åº”
        res.set_content_provider(
            "text/event-stream",
            [task_id = task.id, &ctx](size_t offset, httplib::DataSink & sink) {
                return stream_completion_handler(ctx, task_id, sink);
            }
        );
    } else {
        // ç­‰å¾…å®Œæˆ
        json result = ctx.wait_for_task(task.id);

        // è¿”å›ç»“æœ
        json response = {
            {"id", generate_id()},
            {"object", "chat.completion"},
            {"created", time(nullptr)},
            {"model", model},
            {"choices", json::array({
                {
                    {"index", 0},
                    {"message", {
                        {"role", "assistant"},
                        {"content", result["content"]}
                    }},
                    {"finish_reason", "stop"}
                }
            })},
            {"usage", {
                {"prompt_tokens", result["prompt_tokens"]},
                {"completion_tokens", result["completion_tokens"]},
                {"total_tokens", result["total_tokens"]}
            }}
        };

        res.set_content(response.dump(), "application/json");
    }
}
```

#### æ¨ç†é˜Ÿåˆ—ï¼ˆContinuous Batchingï¼‰

```cpp
void llama_server_context::process_tasks() {
    while (running) {
        // 1. ä»é˜Ÿåˆ—è·å–ä»»åŠ¡
        std::vector<server_task> batch_tasks = get_pending_tasks(batch_size);

        if (batch_tasks.empty()) {
            std::this_thread::sleep_for(std::chrono::milliseconds(10));
            continue;
        }

        // 2. æ„å»ºæ‰¹æ¬¡
        llama_batch batch = llama_batch_init(batch_size, 0, 1);

        for (auto & task : batch_tasks) {
            // æ·»åŠ  prompt tokens åˆ° batch
            for (size_t i = 0; i < task.prompt_tokens.size(); i++) {
                llama_batch_add(
                    batch,
                    task.prompt_tokens[i],
                    i,  // position
                    {task.seq_id},
                    false  // logits
                );
            }

            // æœ€åä¸€ä¸ª token éœ€è¦ logits
            batch.logits[batch.n_tokens - 1] = true;
        }

        // 3. æ¨ç†
        if (llama_decode(ctx, batch) != 0) {
            LOG_ERROR("decode failed");
            continue;
        }

        // 4. é‡‡æ ·
        for (auto & task : batch_tasks) {
            float * logits = llama_get_logits_ith(ctx, task.batch_index);

            llama_token next_token = llama_sampler_sample(
                task.sampler,
                ctx,
                task.batch_index
            );

            // æ›´æ–°ä»»åŠ¡
            task.generated_tokens.push_back(next_token);

            // æ£€æŸ¥ç»“æŸæ¡ä»¶
            if (llama_token_is_eog(model, next_token) ||
                task.generated_tokens.size() >= task.params.n_predict) {
                task.state = TASK_STATE_DONE;
                complete_task(task);
            } else {
                // ç»§ç»­ç”Ÿæˆ
                task.prompt_tokens = {next_token};
            }
        }

        llama_batch_free(batch);
    }
}
```

### 2.3 å¯åŠ¨æœåŠ¡å™¨

```bash
# ç¼–è¯‘
cmake --build build --config Release --target llama-server

# è¿è¡Œ
./build/bin/llama-server \
    -m model.gguf \
    -c 4096 \
    -ngl 32 \
    --port 8080 \
    --threads 8
```

### 2.4 å®¢æˆ·ç«¯è°ƒç”¨

```python
# Python å®¢æˆ·ç«¯ï¼ˆOpenAI å…¼å®¹ï¼‰
import openai

client = openai.OpenAI(
    base_url="http://localhost:8080/v1",
    api_key="dummy"  # llama-server ä¸éœ€è¦ API key
)

# Chat Completion
response = client.chat.completions.create(
    model="llama-3",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "What is the capital of France?"}
    ],
    temperature=0.7,
    max_tokens=100
)

print(response.choices[0].message.content)

# æµå¼å“åº”
stream = client.chat.completions.create(
    model="llama-3",
    messages=[
        {"role": "user", "content": "Write a short poem about AI"}
    ],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end='', flush=True)
```

```bash
# cURL è°ƒç”¨
curl http://localhost:8080/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama-3",
    "messages": [
      {"role": "user", "content": "Hello!"}
    ],
    "temperature": 0.7
  }'
```

## 3. æ€§èƒ½ä¼˜åŒ–å®è·µ

### 3.1 æ€§èƒ½åˆ†æ

```cpp
// æ·»åŠ æ€§èƒ½è®¡æ—¶
#include <chrono>

struct perf_timer {
    std::chrono::high_resolution_clock::time_point start;

    perf_timer() : start(std::chrono::high_resolution_clock::now()) {}

    double elapsed_ms() const {
        auto end = std::chrono::high_resolution_clock::now();
        return std::chrono::duration<double, std::milli>(end - start).count();
    }
};

// ä½¿ç”¨
perf_timer timer;
llama_decode(ctx, batch);
printf("Decode: %.2f ms\n", timer.elapsed_ms());
```

### 3.2 æ‰¹å¤„ç†ä¼˜åŒ–

```cpp
// æ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚
llama_batch batch = llama_batch_init(512, 0, 4);  // æ”¯æŒ4ä¸ªåºåˆ—

// åºåˆ— 0: "The capital of France"
llama_batch_add(batch, token_ids[0], 0, {0}, false);
llama_batch_add(batch, token_ids[1], 1, {0}, false);
// ...

// åºåˆ— 1: "What is 2+2?"
llama_batch_add(batch, token_ids[10], 0, {1}, false);
llama_batch_add(batch, token_ids[11], 1, {1}, false);
// ...

// ä¸€æ¬¡æ¨ç†ï¼Œä¸¤ä¸ªåºåˆ—åŒæ—¶ç”Ÿæˆ
llama_decode(ctx, batch);
```

### 3.3 KV ç¼“å­˜å¤ç”¨

```cpp
// å¤ç”¨å…¬å…± prompt
// ä¾‹å¦‚ï¼šsystem prompt å¯¹æ‰€æœ‰è¯·æ±‚ç›¸åŒ

// 1. é¦–æ¬¡è®¡ç®— system promptï¼Œä¿å­˜ KV ç¼“å­˜
std::vector<llama_token> system_tokens = tokenize("You are a helpful assistant.");
llama_decode_with_cache_save(ctx, system_tokens, "system_cache.bin");

// 2. åç»­è¯·æ±‚ç›´æ¥åŠ è½½
llama_decode_with_cache_load(ctx, "system_cache.bin");

// 3. åªå¤„ç†ç”¨æˆ·è¾“å…¥ï¼ˆèŠ‚çœè®¡ç®—ï¼‰
std::vector<llama_token> user_tokens = tokenize("What is AI?");
llama_decode(ctx, user_tokens);
```

## 4. è¯¾ç¨‹æ€»ç»“

### 4.1 ä½ å·²ç»æŒæ¡çš„æ ¸å¿ƒæŠ€èƒ½

ç»è¿‡ 14 å¤©çš„å­¦ä¹ ï¼Œä½ ç°åœ¨èƒ½å¤Ÿï¼š

#### åŸºç¡€ç†è§£ï¼ˆDay 1-4ï¼‰
- âœ… ç†è§£ llama.cpp çš„åˆ†å±‚æ¶æ„
- âœ… æŒæ¡ GGML å¼ é‡åº“çš„æ ¸å¿ƒæ¦‚å¿µ
- âœ… ç†è§£è®¡ç®—å›¾çš„æ„å»ºä¸æ‰§è¡Œ
- âœ… æŒæ¡å¼ é‡æ“ä½œçš„å®ç°ç»†èŠ‚

#### æ¨¡å‹åŠ è½½ï¼ˆDay 5-6ï¼‰
- âœ… è§£æ GGUF æ–‡ä»¶æ ¼å¼
- âœ… ç†è§£æ¨¡å‹åŠ è½½çš„å®Œæ•´æµç¨‹
- âœ… å®ç°è‡ªå®šä¹‰çš„æ¶æ„æ”¯æŒ
- âœ… ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œ GPU offload

#### æ¨ç†æ ¸å¿ƒï¼ˆDay 7-10ï¼‰
- âœ… æ„å»º Transformer è®¡ç®—å›¾
- âœ… ç†è§£æ³¨æ„åŠ›æœºåˆ¶å®ç°
- âœ… æŒæ¡ KV ç¼“å­˜ä¼˜åŒ–
- âœ… å®ç° RoPE ä½ç½®ç¼–ç 

#### é«˜æ€§èƒ½æŠ€æœ¯ï¼ˆDay 11-12ï¼‰
- âœ… CPU SIMD ä¼˜åŒ–
- âœ… GPU åŠ é€Ÿï¼ˆCUDA/Metalï¼‰
- âœ… åç«¯æŠ½è±¡å±‚è®¾è®¡
- âœ… å¤š GPU å¹¶è¡Œç­–ç•¥

#### åº”ç”¨å¼€å‘ï¼ˆDay 13-14ï¼‰
- âœ… é‡‡æ ·ç­–ç•¥çš„å®ç°
- âœ… æ­å»ºæ¨ç†æœåŠ¡
- âœ… OpenAI API å…¼å®¹
- âœ… æ€§èƒ½è°ƒä¼˜å®è·µ

### 4.2 å…³é”®æŠ€æœ¯ç‚¹å›é¡¾

| æŠ€æœ¯ | æ ¸å¿ƒæ–‡ä»¶ | é‡è¦æ€§ | éš¾åº¦ |
|------|---------|-------|------|
| **GGML å¼ é‡** | ggml/src/ggml.c | â­â­â­â­â­ | â­â­â­ |
| **GGUF æ ¼å¼** | ggml/src/gguf.c | â­â­â­â­ | â­â­ |
| **æ¨¡å‹åŠ è½½** | src/llama-model.cpp | â­â­â­â­â­ | â­â­â­â­ |
| **è®¡ç®—å›¾æ„å»º** | src/llama-graph.cpp | â­â­â­â­â­ | â­â­â­â­ |
| **KV ç¼“å­˜** | src/llama-kv-cache.cpp | â­â­â­â­â­ | â­â­â­ |
| **é‡åŒ–ç³»ç»Ÿ** | ggml/src/ggml-quants.c | â­â­â­â­ | â­â­â­â­ |
| **é‡‡æ ·ç­–ç•¥** | src/llama-sampling.cpp | â­â­â­â­ | â­â­ |
| **åç«¯å®ç°** | ggml/src/ggml-cuda/ | â­â­â­â­ | â­â­â­â­â­ |

## 5. è¿›é˜¶è·¯çº¿

### 5.1 æ·±å…¥æ–¹å‘

#### æ–¹å‘ 1ï¼šæ¶æ„æ‰©å±•
- æ·»åŠ æ–°æ¨¡å‹æ¶æ„æ”¯æŒï¼ˆå¦‚ Llama 4, Qwen 3ï¼‰
- å®ç°è‡ªå®šä¹‰å±‚ç±»å‹
- ä¼˜åŒ–ç‰¹å®šæ¶æ„çš„æ¨ç†

**å­¦ä¹ èµ„æº**ï¼š
- `src/llama-arch.cpp` - æ¶æ„æ³¨å†Œ
- `docs/development/HOWTO-add-model.md`

#### æ–¹å‘ 2ï¼šæ€§èƒ½ä¼˜åŒ–
- å®ç°è‡ªå®šä¹‰ CUDA å†…æ ¸
- ä¼˜åŒ–é‡åŒ–ç®—æ³•
- æ”¹è¿› KV ç¼“å­˜ç­–ç•¥

**å­¦ä¹ èµ„æº**ï¼š
- `ggml/src/ggml-cuda/` - CUDA å®ç°
- CUDA Programming Guide
- FlashAttention è®ºæ–‡

#### æ–¹å‘ 3ï¼šé‡åŒ–ç ”ç©¶
- å®ç°æ–°çš„é‡åŒ–æ–¹æ³•
- ä¼˜åŒ–é‡åŒ–è¯¯å·®
- é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ

**å­¦ä¹ èµ„æº**ï¼š
- `ggml/src/ggml-quants.c`
- LLM.int8() è®ºæ–‡
- GPTQ, AWQ è®ºæ–‡

#### æ–¹å‘ 4ï¼šåº”ç”¨å¼€å‘
- æ„å»ºç”Ÿäº§çº§æœåŠ¡
- å®ç° RAG ç³»ç»Ÿ
- å¤šæ¨¡æ€æ”¯æŒ

**å­¦ä¹ èµ„æº**ï¼š
- `tools/server/` - æœåŠ¡å™¨å®ç°
- LangChain, LlamaIndex

### 5.2 æ¨èé¡¹ç›®

#### åˆçº§é¡¹ç›®
1. **è‡ªå®šä¹‰é‡‡æ ·å™¨**ï¼šå®ç°ä¸€ä¸ªæ–°çš„é‡‡æ ·ç­–ç•¥
2. **æ€§èƒ½ç›‘æ§å·¥å…·**ï¼šå¯è§†åŒ–æ¨ç†æ€§èƒ½
3. **æ¨¡å‹è½¬æ¢å·¥å…·**ï¼šæ”¯æŒæ–°æ ¼å¼è½¬æ¢

#### ä¸­çº§é¡¹ç›®
1. **åˆ†å¸ƒå¼æ¨ç†**ï¼šå¤šæœºå¤šå¡æ¨ç†ç³»ç»Ÿ
2. **é‡åŒ–å·¥å…·é“¾**ï¼šè‡ªåŠ¨åŒ–é‡åŒ–æµç¨‹
3. **RAG ç³»ç»Ÿ**ï¼šé›†æˆå‘é‡æ•°æ®åº“

#### é«˜çº§é¡¹ç›®
1. **è‡ªå®šä¹‰åç«¯**ï¼šæ”¯æŒæ–°ç¡¬ä»¶ï¼ˆå¦‚ NPUï¼‰
2. **æ¨æµ‹è§£ç **ï¼šå®ç° Speculative Decoding
3. **æ··åˆç²¾åº¦æ¨ç†**ï¼šåŠ¨æ€ç²¾åº¦è°ƒæ•´

### 5.3 æŒç»­å­¦ä¹ èµ„æº

#### è®ºæ–‡é˜…è¯»
- **Transformer**: "Attention Is All You Need"
- **LLaMA**: "LLaMA: Open and Efficient Foundation Language Models"
- **FlashAttention**: "FlashAttention: Fast and Memory-Efficient Exact Attention"
- **GPTQ**: "GPTQ: Accurate Post-Training Quantization for GPTs"
- **RoPE**: "RoFormer: Enhanced Transformer with Rotary Position Embedding"

#### å¼€æºé¡¹ç›®
- **GGML**: https://github.com/ggerganov/ggml
- **vLLM**: é«˜æ€§èƒ½æ¨ç†æœåŠ¡å™¨
- **TensorRT-LLM**: NVIDIA çš„ä¼˜åŒ–åº“
- **Text Generation Inference**: Hugging Face çš„æ¨ç†å¼•æ“

#### ç¤¾åŒºèµ„æº
- llama.cpp GitHub Discussions
- r/LocalLLaMA ç¤¾åŒº
- GGML Discord æœåŠ¡å™¨

## 6. æœ€åçš„è¯

æ­å–œä½ å®Œæˆäº†è¿™ä¸ª 14 å¤©çš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹ï¼ğŸ‰

ä½ å·²ç»ä»ä¸€ä¸ª llama.cpp çš„åˆå­¦è€…æˆé•¿ä¸ºèƒ½å¤Ÿï¼š
- é˜…è¯»å’Œç†è§£ 50K+ è¡Œæ ¸å¿ƒä»£ç 
- ä¿®æ”¹å’Œæ‰©å±• llama.cpp åŠŸèƒ½
- ä¼˜åŒ–æ¨ç†æ€§èƒ½
- æ„å»ºç”Ÿäº§çº§åº”ç”¨

### ç»§ç»­å‰è¿›

è®°ä½ï¼ŒçœŸæ­£çš„æŒæ¡æ¥è‡ªå®è·µï¼š

1. **å¤šåŠ¨æ‰‹**ï¼šå°è¯•ä¿®æ”¹ä»£ç ï¼Œè§‚å¯Ÿæ•ˆæœ
2. **å¤šé˜…è¯»**ï¼šé˜…è¯»æœ€æ–°è®ºæ–‡å’Œä»£ç 
3. **å¤šäº¤æµ**ï¼šå‚ä¸ç¤¾åŒºè®¨è®ºï¼Œåˆ†äº«ç»éªŒ
4. **å¤šæ€è€ƒ**ï¼šæ·±å…¥ç†è§£èƒŒåçš„åŸç†

### ä¿æŒè”ç³»

å¦‚æœä½ åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜æˆ–æœ‰ä»»ä½•æ”¹è¿›å»ºè®®ï¼Œæ¬¢è¿ï¼š
- åœ¨ GitHub æ Issue
- å‚ä¸ç¤¾åŒºè®¨è®º
- åˆ†äº«ä½ çš„é¡¹ç›®

ç¥ä½ åœ¨ LLM æ¨ç†é¢†åŸŸå–å¾—æˆåŠŸï¼ğŸš€

---

**æœ€ç»ˆç»ƒä¹ **ï¼š
1. éƒ¨ç½²ä¸€ä¸ª llama-server å®ä¾‹
2. å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„é‡‡æ ·ç­–ç•¥
3. æµ‹è¯•å¹¶ä¼˜åŒ–æ¨ç†æ€§èƒ½
4. åˆ†äº«ä½ çš„å­¦ä¹ å¿ƒå¾—

ğŸ“š **è¯¾ç¨‹å®Œæˆï¼** å›é¡¾ï¼š[README](README.md)

---

*æ„Ÿè°¢ä½ çš„åšæŒä¸åŠªåŠ›ï¼å¸Œæœ›è¿™ä¸ªè¯¾ç¨‹å¯¹ä½ æœ‰æ‰€å¸®åŠ©ã€‚*
