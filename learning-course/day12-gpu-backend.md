# Day 12: GPU åç«¯å®ç°ï¼ˆCUDA/Metalï¼‰

## è¯¾ç¨‹ç›®æ ‡

æŒæ¡ GPU åŠ é€Ÿçš„æ ¸å¿ƒæŠ€æœ¯ï¼š
- CUDA ç¼–ç¨‹åŸºç¡€
- cuBLAS çŸ©é˜µä¹˜æ³•
- è‡ªå®šä¹‰ CUDA å†…æ ¸
- Metal Shadersï¼ˆmacOSï¼‰
- æ€§èƒ½ä¼˜åŒ–æŠ€å·§

## 1. GPU åŠ é€ŸåŸç†

### 1.1 CPU vs GPU

```
CPU:
    â€¢ æ ¸å¿ƒæ•°: 8-64
    â€¢ æ—¶é’Ÿé¢‘ç‡: 3-5 GHz
    â€¢ é€‚åˆ: å¤æ‚é€»è¾‘ã€åˆ†æ”¯å¯†é›†
    â€¢ å³°å€¼æ€§èƒ½: ~1 TFLOPS

GPU (NVIDIA RTX 4090):
    â€¢ æ ¸å¿ƒæ•°: 16,384 (CUDA cores)
    â€¢ æ—¶é’Ÿé¢‘ç‡: ~2.5 GHz
    â€¢ é€‚åˆ: å¤§è§„æ¨¡å¹¶è¡Œè®¡ç®—
    â€¢ å³°å€¼æ€§èƒ½: ~82 TFLOPS (FP32)
                ~330 TFLOPS (FP16 TensorCore)

â†’ GPU åœ¨çŸ©é˜µè¿ç®—ä¸Šæœ‰ 100x+ ä¼˜åŠ¿
```

### 1.2 llama.cpp ä¸­çš„ GPU ä½¿ç”¨

```
æ¨¡å‹æ¨ç†çš„è®¡ç®—åˆ†å¸ƒï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Token åµŒå…¥ (GPU)                 â”‚ 2%
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 32x Transformer Layer:          â”‚
â”‚   â€¢ Q,K,V æŠ•å½± (GPU)             â”‚ 40%
â”‚   â€¢ æ³¨æ„åŠ›è®¡ç®— (GPU)             â”‚ 30%
â”‚   â€¢ FFN (GPU)                    â”‚ 25%
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¾“å‡ºæŠ•å½± (GPU)                   â”‚ 2%
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Softmax/é‡‡æ · (CPU)               â”‚ 1%
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ 95%+ è®¡ç®—åœ¨ GPU ä¸Š
```

## 2. CUDA åç«¯

### 2.1 CUDA åŸºç¡€

**çº¿ç¨‹å±‚æ¬¡**ï¼š
```
Grid (æ•´ä¸ªè®¡ç®—)
  â†“
Block (å·¥ä½œç»„, å¦‚ 256 çº¿ç¨‹)
  â†“
Thread (å•ä¸ªçº¿ç¨‹)
  â†“
Warp (32 çº¿ç¨‹, ç¡¬ä»¶è°ƒåº¦å•ä½)
```

**å†…å­˜å±‚æ¬¡**ï¼š
```
Global Memory (GPU VRAM)    ~16-24 GB, æ…¢ (~1000 GB/s)
  â†“
L2 Cache                    ~MB çº§, ä¸­é€Ÿ
  â†“
Shared Memory (per block)   ~100 KB, å¿« (~20 TB/s)
  â†“
Registers (per thread)      ~64 KB, æå¿«
```

### 2.2 ç®€å•çš„ CUDA å†…æ ¸

```cuda
// ä½ç½®ï¼šggml/src/ggml-cuda/vector-add.cu
__global__ void vector_add_kernel(
    const float * x,
    const float * y,
    float * z,
    int n) {

    // å…¨å±€çº¿ç¨‹ç´¢å¼•
    int idx = blockIdx.x * blockDim.x + threadIdx.x;

    if (idx < n) {
        z[idx] = x[idx] + y[idx];
    }
}

// ä¸»æœºç«¯è°ƒç”¨
void ggml_cuda_op_add(
    const ggml_tensor * src0,
    const ggml_tensor * src1,
    ggml_tensor * dst) {

    const int n = ggml_nelements(dst);

    // é…ç½®çº¿ç¨‹å—
    const int block_size = 256;
    const int grid_size = (n + block_size - 1) / block_size;

    // å¯åŠ¨å†…æ ¸
    vector_add_kernel<<<grid_size, block_size>>>(
        (float *)src0->data,
        (float *)src1->data,
        (float *)dst->data,
        n);

    // åŒæ­¥ç­‰å¾…å®Œæˆ
    cudaDeviceSynchronize();
}
```

### 2.3 cuBLAS çŸ©é˜µä¹˜æ³•

```cpp
// ä½ç½®ï¼šggml/src/ggml-cuda/ggml-cuda.cpp:1234
void ggml_cuda_mul_mat_cublas(
    const ggml_tensor * src0,  // [K, M]
    const ggml_tensor * src1,  // [K, N]
    ggml_tensor * dst) {       // [M, N]

    const int M = src0->ne[1];
    const int N = src1->ne[1];
    const int K = src0->ne[0];

    const float alpha = 1.0f;
    const float beta = 0.0f;

    // cuBLAS çŸ©é˜µä¹˜æ³•ï¼šC = alpha * A @ B + beta * C
    cublasStatus_t status = cublasSgemm(
        cublas_handle,
        CUBLAS_OP_T,    // A è½¬ç½®
        CUBLAS_OP_N,    // B ä¸è½¬ç½®
        M, N, K,
        &alpha,
        (float *)src0->data, K,
        (float *)src1->data, K,
        &beta,
        (float *)dst->data, M);

    CUDA_CHECK(status);
}
// æ€§èƒ½: ~15 TFLOPS (RTX 4090)
// cuBLAS å·²ç»é«˜åº¦ä¼˜åŒ–ï¼Œé€šå¸¸ä¸éœ€è¦è‡ªå·±å†™
```

### 2.4 é‡åŒ–çŸ©é˜µä¹˜æ³•å†…æ ¸

llama.cpp çš„æ ¸å¿ƒä¼˜åŒ–ï¼šè‡ªå®šä¹‰é‡åŒ– CUDA å†…æ ¸

```cuda
// ä½ç½®ï¼šggml/src/ggml-cuda/mmq.cuh:456
// Q4_K Ã— Q8_0 çŸ©é˜µä¹˜æ³•

template<int qk, int qr, int qi>
__global__ void mul_mat_q4_K(
    const void * __restrict__ vx,  // Q4_K æƒé‡ [K, M]
    const void * __restrict__ vy,  // Q8_0 æ¿€æ´» [K, N]
    float * __restrict__ dst,      // è¾“å‡º [M, N]
    const int ncols_x,
    const int nrows_x) {

    // æ¯ä¸ª block å¤„ç†ä¸€è¡Œ
    const int row = blockIdx.x;
    const int tid = threadIdx.x;

    // å…±äº«å†…å­˜ï¼šå‡å°‘ global memory è®¿é—®
    __shared__ float tmp[WARP_SIZE];

    const block_q4_K * x = (const block_q4_K *)vx + row * (ncols_x / QK_K);
    const block_q8_0 * y = (const block_q8_0 *)vy;

    float sumf = 0.0f;

    // éå†åˆ—æ–¹å‘ï¼ˆK ç»´åº¦ï¼‰
    for (int i = 0; i < ncols_x; i += QK_K) {
        const int ib = i / QK_K;

        // 1. åŠ è½½é‡åŒ–å‚æ•°
        const float d = x[ib].d;
        const float dmin = x[ib].dmin;

        // 2. åé‡åŒ–å¹¶ç‚¹ç§¯
        int sumi = 0;
        for (int j = 0; j < QK_K/2; j++) {
            // ä» 4-bit æå–
            const int q = get_q4_value(x[ib].qs, j);

            // Q4 Ã— Q8 æ•´æ•°ä¹˜æ³•
            const int y_val = y[i/QK8_0 + j].qs[tid];
            sumi += q * y_val;
        }

        // 3. ç¼©æ”¾å¹¶ç´¯åŠ 
        sumf += d * sumi - dmin;
    }

    // 4. Warp å†…å½’çº¦
    sumf = warp_reduce_sum(sumf);

    // 5. å†™å›ç»“æœ
    if (tid == 0) {
        dst[row] = sumf;
    }
}
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
```
FP32 MatMul (cuBLAS):  15 TFLOPS
Q4_K MatMul (custom):  40+ TFLOPS (æœ‰æ•ˆ)
    â€¢ è®¡ç®—é‡å‡å°‘ 8x (4-bit vs 32-bit)
    â€¢ å†…å­˜å¸¦å®½å‡å°‘ 8x
    â€¢ æ€»ä½“æ€§èƒ½æå‡ 2-3x
```

### 2.5 Flash Attention CUDA å®ç°

```cuda
// ä½ç½®ï¼šggml/src/ggml-cuda/fattn.cuh:823
__global__ void flash_attn_kernel(
    const float * Q,           // [n_head, n_tokens, d_head]
    const float * K,           // [n_head, kv_size, d_head]
    const float * V,           // [n_head, kv_size, d_head]
    float * O,                 // [n_head, n_tokens, d_head]
    const int n_tokens,
    const int kv_size,
    const int d_head,
    const float scale) {

    const int head_idx = blockIdx.y;
    const int token_idx = blockIdx.x;
    const int tid = threadIdx.x;

    // å…±äº«å†…å­˜
    __shared__ float Q_shared[128];
    __shared__ float K_shared[128];
    __shared__ float V_shared[128];

    // 1. åŠ è½½ Q åˆ°å…±äº«å†…å­˜
    if (tid < d_head) {
        Q_shared[tid] = Q[head_idx * n_tokens * d_head + token_idx * d_head + tid];
    }
    __syncthreads();

    // åœ¨çº¿ softmax
    float row_max = -INFINITY;
    float row_sum = 0.0f;
    float output[128] = {0};

    // 2. åˆ†å—å¤„ç† K, V
    const int n_blocks = (kv_size + 31) / 32;
    for (int block = 0; block < n_blocks; block++) {
        int kv_idx = block * 32 + tid;

        // 2a. åŠ è½½ K å—
        if (kv_idx < kv_size && tid < d_head) {
            K_shared[tid] = K[head_idx * kv_size * d_head + kv_idx * d_head + tid];
        }
        __syncthreads();

        // 2b. Q @ K^T
        float score = 0.0f;
        if (kv_idx < kv_size) {
            for (int i = 0; i < d_head; i++) {
                score += Q_shared[i] * K_shared[i];
            }
            score *= scale;

            // 2c. åœ¨çº¿æ›´æ–°æœ€å¤§å€¼
            float new_max = fmaxf(row_max, score);
            float exp_diff = expf(row_max - new_max);

            row_sum = row_sum * exp_diff + expf(score - new_max);
            row_max = new_max;
        }

        // 2d. åŠ è½½ V å¹¶ç´¯ç§¯
        if (kv_idx < kv_size && tid < d_head) {
            V_shared[tid] = V[head_idx * kv_size * d_head + kv_idx * d_head + tid];
        }
        __syncthreads();

        if (kv_idx < kv_size) {
            float attn = expf(score - row_max) / row_sum;
            for (int i = 0; i < d_head; i++) {
                output[i] += attn * V_shared[i];
            }
        }
        __syncthreads();
    }

    // 3. å†™å›è¾“å‡º
    if (tid < d_head) {
        O[head_idx * n_tokens * d_head + token_idx * d_head + tid] = output[tid];
    }
}
```

## 3. Metal åç«¯ï¼ˆmacOSï¼‰

### 3.1 Metal Shading Language

```metal
// ä½ç½®ï¼šggml/src/ggml-metal/ggml-metal.metal:234
kernel void kernel_add(
    device const float * src0 [[buffer(0)]],
    device const float * src1 [[buffer(1)]],
    device float * dst [[buffer(2)]],
    uint id [[thread_position_in_grid]]) {

    dst[id] = src0[id] + src1[id];
}
```

### 3.2 Metal çŸ©é˜µä¹˜æ³•

```metal
// ä½¿ç”¨ Metal Performance Shaders (MPS)
kernel void kernel_mul_mat_f32(
    device const float * src0 [[buffer(0)]],  // [K, M]
    device const float * src1 [[buffer(1)]],  // [K, N]
    device float * dst [[buffer(2)]],         // [M, N]
    constant int & M [[buffer(3)]],
    constant int & N [[buffer(4)]],
    constant int & K [[buffer(5)]],
    uint2 gid [[thread_position_in_grid]]) {

    const int row = gid.y;  // M ç»´åº¦
    const int col = gid.x;  // N ç»´åº¦

    if (row >= M || col >= N) return;

    float sum = 0.0f;
    for (int k = 0; k < K; k++) {
        sum += src0[row * K + k] * src1[k * N + col];
    }

    dst[row * N + col] = sum;
}
```

**è°ƒç”¨ä»£ç **ï¼š
```objc
// ä½ç½®ï¼šggml/src/ggml-metal/ggml-metal.m:567
id<MTLComputePipelineState> pipeline = ...;
id<MTLComputeCommandEncoder> encoder = [commandBuffer computeCommandEncoder];

[encoder setComputePipelineState:pipeline];
[encoder setBuffer:src0_buf offset:0 atIndex:0];
[encoder setBuffer:src1_buf offset:0 atIndex:1];
[encoder setBuffer:dst_buf offset:0 atIndex:2];

MTLSize gridSize = MTLSizeMake(N, M, 1);
MTLSize threadGroupSize = MTLSizeMake(8, 8, 1);

[encoder dispatchThreads:gridSize
  threadsPerThreadgroup:threadGroupSize];

[encoder endEncoding];
[commandBuffer commit];
[commandBuffer waitUntilCompleted];
```

## 4. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

### 4.1 å†…å­˜åˆå¹¶è®¿é—®

```cuda
// âŒ åä¾‹å­ï¼šéåˆå¹¶è®¿é—®
__global__ void bad_kernel(float * data, int stride) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    data[idx * stride] = ...;  // è·¨æ­¥è®¿é—®ï¼Œæ— æ³•åˆå¹¶
}

// âœ… å¥½ä¾‹å­ï¼šåˆå¹¶è®¿é—®
__global__ void good_kernel(float * data) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    data[idx] = ...;  // è¿ç»­è®¿é—®ï¼Œå¯ä»¥åˆå¹¶
}
// æ€§èƒ½å·®å¼‚ï¼š10x+
```

### 4.2 å…±äº«å†…å­˜ä¼˜åŒ–

```cuda
// ä½¿ç”¨å…±äº«å†…å­˜ç¼“å­˜æ•°æ®
__global__ void optimized_matmul(
    const float * A,
    const float * B,
    float * C,
    int M, int N, int K) {

    __shared__ float As[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE];

    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;
    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;

    float sum = 0.0f;

    // åˆ†å—åŠ è½½åˆ°å…±äº«å†…å­˜
    for (int tile = 0; tile < K / BLOCK_SIZE; tile++) {
        // åŠ è½½ A å—
        As[threadIdx.y][threadIdx.x] = A[row * K + tile * BLOCK_SIZE + threadIdx.x];

        // åŠ è½½ B å—
        Bs[threadIdx.y][threadIdx.x] = B[(tile * BLOCK_SIZE + threadIdx.y) * N + col];

        __syncthreads();

        // è®¡ç®—
        for (int k = 0; k < BLOCK_SIZE; k++) {
            sum += As[threadIdx.y][k] * Bs[k][threadIdx.x];
        }

        __syncthreads();
    }

    C[row * N + col] = sum;
}
// æ€§èƒ½æå‡ï¼š5-10x
```

### 4.3 å ç”¨ç‡ä¼˜åŒ–

```cpp
// æ£€æŸ¥å†…æ ¸å ç”¨ç‡
cudaOccupancyMaxActiveBlocksPerMultiprocessor(
    &num_blocks,
    my_kernel,
    block_size,
    dynamic_smem_size);

printf("Occupancy: %.2f%%\n", 100.0 * num_blocks / max_blocks_per_sm);

// ä¼˜åŒ–ç›®æ ‡ï¼š>50% å ç”¨ç‡
```

### 4.4 æµæ°´çº¿å¹¶è¡Œ

```cpp
// ä½¿ç”¨ CUDA Streams å¹¶è¡Œæ‰§è¡Œ
cudaStream_t stream1, stream2;
cudaStreamCreate(&stream1);
cudaStreamCreate(&stream2);

// Layer 0 åœ¨ stream1
kernel_layer_0<<<grid, block, 0, stream1>>>(input, layer0_out);

// Layer 1 åœ¨ stream2ï¼ˆä¸ layer0 å¹¶è¡Œï¼‰
kernel_layer_1<<<grid, block, 0, stream2>>>(layer0_out, layer1_out);

// åŒæ­¥
cudaStreamSynchronize(stream1);
cudaStreamSynchronize(stream2);
```

## 5. å¤š GPU æ”¯æŒ

### 5.1 å¼ é‡å¹¶è¡Œ

```cpp
// ä½ç½®ï¼šsrc/llama-model.cpp:5123
void distribute_tensor_across_gpus(
    ggml_tensor * tensor,
    const std::vector<int> & gpu_ids) {

    const int n_gpu = gpu_ids.size();
    const int rows_per_gpu = tensor->ne[1] / n_gpu;

    for (int i = 0; i < n_gpu; i++) {
        cudaSetDevice(gpu_ids[i]);

        // ä¸ºæ¯ä¸ª GPU åˆ†é…ä¸€éƒ¨åˆ†æ•°æ®
        void * gpu_data;
        size_t slice_size = rows_per_gpu * tensor->ne[0] * ggml_element_size(tensor);
        cudaMalloc(&gpu_data, slice_size);

        // æ‹·è´æ•°æ®ç‰‡æ®µ
        cudaMemcpy(gpu_data,
                   (char*)tensor->data + i * slice_size,
                   slice_size,
                   cudaMemcpyHostToDevice);

        tensor->extra_gpu[i] = gpu_data;
    }
}
```

### 5.2 æµæ°´çº¿å¹¶è¡Œ

```
GPU 0: Layer 0-10
GPU 1: Layer 11-21
GPU 2: Layer 22-31

Input â†’ GPU0 â†’ (Transfer) â†’ GPU1 â†’ (Transfer) â†’ GPU2 â†’ Output
```

## 6. æ€§èƒ½å¯¹æ¯”

| å¹³å° | ç¡¬ä»¶ | FP32 (TFLOPS) | Q4_K (æœ‰æ•ˆTFLOPS) | 7Bæ¨¡å‹é€Ÿåº¦ |
|------|------|--------------|-----------------|-----------|
| **CPU** | i9-13900K | 1.0 | 2 | ~5 t/s |
| **GPU (CUDA)** | RTX 4090 | 82 | 150+ | ~120 t/s |
| **GPU (Metal)** | M2 Max | 13 | 25 | ~40 t/s |
| **å¤šGPU** | 4Ã— RTX 4090 | 328 | 600+ | ~400 t/s |

t/s = tokens per second (æ¯ç§’ç”Ÿæˆtokenæ•°)

## 7. æ€»ç»“

ä»Šå¤©æˆ‘ä»¬å­¦ä¹ äº† GPU åç«¯å®ç°ï¼š

âœ… **CUDA åŸºç¡€**ï¼šçº¿ç¨‹æ¨¡å‹ã€å†…å­˜å±‚æ¬¡
âœ… **cuBLAS**ï¼šé«˜æ€§èƒ½çŸ©é˜µä¹˜æ³•
âœ… **è‡ªå®šä¹‰å†…æ ¸**ï¼šé‡åŒ–ã€Flash Attention
âœ… **Metal**ï¼šmacOS GPU åŠ é€Ÿ
âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šå†…å­˜è®¿é—®ã€å…±äº«å†…å­˜ã€å¹¶è¡Œ

### å…³é”®è¦ç‚¹

1. **cuBLAS ç”¨äºæ ‡å‡† MatMul**ï¼šå·²ä¼˜åŒ–åˆ°æè‡´
2. **è‡ªå®šä¹‰å†…æ ¸ç”¨äºé‡åŒ–**ï¼šç‰¹æ®Šæ“ä½œéœ€è¦å®šåˆ¶
3. **å†…å­˜è®¿é—®æ˜¯å…³é”®**ï¼šåˆå¹¶è®¿é—®ã€å…±äº«å†…å­˜
4. **å¤š GPU æ‰©å±•**ï¼šå¼ é‡å¹¶è¡Œ + æµæ°´çº¿å¹¶è¡Œ

## è¯¾ç¨‹å®Œæˆï¼

æ­å–œä½ å®Œæˆäº† llama.cpp çš„ 14 å¤©æ·±åº¦å­¦ä¹ è¯¾ç¨‹ï¼

å›é¡¾ä¸€ä¸‹ä½ å·²ç»æŒæ¡çš„æŠ€èƒ½ï¼š
- âœ… llama.cpp æ¶æ„è®¾è®¡
- âœ… GGML å¼ é‡åº“
- âœ… GGUF æ–‡ä»¶æ ¼å¼
- âœ… æ¨¡å‹åŠ è½½ä¸æ¨ç†
- âœ… æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–
- âœ… KV ç¼“å­˜ç³»ç»Ÿ
- âœ… é‡åŒ–æŠ€æœ¯
- âœ… é‡‡æ ·ç­–ç•¥
- âœ… CPU/GPU åç«¯ä¼˜åŒ–

**ä¸‹ä¸€æ­¥**ï¼šå¼€å§‹å®è·µé¡¹ç›®ï¼Œä¸º llama.cpp è´¡çŒ®ä»£ç ï¼

---

ğŸ“š [å›åˆ°è¯¾ç¨‹æ€»è§ˆ](README.md) | [Day 14: å®æˆ˜é¡¹ç›®ä¸æ€»ç»“](day14-tools-practice.md)
