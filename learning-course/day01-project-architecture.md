# Day 1: é¡¹ç›®æ¶æ„ä¸è®¾è®¡å“²å­¦

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**ï¼šç†è§£llama.cppçš„æ•´ä½“æ¶æ„ã€è®¾è®¡ç†å¿µï¼Œä»¥åŠå„ä¸ªæ¨¡å—ä¹‹é—´çš„å…³ç³»ã€‚

## 1. é¡¹ç›®ç®€ä»‹

### 1.1 ä»€ä¹ˆæ˜¯llama.cppï¼Ÿ

llama.cpp æ˜¯ä¸€ä¸ªç”¨çº¯C/C++ç¼–å†™çš„LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰æ¨ç†å¼•æ“ï¼Œå®ƒçš„æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

- **æœ€å°åŒ–ä¾èµ–**ï¼šæ— éœ€Pythonã€PyTorchç­‰é‡é‡çº§æ¡†æ¶
- **é«˜æ€§èƒ½**ï¼šå……åˆ†åˆ©ç”¨CPU/GPUç¡¬ä»¶ç‰¹æ€§
- **è·¨å¹³å°**ï¼šæ”¯æŒLinuxã€macOSã€Windowsã€ç§»åŠ¨è®¾å¤‡
- **å†…å­˜é«˜æ•ˆ**ï¼šé€šè¿‡é‡åŒ–æŠ€æœ¯é™ä½å†…å­˜å ç”¨

### 1.2 è®¾è®¡å“²å­¦

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  è®¾è®¡åŸåˆ™                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. Simple > Complex                 â”‚
â”‚    - é¿å…è¿‡åº¦è®¾è®¡                    â”‚
â”‚    - ä»£ç å¯è¯»æ€§ä¼˜å…ˆ                  â”‚
â”‚                                     â”‚
â”‚ 2. Performance > Elegance           â”‚
â”‚    - æ€§èƒ½å…³é”®è·¯å¾„ä¸è¿½æ±‚ä¼˜é›…          â”‚
â”‚    - å…è®¸ä½¿ç”¨åº•å±‚ä¼˜åŒ–                â”‚
â”‚                                     â”‚
â”‚ 3. Minimal Dependencies             â”‚
â”‚    - å‡å°‘å¤–éƒ¨ä¾èµ–                    â”‚
â”‚    - æ˜“äºé›†æˆåˆ°å…¶ä»–é¡¹ç›®              â”‚
â”‚                                     â”‚
â”‚ 4. Platform Agnostic                â”‚
â”‚    - è·¨å¹³å°è®¾è®¡                      â”‚
â”‚    - ç¡¬ä»¶æŠ½è±¡                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2. æ•´ä½“æ¶æ„

### 2.1 åˆ†å±‚æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     åº”ç”¨å±‚ (Applications)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ llama-cliâ”‚  â”‚  server  â”‚  â”‚  bench   â”‚  â”‚  custom  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  llama.cpp API Layer                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ include/llama.h - å…¬å…±Cæ¥å£                           â”‚â”‚
â”‚  â”‚  â€¢ llama_model_load()                                â”‚â”‚
â”‚  â”‚  â€¢ llama_decode()                                    â”‚â”‚
â”‚  â”‚  â€¢ llama_sample()                                    â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                æ ¸å¿ƒå®ç°å±‚ (Core Implementation)           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Model       â”‚  â”‚  Context    â”‚  â”‚  Sampling   â”‚     â”‚
â”‚  â”‚ Loading     â”‚  â”‚  Management â”‚  â”‚  Strategy   â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Architectureâ”‚  â”‚  KV Cache   â”‚  â”‚  Vocabulary â”‚     â”‚
â”‚  â”‚ Definition  â”‚  â”‚  Management â”‚  â”‚  Processing â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               GGMLå¼ é‡åº“ (GGML Tensor Library)            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ ggml/include/ggml.h - å¼ é‡æ“ä½œAPI                     â”‚â”‚
â”‚  â”‚  â€¢ ggml_new_tensor()                                 â”‚â”‚
â”‚  â”‚  â€¢ ggml_mul_mat()                                    â”‚â”‚
â”‚  â”‚  â€¢ ggml_graph_compute()                              â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               åç«¯å±‚ (Backend Layer)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   CPU    â”‚  â”‚   CUDA   â”‚  â”‚  Metal   â”‚  â”‚   ...    â”‚ â”‚
â”‚  â”‚ (AVX2/   â”‚  â”‚ (NVIDIA) â”‚  â”‚  (Apple) â”‚  â”‚          â”‚ â”‚
â”‚  â”‚  ARM)    â”‚  â”‚          â”‚  â”‚          â”‚  â”‚          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 ç›®å½•ç»“æ„è¯¦è§£

è®©æˆ‘ä»¬æ·±å…¥äº†è§£æ¯ä¸ªç›®å½•çš„ä½œç”¨ï¼š

```bash
llama.cpp/
â”œâ”€â”€ include/              # ğŸ“‹ å…¬å…±APIå¤´æ–‡ä»¶
â”‚   â”œâ”€â”€ llama.h          # ä¸»APIæ¥å£ï¼ˆ80KBï¼Œæ ¸å¿ƒAPIï¼‰
â”‚   â””â”€â”€ llama-cpp.h      # C++è¾…åŠ©æ¥å£
â”‚
â”œâ”€â”€ src/                 # ğŸ”¥ æ ¸å¿ƒå®ç°ï¼ˆæœ€é‡è¦ï¼‰
â”‚   â”œâ”€â”€ llama.cpp        # ä¸»å…¥å£æ–‡ä»¶ï¼ˆ51KBï¼‰
â”‚   â”œâ”€â”€ llama-model.cpp  # æ¨¡å‹åŠ è½½ä¸æ¶æ„å®šä¹‰ï¼ˆ483KBï¼Œæœ€å¤§æ–‡ä»¶ï¼ï¼‰
â”‚   â”œâ”€â”€ llama-context.cpp # æ¨ç†ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆ129KBï¼‰
â”‚   â”œâ”€â”€ llama-kv-cache.cpp # KVç¼“å­˜å®ç°ï¼ˆ75KBï¼‰
â”‚   â”œâ”€â”€ llama-vocab.cpp  # è¯è¡¨ä¸tokenizerï¼ˆ156KBï¼‰
â”‚   â”œâ”€â”€ llama-sampling.cpp # é‡‡æ ·ç­–ç•¥ï¼ˆ133KBï¼‰
â”‚   â”œâ”€â”€ llama-graph.cpp  # è®¡ç®—å›¾æ„å»ºï¼ˆ77KBï¼‰
â”‚   â”œâ”€â”€ llama-grammar.cpp # è¯­æ³•çº¦æŸï¼ˆ54KBï¼‰
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ ggml/                # ğŸ“ å¼ é‡åº“ï¼ˆæ ¸å¿ƒä¾èµ–ï¼‰
â”‚   â”œâ”€â”€ include/
â”‚   â”‚   â”œâ”€â”€ ggml.h       # å¼ é‡APIï¼ˆ102KBï¼‰
â”‚   â”‚   â”œâ”€â”€ ggml-backend.h # åç«¯æŠ½è±¡ï¼ˆ21KBï¼‰
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ ggml.c       # å¼ é‡æ“ä½œå®ç°ï¼ˆ242KBï¼‰
â”‚       â”œâ”€â”€ ggml-backend.cpp # åç«¯ç®¡ç†ï¼ˆ89KBï¼‰
â”‚       â”œâ”€â”€ ggml-alloc.c # å†…å­˜åˆ†é…å™¨ï¼ˆ48KBï¼‰
â”‚       â”œâ”€â”€ ggml-quants.c # é‡åŒ–å®ç°ï¼ˆ217KBï¼‰
â”‚       â””â”€â”€ ggml-cpu/    # CPUåç«¯
â”‚       â”‚   â”œâ”€â”€ ggml-cpu.cpp
â”‚       â”‚   â””â”€â”€ ...
â”‚       â””â”€â”€ ggml-cuda/   # CUDAåç«¯
â”‚           â””â”€â”€ ...
â”‚
â”œâ”€â”€ common/              # ğŸ› ï¸ é€šç”¨å·¥å…·
â”‚   â”œâ”€â”€ common.cpp       # å‚æ•°è§£æã€è¾…åŠ©å‡½æ•°
â”‚   â”œâ”€â”€ sampling.cpp     # é‡‡æ ·å·¥å…·
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ tools/               # ğŸ”§ å·¥å…·é›†
â”‚   â”œâ”€â”€ cli/            # å‘½ä»¤è¡Œå·¥å…·
â”‚   â”œâ”€â”€ server/         # HTTPæœåŠ¡å™¨ï¼ˆOpenAIå…¼å®¹ï¼‰
â”‚   â”œâ”€â”€ quantize/       # æ¨¡å‹é‡åŒ–
â”‚   â”œâ”€â”€ perplexity/     # å›°æƒ‘åº¦æµ‹è¯•
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ gguf-py/            # ğŸ Pythonå·¥å…·
â”‚   â””â”€â”€ gguf/           # GGUFæ–‡ä»¶æ ¼å¼å¤„ç†
â”‚
â””â”€â”€ tests/              # ğŸ§ª æµ‹è¯•
    â””â”€â”€ test-*.cpp      # å„ç§å•å…ƒæµ‹è¯•
```

## 3. æ ¸å¿ƒæ•°æ®æµ

### 3.1 æ¨ç†æµç¨‹æ¦‚è§ˆ

```
ç”¨æˆ·è¾“å…¥ (Prompt)
    â”‚
    â”œâ”€> 1. Tokenization (è¯æ³•åˆ†æ)
    â”‚      llama-vocab.cpp: token_to_id()
    â”‚      "Hello" -> [15339, 995]
    â”‚
    â”œâ”€> 2. Model Inference (æ¨¡å‹æ¨ç†)
    â”‚      llama-context.cpp: llama_decode()
    â”‚      â”‚
    â”‚      â”œâ”€> 2.1 Build Computation Graph
    â”‚      â”‚      llama-graph.cpp: llama_build_graph()
    â”‚      â”‚      æ„å»ºå‰å‘ä¼ æ’­è®¡ç®—å›¾
    â”‚      â”‚
    â”‚      â”œâ”€> 2.2 Execute Graph
    â”‚      â”‚      ggml-backend.cpp: ggml_graph_compute()
    â”‚      â”‚      åœ¨é€‰å®šçš„åç«¯ä¸Šæ‰§è¡Œ
    â”‚      â”‚
    â”‚      â””â”€> 2.3 Update KV Cache
    â”‚             llama-kv-cache.cpp: update()
    â”‚             ç¼“å­˜é”®å€¼å¯¹ï¼ŒåŠ é€Ÿåç»­æ¨ç†
    â”‚
    â”œâ”€> 3. Sampling (é‡‡æ ·)
    â”‚      llama-sampling.cpp: llama_sampler_sample()
    â”‚      logits -> next_token (é€šè¿‡æ¸©åº¦ã€top-kç­‰ç­–ç•¥)
    â”‚
    â””â”€> 4. Detokenization (åè¯æ³•åˆ†æ)
           llama-vocab.cpp: id_to_token()
           [15339] -> "Hello"
```

### 3.2 å…³é”®å¯¹è±¡ç”Ÿå‘½å‘¨æœŸ

```cpp
// 1. åŠ è½½æ¨¡å‹ï¼ˆä¸€æ¬¡æ€§ï¼‰
llama_model* model = llama_model_load_from_file("model.gguf", params);
//    â”œâ”€> è¯»å–GGUFæ–‡ä»¶å¤´
//    â”œâ”€> åŠ è½½å…ƒæ•°æ®ï¼ˆhyperparametersï¼‰
//    â”œâ”€> åŠ è½½å¼ é‡ï¼ˆweightsï¼‰
//    â””â”€> åˆå§‹åŒ–è®¾å¤‡ï¼ˆGPU/CPUï¼‰

// 2. åˆ›å»ºä¸Šä¸‹æ–‡ï¼ˆå¯å¤ç”¨æ¨¡å‹ï¼‰
llama_context* ctx = llama_new_context_with_model(model, params);
//    â”œâ”€> åˆ†é…KVç¼“å­˜
//    â”œâ”€> åˆå§‹åŒ–è®¡ç®—å›¾
//    â””â”€> å‡†å¤‡åç«¯èµ„æº

// 3. æ¨ç†å¾ªç¯ï¼ˆæŒç»­ï¼‰
while (not_finished) {
    llama_decode(ctx, batch);  // æ‰§è¡Œä¸€æ­¥å‰å‘ä¼ æ’­
    //    â”œâ”€> æ„å»ºè®¡ç®—å›¾
    //    â”œâ”€> æ‰§è¡Œå›¾è®¡ç®—
    //    â””â”€> æ›´æ–°KVç¼“å­˜

    int token = llama_sample(ctx, sampler);  // é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
}

// 4. æ¸…ç†èµ„æº
llama_free(ctx);
llama_model_free(model);
```

## 4. å…³é”®æ¨¡å—èŒè´£

### 4.1 æ¨¡å‹å±‚ (Model Layer)

**æ–‡ä»¶**: `src/llama-model.cpp` (483KB)

**èŒè´£**ï¼š
```cpp
// æ¨¡å‹æ¶æ„å®šä¹‰
struct llama_model {
    llm_arch arch;              // æ¨¡å‹æ¶æ„ç±»å‹ï¼ˆLLaMA/GPT/...ï¼‰
    llama_hparams hparams;      // è¶…å‚æ•°
    llama_vocab vocab;          // è¯è¡¨
    ggml_tensor* tensors;       // æƒé‡å¼ é‡
    std::vector<ggml_backend_dev_t> devices;  // è®¡ç®—è®¾å¤‡
};

// æ”¯æŒçš„æ¶æ„ç±»å‹
enum llm_arch {
    LLM_ARCH_LLAMA,     // LLaMAç³»åˆ—
    LLM_ARCH_FALCON,    // Falcon
    LLM_ARCH_GPT2,      // GPT-2
    LLM_ARCH_GPTJ,      // GPT-J
    // ... 100+ æ¶æ„
};
```

**å…³é”®å‡½æ•°**ï¼š
- `llama_model_load()`: ä»GGUFæ–‡ä»¶åŠ è½½æ¨¡å‹
- `llama_model_meta()`: è·å–æ¨¡å‹å…ƒæ•°æ®
- `weight_buft_supported()`: æ£€æŸ¥æƒé‡bufferç±»å‹æ”¯æŒ

### 4.2 ä¸Šä¸‹æ–‡å±‚ (Context Layer)

**æ–‡ä»¶**: `src/llama-context.cpp` (129KB)

**èŒè´£**ï¼š
```cpp
struct llama_context {
    llama_model& model;           // å…³è”çš„æ¨¡å‹
    llama_cparams cparams;        // ä¸Šä¸‹æ–‡å‚æ•°
    llama_kv_cache kv_cache;      // KVç¼“å­˜
    ggml_cgraph* cgraph;          // è®¡ç®—å›¾
    ggml_backend_sched* sched;    // åç«¯è°ƒåº¦å™¨
};
```

**å…³é”®å‡½æ•°**ï¼š
- `llama_decode()`: ä¸»æ¨ç†å‡½æ•°
- `llama_get_logits()`: è·å–è¾“å‡ºæ¦‚ç‡
- `llama_set_n_threads()`: è®¾ç½®çº¿ç¨‹æ•°

### 4.3 KVç¼“å­˜å±‚ (KV Cache Layer)

**æ–‡ä»¶**: `src/llama-kv-cache.cpp` (75KB)

**èŒè´£**ï¼š
```cpp
struct llama_kv_cache {
    struct layer {
        ggml_tensor* k;  // Keyç¼“å­˜
        ggml_tensor* v;  // Valueç¼“å­˜
    };

    std::vector<layer> layers;     // æ¯å±‚çš„KVç¼“å­˜
    std::vector<llama_kv_cell> cells;  // ç¼“å­˜å•å…ƒ
    uint32_t size;                 // ç¼“å­˜å¤§å°
};
```

**ä¸ºä»€ä¹ˆéœ€è¦KVç¼“å­˜ï¼Ÿ**
```
æ²¡æœ‰KVç¼“å­˜ï¼š
Token 1:  è®¡ç®— Q1, K1, V1 -> Attention(Q1, K1, V1)
Token 2:  è®¡ç®— Q2, K2, V2 -> Attention(Q2, [K1,K2], [V1,V2])  âŒ é‡å¤è®¡ç®—K1,V1
Token 3:  è®¡ç®— Q3, K3, V3 -> Attention(Q3, [K1,K2,K3], [V1,V2,V3])  âŒ é‡å¤è®¡ç®—

æœ‰KVç¼“å­˜ï¼š
Token 1:  è®¡ç®— Q1, K1, V1 -> ç¼“å­˜ K1, V1
Token 2:  è®¡ç®— Q2, K2, V2 -> ç¼“å­˜ K2, V2 -> ä½¿ç”¨ç¼“å­˜çš„ [K1,K2], [V1,V2] âœ…
Token 3:  è®¡ç®— Q3, K3, V3 -> ç¼“å­˜ K3, V3 -> ä½¿ç”¨ç¼“å­˜çš„ [K1,K2,K3], [V1,V2,V3] âœ…

æ—¶é—´å¤æ‚åº¦ï¼šO(nÂ²) -> O(n)
```

### 4.4 GGMLå¼ é‡å±‚ (GGML Tensor Layer)

**æ–‡ä»¶**: `ggml/src/ggml.c` (242KB)

**èŒè´£**ï¼š
```cpp
// å¼ é‡æ•°æ®ç»“æ„
struct ggml_tensor {
    enum ggml_type type;    // æ•°æ®ç±»å‹ï¼ˆF32, F16, Q4_0...ï¼‰
    int64_t ne[4];          // ç»´åº¦å¤§å° [dim0, dim1, dim2, dim3]
    size_t  nb[4];          // æ¯ä¸ªç»´åº¦çš„å­—èŠ‚æ­¥é•¿ï¼ˆstrideï¼‰
    void*   data;           // æ•°æ®æŒ‡é’ˆ
    struct ggml_tensor* src[2];  // æºå¼ é‡ï¼ˆç”¨äºæ„å»ºè®¡ç®—å›¾ï¼‰
};

// æ ¸å¿ƒæ“ä½œ
ggml_tensor* ggml_mul_mat(ctx, a, b);   // çŸ©é˜µä¹˜æ³•
ggml_tensor* ggml_add(ctx, a, b);       // åŠ æ³•
ggml_tensor* ggml_rope(ctx, a, ...);    // RoPEä½ç½®ç¼–ç 
```

## 5. ä»£ç é˜…è¯»è·¯çº¿å›¾

### 5.1 åˆå­¦è€…è·¯çº¿

```
ç¬¬ä¸€å‘¨ï¼šç†è§£åŸºæœ¬æµç¨‹
â”œâ”€> 1. examples/simple/simple.cpp
â”‚      æœ€ç®€å•çš„ä½¿ç”¨ç¤ºä¾‹ï¼ˆ~100è¡Œï¼‰
â”‚
â”œâ”€> 2. include/llama.h
â”‚      å…¬å…±APIæ¥å£å®šä¹‰
â”‚
â”œâ”€> 3. src/llama.cpp
â”‚      APIå®ç°çš„ä¸»å…¥å£
â”‚
â””â”€> 4. ggml/include/ggml.h
       å¼ é‡æ“ä½œAPIï¼ˆåªçœ‹æ³¨é‡Šå’Œå‡½æ•°ç­¾åï¼‰
```

### 5.2 è¿›é˜¶è·¯çº¿

```
ç¬¬äºŒå‘¨ï¼šæ·±å…¥æ ¸å¿ƒæ¨¡å—
â”œâ”€> 1. src/llama-model.cpp: llama_model_load_from_file()
â”‚      ç†è§£æ¨¡å‹åŠ è½½æµç¨‹
â”‚
â”œâ”€> 2. src/llama-context.cpp: llama_decode()
â”‚      ç†è§£æ¨ç†ä¸»å¾ªç¯
â”‚
â”œâ”€> 3. src/llama-graph.cpp: llama_build_graph()
â”‚      ç†è§£è®¡ç®—å›¾æ„å»º
â”‚
â””â”€> 4. src/llama-kv-cache.cpp
       ç†è§£KVç¼“å­˜æœºåˆ¶
```

### 5.3 ä¸“å®¶è·¯çº¿

```
ç¬¬ä¸‰å‘¨ï¼šæŒæ¡ä¼˜åŒ–æŠ€å·§
â”œâ”€> 1. ggml/src/ggml.c: ggml_compute_forward_*()
â”‚      ç†è§£å¼ é‡æ“ä½œå®ç°
â”‚
â”œâ”€> 2. ggml/src/ggml-cpu/: SIMDä¼˜åŒ–
â”‚      ç†è§£CPUä¼˜åŒ–æŠ€å·§
â”‚
â”œâ”€> 3. ggml/src/ggml-cuda/: GPUå®ç°
â”‚      ç†è§£GPUåŠ é€Ÿ
â”‚
â””â”€> 4. src/llama-sampling.cpp
       ç†è§£é«˜çº§é‡‡æ ·ç­–ç•¥
```

## 6. å®è·µç»ƒä¹ 

### ç»ƒä¹ 1ï¼šè·Ÿè¸ªç®€å•æ¨ç†

```bash
# ç¼–è¯‘debugç‰ˆæœ¬
cmake -B build -DCMAKE_BUILD_TYPE=Debug
cmake --build build

# ä½¿ç”¨gdbè·Ÿè¸ª
gdb --args ./build/bin/llama-cli -m model.gguf -p "Hello"

# è®¾ç½®æ–­ç‚¹
(gdb) break llama_model_load_from_file
(gdb) break llama_decode
(gdb) break llama_sample
(gdb) run

# è§‚å¯Ÿè°ƒç”¨æ ˆ
(gdb) bt
```

### ç»ƒä¹ 2ï¼šæ‰“å°æ¨¡å‹ä¿¡æ¯

åˆ›å»ºä¸€ä¸ªç®€å•ç¨‹åºæ‰“å°æ¨¡å‹ä¿¡æ¯ï¼š

```cpp
#include "llama.h"
#include <stdio.h>

int main() {
    // åˆå§‹åŒ–å‚æ•°
    llama_model_params model_params = llama_model_default_params();

    // åŠ è½½æ¨¡å‹
    llama_model* model = llama_model_load_from_file(
        "model.gguf",
        model_params
    );

    if (!model) {
        printf("Failed to load model\n");
        return 1;
    }

    // æ‰“å°æ¨¡å‹ä¿¡æ¯
    printf("Model info:\n");
    printf("  n_vocab: %d\n", llama_n_vocab(model));
    printf("  n_ctx_train: %d\n", llama_n_ctx_train(model));
    printf("  n_embd: %d\n", llama_n_embd(model));
    printf("  n_layer: %d\n", llama_n_layer(model));

    // æ¸…ç†
    llama_model_free(model);
    return 0;
}
```

### ç»ƒä¹ 3ï¼šç†è§£é…ç½®å‚æ•°

é˜…è¯»å¹¶ç†è§£å…³é”®é…ç½®å‚æ•°ï¼š

```cpp
// æ¨¡å‹å‚æ•°ï¼ˆåŠ è½½æ—¶ï¼‰
struct llama_model_params {
    int32_t  n_gpu_layers;  // å¸è½½åˆ°GPUçš„å±‚æ•°
    bool     use_mmap;      // æ˜¯å¦ä½¿ç”¨å†…å­˜æ˜ å°„
    bool     use_mlock;     // æ˜¯å¦é”å®šå†…å­˜
    // ...
};

// ä¸Šä¸‹æ–‡å‚æ•°ï¼ˆæ¨ç†æ—¶ï¼‰
struct llama_context_params {
    uint32_t n_ctx;        // ä¸Šä¸‹æ–‡çª—å£å¤§å°
    uint32_t n_batch;      // æ‰¹æ¬¡å¤§å°
    uint32_t n_ubatch;     // ç‰©ç†æ‰¹æ¬¡å¤§å°
    uint32_t n_threads;    // CPUçº¿ç¨‹æ•°
    // ...
};
```

## 7. å¸¸è§é—®é¢˜

**Q1: ä¸ºä»€ä¹ˆllama.cppæ€§èƒ½è¿™ä¹ˆå¥½ï¼Ÿ**
- âœ… é‡åŒ–æŠ€æœ¯ï¼ˆ4-bit, 8-bitï¼‰å‡å°‘å†…å­˜å ç”¨å’Œå¸¦å®½
- âœ… KVç¼“å­˜é¿å…é‡å¤è®¡ç®—
- âœ… SIMDæŒ‡ä»¤ä¼˜åŒ–ï¼ˆAVX2, NEONï¼‰
- âœ… GPUåŠ é€Ÿï¼ˆCUDA, Metalï¼‰
- âœ… å†…å­˜æ˜ å°„å‡å°‘åŠ è½½æ—¶é—´

**Q2: GGMLå’ŒPyTorchæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**
```
PyTorch:
- Pythonæ¥å£ï¼Œæ˜“ç”¨æ€§å¥½
- è‡ªåŠ¨å¾®åˆ†ï¼ˆé€‚åˆè®­ç»ƒï¼‰
- ä¾èµ–é‡ï¼ˆCUDA, cuDNNç­‰ï¼‰
- å†…å­˜å ç”¨å¤§

GGML:
- Cæ¥å£ï¼Œæ€§èƒ½ä¼˜å…ˆ
- åªå®ç°å‰å‘ä¼ æ’­ï¼ˆæ¨ç†ä¼˜åŒ–ï¼‰
- é›¶ä¾èµ–
- æ”¯æŒé‡åŒ–ï¼Œå†…å­˜é«˜æ•ˆ
```

**Q3: å¦‚ä½•é€‰æ‹©åç«¯ï¼Ÿ**
```
CPU:
- é€‚åˆå°æ¨¡å‹ (<7B)
- æ— GPUæ—¶çš„fallback

CUDA:
- NVIDIA GPU
- æœ€æˆç†Ÿçš„GPUåç«¯

Metal:
- Apple Silicon (M1/M2/M3)
- macOSåŸç”ŸåŠ é€Ÿ

SYCL:
- Intel GPU
- è·¨å¹³å°ç»Ÿä¸€æ¥å£
```

## 8. ä¸‹ä¸€æ­¥

æ­å–œå®ŒæˆDay 1ï¼æ˜å¤©æˆ‘ä»¬å°†æ·±å…¥å­¦ä¹ ï¼š

**Day 2 é¢„å‘Šï¼šGGMLå¼ é‡åº“åŸºç¡€**
- å¼ é‡æ•°æ®ç»“æ„
- å†…å­˜å¸ƒå±€ä¸stride
- åŸºæœ¬å¼ é‡æ“ä½œ
- è®¡ç®—å›¾æ¦‚å¿µ

## ä½œä¸š

1. âœï¸ ç»˜åˆ¶llama.cppçš„æ¨¡å—ä¾èµ–å›¾
2. ğŸ” ä½¿ç”¨gdbå•æ­¥è·Ÿè¸ªä¸€æ¬¡ç®€å•æ¨ç†
3. ğŸ“– é˜…è¯» `include/llama.h` ä¸­çš„æ‰€æœ‰APIæ³¨é‡Š
4. ğŸ’» ç¼–è¯‘å¹¶è¿è¡Œ `examples/simple/simple.cpp`

---

**ç»§ç»­å­¦ä¹ **: [Day 2: GGMLå¼ é‡åº“åŸºç¡€](day02-ggml-basics.md) â†’
