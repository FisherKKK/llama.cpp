# Day 9: KVç¼“å­˜æœºåˆ¶è¯¦è§£

> ğŸ¯ **å­¦ä¹ ç›®æ ‡**ï¼šæ·±å…¥ç†è§£KVç¼“å­˜çš„å®ç°åŸç†ã€ä¼˜åŒ–æŠ€å·§å’Œå†…å­˜ç®¡ç†ç­–ç•¥ã€‚

## 1. ä¸ºä»€ä¹ˆéœ€è¦KVç¼“å­˜ï¼Ÿ

### 1.1 é—®é¢˜èƒŒæ™¯

åœ¨Transformerè§£ç è¿‡ç¨‹ä¸­ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–°tokenéƒ½éœ€è¦ï¼š

```
ç”Ÿæˆç¬¬1ä¸ªtoken:
  Q1 @ [K1, V1] -> Output1

ç”Ÿæˆç¬¬2ä¸ªtoken:
  Q2 @ [K1, K2, V1, V2] -> Output2  âŒ K1, V1 éœ€è¦é‡æ–°è®¡ç®—

ç”Ÿæˆç¬¬3ä¸ªtoken:
  Q3 @ [K1, K2, K3, V1, V2, V3] -> Output3  âŒ å‰é¢çš„K, Véƒ½è¦é‡æ–°è®¡ç®—

...

æ—¶é—´å¤æ‚åº¦: O(nÂ²) - éšåºåˆ—é•¿åº¦å¹³æ–¹å¢é•¿ï¼
```

### 1.2 KVç¼“å­˜çš„è§£å†³æ–¹æ¡ˆ

```
ä½¿ç”¨KVç¼“å­˜ï¼š

Token 1: è®¡ç®—K1, V1 -> ç¼“å­˜
Token 2: è®¡ç®—K2, V2 -> ç¼“å­˜ï¼Œä½¿ç”¨ç¼“å­˜çš„[K1, K2], [V1, V2]  âœ…
Token 3: è®¡ç®—K3, V3 -> ç¼“å­˜ï¼Œä½¿ç”¨ç¼“å­˜çš„[K1, K2, K3], [V1, V2, V3]  âœ…

æ—¶é—´å¤æ‚åº¦: O(n) - çº¿æ€§å¢é•¿ï¼
å†…å­˜å¼€é”€: O(n * n_layer * n_embd) - éœ€è¦é¢å¤–å†…å­˜
```

### 1.3 æ€§èƒ½å¯¹æ¯”

```
ä»¥7Bæ¨¡å‹ã€ç”Ÿæˆ2048 tokensä¸ºä¾‹ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ–¹æ³•        â”‚ è®¡ç®—é‡        â”‚ ç”Ÿæˆé€Ÿåº¦      â”‚ å†…å­˜    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ— KVç¼“å­˜    â”‚ ~4M FLOPs     â”‚ 0.5 tok/s     â”‚ 14GB    â”‚
â”‚ æœ‰KVç¼“å­˜    â”‚ ~2K FLOPs     â”‚ 50 tok/s      â”‚ 16GB    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ åŠ é€Ÿæ¯”      â”‚ 2000x         â”‚ 100x          â”‚ +14%    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç»“è®ºï¼šç”¨å°‘é‡å†…å­˜æ¢å–å·¨å¤§çš„æ€§èƒ½æå‡ï¼
```

## 2. KVç¼“å­˜æ•°æ®ç»“æ„

### 2.1 æ ¸å¿ƒç»“æ„

```cpp
// æ–‡ä»¶: src/llama-kv-cache.h

struct llama_kv_cache {
    const llama_model& model;
    const llama_hparams& hparams;

    // ç¼“å­˜ç»´åº¦ä¿¡æ¯
    bool   v_trans;        // Væ˜¯å¦è½¬ç½®å­˜å‚¨
    uint32_t n_seq_max;    // æœ€å¤§åºåˆ—æ•°ï¼ˆæ‰¹å¤„ç†ï¼‰
    uint32_t n_stream;     // æµæ•°é‡ï¼ˆç»Ÿä¸€/ç‹¬ç«‹ï¼‰
    uint32_t n_pad;        // paddingå¤§å°
    uint32_t n_swa;        // Sliding Window Attentionå¤§å°
    llama_swa_type swa_type;  // SWAç±»å‹

    // æ¯å±‚çš„KVå¼ é‡
    struct layer {
        uint32_t il;                       // å±‚ç´¢å¼•
        ggml_tensor* k;                    // Kç¼“å­˜å¼ é‡ [n_embd_k, kv_size, n_stream]
        ggml_tensor* v;                    // Vç¼“å­˜å¼ é‡ [n_embd_v, kv_size, n_stream]
        std::vector<ggml_tensor*> k_stream; // æ¯ä¸ªstreamçš„Kè§†å›¾
        std::vector<ggml_tensor*> v_stream; // æ¯ä¸ªstreamçš„Vè§†å›¾
    };
    std::vector<layer> layers;             // æ‰€æœ‰å±‚çš„ç¼“å­˜

    // ç¼“å­˜å•å…ƒç®¡ç†
    std::vector<std::vector<llama_kv_cell>> v_cells;  // [stream][pos]
    std::vector<uint32_t> v_heads;                     // æ¯ä¸ªstreamçš„å½“å‰å¤´ä½ç½®

    // åºåˆ—åˆ°streamçš„æ˜ å°„
    std::vector<uint32_t> seq_to_stream;               // [seq_id] -> stream_id

    // å±‚IDæ˜ å°„
    std::map<uint32_t, uint32_t> map_layer_ids;       // layer_id -> index

    // å†…å­˜ç®¡ç†
    std::vector<std::pair<ggml_context_ptr, ggml_backend_buffer_t>> ctxs_bufs;
};
```

### 2.2 ç¼“å­˜å•å…ƒ

```cpp
// æ–‡ä»¶: src/llama-kv-cells.h

struct llama_kv_cell {
    llama_pos pos   = -1;  // åœ¨åºåˆ—ä¸­çš„ä½ç½®
    llama_pos delta = 0;   // RoPE deltaï¼ˆç”¨äºä½ç½®ç¼–ç ï¼‰

    int32_t src = -1;      // æºå•å…ƒï¼ˆç”¨äºå¤åˆ¶ï¼‰
    int32_t tail = -1;     // é“¾è¡¨å°¾ï¼ˆç”¨äºç®¡ç†ï¼‰

    std::set<llama_seq_id> seq_id;  // æ­¤å•å…ƒå±äºå“ªäº›åºåˆ—

    bool has_seq_id(const llama_seq_id & id) const {
        return seq_id.find(id) != seq_id.end();
    }

    bool is_empty() const {
        return seq_id.empty();
    }

    bool is_same_seq(const llama_kv_cell & other) const {
        return seq_id == other.seq_id;
    }
};
```

### 2.3 å†…å­˜å¸ƒå±€

```
å¯¹äºä¸€ä¸ª32å±‚ã€hidden_dim=4096ã€n_head=32çš„æ¨¡å‹ï¼š

Kç¼“å­˜æ¯å±‚: [n_embd_k, kv_size, n_stream]
         = [4096, 2048, 1]
         = 33MB (FP16)

Vç¼“å­˜æ¯å±‚: [n_embd_v, kv_size, n_stream]
         = [4096, 2048, 1]
         = 33MB (FP16)

æ€»KVç¼“å­˜: (33MB + 33MB) * 32å±‚ = 2.1GB

é‡åŒ–åˆ°Q8_0: 2.1GB * 0.5 = 1.05GB  âœ… èŠ‚çœ50%å†…å­˜
é‡åŒ–åˆ°Q4_0: 2.1GB * 0.25 = 525MB  âœ… èŠ‚çœ75%å†…å­˜

ç‰©ç†å¸ƒå±€ï¼ˆè¿ç»­å†…å­˜ï¼‰ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer 0                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ K: [4096 x 2048]               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ V: [4096 x 2048]               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Layer 1                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ K: [4096 x 2048]               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ V: [4096 x 2048]               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ...                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3. KVç¼“å­˜å®ç°

### 3.1 åˆå§‹åŒ–

```cpp
// æ–‡ä»¶: src/llama-kv-cache.cpp

llama_kv_cache::llama_kv_cache(
        const llama_model& model,
        ggml_type type_k,              // Kçš„æ•°æ®ç±»å‹
        ggml_type type_v,              // Vçš„æ•°æ®ç±»å‹
        bool v_trans,                  // Væ˜¯å¦è½¬ç½®
        bool offload,                  // æ˜¯å¦å¸è½½åˆ°GPU
        bool unified,                  // æ˜¯å¦ç»Ÿä¸€ç¼“å­˜ï¼ˆå¤šåºåˆ—ï¼‰
        uint32_t kv_size,              // ç¼“å­˜å¤§å°
        uint32_t n_seq_max,            // æœ€å¤§åºåˆ—æ•°
        uint32_t n_pad,                // padding
        uint32_t n_swa,                // SWAå¤§å°
        llama_swa_type swa_type,       // SWAç±»å‹
        const layer_filter_cb& filter, // å±‚è¿‡æ»¤
        const layer_reuse_cb& reuse)   // å±‚å¤ç”¨
    : model(model), hparams(model.hparams),
      v_trans(v_trans), n_seq_max(n_seq_max),
      n_stream(unified ? 1 : n_seq_max),
      n_pad(n_pad), n_swa(n_swa), swa_type(swa_type) {

    // æ£€æŸ¥å¯¹é½
    GGML_ASSERT(kv_size % n_pad == 0);

    // åˆå§‹åŒ–stream
    v_heads.resize(n_stream, 0);
    v_cells.resize(n_stream);
    for (uint32_t s = 0; s < n_stream; ++s) {
        v_cells[s].resize(kv_size);
    }

    // åºåˆ—åˆ°streamæ˜ å°„
    seq_to_stream.resize(LLAMA_MAX_SEQ, 0);
    if (n_stream > 1) {
        for (uint32_t s = 0; s < n_stream; ++s) {
            seq_to_stream[s] = s;
        }
    }

    // ä¸ºæ¯å±‚åˆ›å»ºKã€Vå¼ é‡
    for (uint32_t il = 0; il < hparams.n_layer; il++) {
        if (!hparams.has_kv(il)) continue;
        if (filter && !filter(il)) continue;

        const uint32_t n_embd_k_gqa = hparams.n_embd_k_gqa(il);
        const uint32_t n_embd_v_gqa = !v_trans ?
            hparams.n_embd_v_gqa(il) : hparams.n_embd_v_gqa_max();

        // é€‰æ‹©è®¾å¤‡
        ggml_backend_buffer_type_t buft = ggml_backend_cpu_buffer_type();
        if (offload) {
            auto* dev = model.dev_layer(il);
            buft = ggml_backend_dev_buffer_type(dev);
        }

        // åˆ›å»ºä¸Šä¸‹æ–‡
        ggml_context* ctx = ctx_for_buft(buft);

        // åˆ›å»ºKã€Vå¼ é‡
        ggml_tensor* k = ggml_new_tensor_3d(ctx, type_k,
                                           n_embd_k_gqa, kv_size, n_stream);
        ggml_tensor* v = ggml_new_tensor_3d(ctx, type_v,
                                           n_embd_v_gqa, kv_size, n_stream);

        ggml_format_name(k, "cache_k_l%d", il);
        ggml_format_name(v, "cache_v_l%d", il);

        // ä¸ºæ¯ä¸ªstreamåˆ›å»ºè§†å›¾
        std::vector<ggml_tensor*> k_stream, v_stream;
        for (uint32_t s = 0; s < n_stream; ++s) {
            k_stream.push_back(ggml_view_2d(ctx, k,
                n_embd_k_gqa, kv_size, k->nb[1], s * k->nb[2]));
            v_stream.push_back(ggml_view_2d(ctx, v,
                n_embd_v_gqa, kv_size, v->nb[1], s * v->nb[2]));
        }

        map_layer_ids[il] = layers.size();
        layers.push_back({il, k, v, k_stream, v_stream});
    }

    // åˆ†é…å†…å­˜å¹¶åˆå§‹åŒ–ä¸º0
    for (auto& [buft, ctx] : ctx_map) {
        ggml_backend_buffer_t buf = ggml_backend_alloc_ctx_tensors_from_buft(
            ctx.get(), buft);
        ggml_backend_buffer_clear(buf, 0);
        ctxs_bufs.emplace_back(std::move(ctx), buf);
    }
}
```

### 3.2 æ›´æ–°ç¼“å­˜

```cpp
// æ–‡ä»¶: src/llama-kv-cache.cpp

void llama_kv_cache_update(llama_kv_cache& cache) {
    // éå†æ‰€æœ‰stream
    for (uint32_t s = 0; s < cache.n_stream; ++s) {
        uint32_t& head = cache.v_heads[s];
        auto& cells = cache.v_cells[s];

        // Sliding Window Attentionå¤„ç†
        if (cache.n_swa > 0 && head >= cache.n_swa) {
            // ç§»åŠ¨çª—å£
            const uint32_t swa_offset = head - cache.n_swa + 1;

            // æ¸…ç†è¶…å‡ºçª—å£çš„å•å…ƒ
            for (uint32_t i = 0; i < swa_offset; ++i) {
                cells[i].pos = -1;
                cells[i].seq_id.clear();
            }

            // è°ƒæ•´ä½ç½®
            for (uint32_t i = swa_offset; i < head; ++i) {
                if (cells[i].pos >= 0) {
                    cells[i].pos -= swa_offset;
                }
            }

            head = cache.n_swa - 1;
        }
    }
}
```

### 3.3 åºåˆ—æ“ä½œ

```cpp
// æ·»åŠ åºåˆ—
void llama_kv_cache_seq_add(
        llama_kv_cache& cache,
        llama_seq_id seq_id,
        llama_pos p0,
        llama_pos p1,
        llama_pos delta) {

    uint32_t s = cache.seq_to_stream[seq_id];
    auto& cells = cache.v_cells[s];

    for (uint32_t i = 0; i < cells.size(); ++i) {
        auto& cell = cells[i];

        if (cell.has_seq_id(seq_id) &&
            cell.pos >= p0 && cell.pos < p1) {
            cell.delta += delta;
            cell.pos += delta;
        }
    }
}

// ç§»é™¤åºåˆ—
void llama_kv_cache_seq_rm(
        llama_kv_cache& cache,
        llama_seq_id seq_id,
        llama_pos p0,
        llama_pos p1) {

    uint32_t s = cache.seq_to_stream[seq_id];
    auto& cells = cache.v_cells[s];

    for (uint32_t i = 0; i < cells.size(); ++i) {
        auto& cell = cells[i];

        if (cell.has_seq_id(seq_id) &&
            (p0 < 0 || (cell.pos >= p0 && cell.pos < p1))) {
            cell.seq_id.erase(seq_id);

            if (cell.is_empty()) {
                cell.pos = -1;
            }
        }
    }
}

// å¤åˆ¶åºåˆ—
void llama_kv_cache_seq_cp(
        llama_kv_cache& cache,
        llama_seq_id seq_id_src,
        llama_seq_id seq_id_dst,
        llama_pos p0,
        llama_pos p1) {

    uint32_t s_src = cache.seq_to_stream[seq_id_src];
    uint32_t s_dst = cache.seq_to_stream[seq_id_dst];

    if (s_src == s_dst) {
        // åŒä¸€streamï¼Œç›´æ¥æ·»åŠ åºåˆ—ID
        auto& cells = cache.v_cells[s_src];
        for (auto& cell : cells) {
            if (cell.has_seq_id(seq_id_src) &&
                (p0 < 0 || (cell.pos >= p0 && cell.pos < p1))) {
                cell.seq_id.insert(seq_id_dst);
            }
        }
    } else {
        // ä¸åŒstreamï¼Œéœ€è¦å¤åˆ¶æ•°æ®
        // å®ç°ç•¥...
    }
}
```

## 4. è®¡ç®—å›¾ä¸­çš„KVç¼“å­˜

### 4.1 åœ¨æ³¨æ„åŠ›ä¸­ä½¿ç”¨

```cpp
// æ–‡ä»¶: src/llama-graph.cpp

static struct ggml_tensor * llm_build_kv(
        struct ggml_context * ctx,
        const llama_model& model,
        const llama_hparams& hparams,
        const llama_ubatch& batch,
        struct ggml_tensor * k_cur,  // å½“å‰çš„K [n_embd_k_gqa, n_tokens]
        struct ggml_tensor * v_cur,  // å½“å‰çš„V [n_embd_v_gqa, n_tokens]
        struct ggml_tensor * kv_pe,  // ä½ç½®ç¼–ç 
        llama_kv_cache& kv_cache,
        int il) {                     // å±‚ç´¢å¼•

    const auto& layer = kv_cache.layers[kv_cache.map_layer_ids.at(il)];
    const uint32_t n_tokens = batch.n_tokens;

    // 1. åº”ç”¨RoPEåˆ°å½“å‰K
    if (kv_pe) {
        k_cur = ggml_rope_ext(..., k_cur, kv_pe, ...);
    }

    // 2. æ›´æ–°KVç¼“å­˜
    // è¿™æ˜¯ä¸€ä¸ªè‡ªå®šä¹‰æ“ä½œï¼Œå°†k_curå’Œv_curå†™å…¥ç¼“å­˜
    struct ggml_tensor * k_cache_view = ggml_view_1d(ctx, layer.k,
        n_tokens * n_embd_k_gqa, /* offset = */ ...);
    struct ggml_tensor * v_cache_view = ggml_view_1d(ctx, layer.v,
        n_tokens * n_embd_v_gqa, /* offset = */ ...);

    ggml_build_forward_expand(&gf,
        ggml_cpy(ctx, k_cur, k_cache_view));
    ggml_build_forward_expand(&gf,
        ggml_cpy(ctx, v_cur, v_cache_view));

    // 3. è·å–æ‰€æœ‰éœ€è¦çš„Kå’ŒVï¼ˆåŒ…æ‹¬ç¼“å­˜çš„ï¼‰
    struct ggml_tensor * k = ggml_view_2d(ctx, layer.k,
        n_embd_k_gqa, n_kv, /* stride = */ ..., /* offset = */ ...);
    struct ggml_tensor * v = ggml_view_2d(ctx, layer.v,
        n_kv, n_embd_v_gqa, /* stride = */ ..., /* offset = */ ...);

    return {k, v};
}

// ä½¿ç”¨ç¤ºä¾‹
static struct ggml_tensor * llm_build_attn(
        struct ggml_context * ctx,
        ...,
        struct ggml_tensor * q_cur,  // [n_embd, n_tokens]
        struct ggml_tensor * k_cur,  // [n_embd_k_gqa, n_tokens]
        struct ggml_tensor * v_cur,  // [n_embd_v_gqa, n_tokens]
        ...) {

    // è·å–å®Œæ•´çš„Kå’ŒVï¼ˆåŒ…æ‹¬ç¼“å­˜ï¼‰
    auto [k, v] = llm_build_kv(ctx, model, hparams, batch,
                                k_cur, v_cur, kv_pe, kv_cache, il);

    // Q @ K^T
    struct ggml_tensor * kq = ggml_mul_mat(ctx, k, q);  // [n_tokens, n_kv]

    // Mask (å¯é€‰)
    if (mask) {
        kq = ggml_add(ctx, kq, mask);
    }

    // Softmax
    kq = ggml_soft_max_ext(ctx, kq, ...);

    // @ V
    struct ggml_tensor * kqv = ggml_mul_mat(ctx, v, kq);  // [n_embd_v, n_tokens]

    return kqv;
}
```

## 5. ä¼˜åŒ–æŠ€å·§

### 5.1 é‡åŒ–KVç¼“å­˜

```cpp
// ä¸åŒé‡åŒ–çº§åˆ«çš„æƒè¡¡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç±»å‹    â”‚ ç²¾åº¦     â”‚ å¤§å°     â”‚ è´¨é‡     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ F16     â”‚ 16-bit   â”‚ 2.1GB    â”‚ 100%     â”‚
â”‚ Q8_0    â”‚ 8-bit    â”‚ 1.05GB   â”‚ 99.5%    â”‚
â”‚ Q6_K    â”‚ 6-bit    â”‚ 800MB    â”‚ 98%      â”‚
â”‚ Q4_0    â”‚ 4-bit    â”‚ 525MB    â”‚ 95%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä½¿ç”¨æ–¹æ³•ï¼š
llama_context_params cparams = llama_context_default_params();
cparams.type_k = GGML_TYPE_Q8_0;  // é‡åŒ–K
cparams.type_v = GGML_TYPE_Q8_0;  // é‡åŒ–V
```

### 5.2 Vè½¬ç½®å­˜å‚¨

```cpp
// é—®é¢˜ï¼šVåœ¨æ³¨æ„åŠ›è®¡ç®—æ—¶éœ€è¦è½¬ç½®
// attn = softmax(Q @ K^T) @ V
//                            â†‘ Véœ€è¦æŒ‰åˆ—è®¿é—®

// æ–¹æ¡ˆ1ï¼šè¿è¡Œæ—¶è½¬ç½®ï¼ˆæ…¢ï¼‰
V_transposed = ggml_transpose(ctx, V);  // æ¯æ¬¡éƒ½è½¬ç½®

// æ–¹æ¡ˆ2ï¼šé¢„è½¬ç½®å­˜å‚¨ï¼ˆå¿«ï¼‰
// åœ¨ç¼“å­˜æ—¶å°±è½¬ç½®Vï¼ŒèŠ‚çœè¿è¡Œæ—¶å¼€é”€
cparams.v_trans = true;

// å†…å­˜å¸ƒå±€å¯¹æ¯”ï¼š
æ­£å¸¸: V [n_embd_v, n_kv]    æŒ‰è¡Œå­˜å‚¨ [v0_0, v0_1, ..., v1_0, v1_1, ...]
è½¬ç½®: V [n_kv, n_embd_v]    æŒ‰åˆ—å­˜å‚¨ [v0_0, v1_0, ..., v0_1, v1_1, ...]
                            â†‘ è®¿é—®æ—¶ç¼“å­˜å‹å¥½ï¼
```

### 5.3 Sliding Window Attention (SWA)

```cpp
// é•¿åºåˆ—é—®é¢˜ï¼šKVç¼“å­˜çº¿æ€§å¢é•¿
// 2048 tokens: 2.1GB
// 4096 tokens: 4.2GB
// 8192 tokens: 8.4GB  âŒ å†…å­˜çˆ†ç‚¸

// SWAè§£å†³æ–¹æ¡ˆï¼šåªä¿ç•™æœ€è¿‘çš„Nä¸ªtokens
cparams.n_swa = 2048;  // çª—å£å¤§å°

// æ•ˆæœï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Tokens: [0, 1, 2, ..., 8191]        â”‚
â”‚                                      â”‚
â”‚ Window: [6144, 6145, ..., 8191]     â”‚â† åªä¿ç•™è¿™éƒ¨åˆ†
â”‚         â””â”€â”€â”€â”€â”€â”€â”€ 2048 â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                      â”‚
â”‚ Memory: å›ºå®š2.1GB âœ…                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

// æ€§èƒ½å½±å“ï¼š
â€¢ å¯¹é•¿åºåˆ—ï¼šå‡ ä¹æ— å½±å“
â€¢ å¯¹ä¾èµ–è¿œè·ç¦»ä¸Šä¸‹æ–‡çš„ä»»åŠ¡ï¼šå¯èƒ½æœ‰å½±å“
```

### 5.4 æ‰¹å¤„ç†ä¼˜åŒ–

```cpp
// é—®é¢˜ï¼šå¤šä¸ªåºåˆ—å¹¶è¡Œæ—¶å¦‚ä½•ç®¡ç†KVç¼“å­˜ï¼Ÿ

// æ–¹æ¡ˆ1ï¼šç‹¬ç«‹ç¼“å­˜ï¼ˆæ¯ä¸ªåºåˆ—ä¸€ä¸ªstreamï¼‰
n_stream = n_seq_max;
// ä¼˜ç‚¹ï¼šå®Œå…¨éš”ç¦»ï¼Œçµæ´»
// ç¼ºç‚¹ï¼šå†…å­˜å ç”¨å¤§

// æ–¹æ¡ˆ2ï¼šç»Ÿä¸€ç¼“å­˜ï¼ˆæ‰€æœ‰åºåˆ—å…±äº«ä¸€ä¸ªstreamï¼‰
n_stream = 1;
// ä¼˜ç‚¹ï¼šå†…å­˜èŠ‚çœ
// ç¼ºç‚¹ï¼šéœ€è¦ä»”ç»†ç®¡ç†seq_id

// ç¤ºä¾‹ï¼šæ‰¹å¤„ç†3ä¸ªåºåˆ—
llama_batch batch = llama_batch_init(512, 0, 3);

// åºåˆ—0: "Hello"
batch.token[0] = token_hello;
batch.pos[0] = 0;
batch.n_seq_id[0] = 1;
batch.seq_id[0][0] = 0;  // seq_id = 0

// åºåˆ—1: "World"
batch.token[1] = token_world;
batch.pos[1] = 0;
batch.n_seq_id[1] = 1;
batch.seq_id[1][0] = 1;  // seq_id = 1

// ...

llama_decode(ctx, batch);
// KVç¼“å­˜è‡ªåŠ¨ç®¡ç†æ¯ä¸ªåºåˆ—çš„ä½ç½®
```

## 6. å®è·µç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæŸ¥çœ‹KVç¼“å­˜å¤§å°

```cpp
#include "llama.h"
#include <stdio.h>

void print_kv_cache_info(llama_context* ctx) {
    // è·å–ç¼“å­˜å¤§å°
    size_t size_k = llama_get_state_size(ctx) / 2;  // è¿‘ä¼¼
    size_t size_v = llama_get_state_size(ctx) / 2;

    printf("KV Cache Info:\n");
    printf("  K size: %.2f MB\n", size_k / 1024.0 / 1024.0);
    printf("  V size: %.2f MB\n", size_v / 1024.0 / 1024.0);
    printf("  Total: %.2f MB\n", (size_k + size_v) / 1024.0 / 1024.0);
}
```

### ç»ƒä¹ 2ï¼šæµ‹è¯•ä¸åŒé‡åŒ–çº§åˆ«

```bash
#!/bin/bash

for type in "f16" "q8_0" "q6_k" "q4_0"; do
    echo "Testing type_k=$type"
    time ./llama-cli -m model.gguf \
        --ctx-size 2048 \
        --cache-type-k $type \
        --cache-type-v $type \
        -p "Hello world" -n 100
done
```

### ç»ƒä¹ 3ï¼šå®ç°ç®€å•çš„ç¼“å­˜ç®¡ç†

```cpp
// ç®€åŒ–ç‰ˆKVç¼“å­˜ç®¡ç†
struct SimpleKVCache {
    std::vector<std::vector<float>> k_cache;  // [n_layer][...]
    std::vector<std::vector<float>> v_cache;
    size_t current_pos = 0;
    size_t capacity = 2048;

    void update(int layer, const std::vector<float>& k, const std::vector<float>& v) {
        if (current_pos >= capacity) {
            // ç®€å•ç­–ç•¥ï¼šè¦†ç›–æœ€æ—§çš„
            current_pos = 0;
        }

        // å¤åˆ¶åˆ°ç¼“å­˜
        // ... å®ç°ç•¥
        current_pos++;
    }

    std::pair<std::vector<float>, std::vector<float>> get(int layer) {
        // è¿”å›å½“å‰æ‰€æœ‰ç¼“å­˜çš„Kå’ŒV
        // ... å®ç°ç•¥
    }
};
```

## 7. è°ƒè¯•æŠ€å·§

```cpp
// 1. æ‰“å°ç¼“å­˜ä½¿ç”¨æƒ…å†µ
void debug_kv_cache(llama_context* ctx) {
    // è·å–ä½¿ç”¨çš„å•å…ƒæ•°
    int used_cells = 0;
    // ... éå†cellsç»Ÿè®¡

    printf("KV Cache Usage: %d / %d (%.1f%%)\n",
           used_cells, total_cells,
           100.0 * used_cells / total_cells);
}

// 2. éªŒè¯ç¼“å­˜ä¸€è‡´æ€§
void verify_kv_cache(llama_context* ctx) {
    // æ£€æŸ¥æ¯ä¸ªå•å…ƒçš„seq_idæ˜¯å¦æœ‰æ•ˆ
    // æ£€æŸ¥posæ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
    // ... å®ç°ç•¥
}

// 3. å¯è§†åŒ–ç¼“å­˜çŠ¶æ€
void visualize_kv_cache(llama_context* ctx) {
    // æ‰“å°æ¯ä¸ªä½ç½®çš„å ç”¨æƒ…å†µ
    // X = å·²å ç”¨, . = ç©ºé—²
    // [XXXXXX.................XXXXX]
    // ... å®ç°ç•¥
}
```

## 8. å¸¸è§é—®é¢˜

**Q1: KVç¼“å­˜ä¸ºä»€ä¹ˆè¿™ä¹ˆå¤§ï¼Ÿ**
- æ¯ä¸ªtokenåœ¨æ¯å±‚éƒ½è¦ä¿å­˜Kå’ŒV
- 7Bæ¨¡å‹32å±‚ï¼Œæ¯å±‚hidden_dim=4096
- 2048 tokens Ã— 32å±‚ Ã— 4096 Ã— 2 (K+V) Ã— 2å­—èŠ‚(FP16) = 2.1GB

**Q2: èƒ½å¦åªç¼“å­˜éƒ¨åˆ†å±‚ï¼Ÿ**
å¯ä»¥ï¼Œä½¿ç”¨layer_filterå›è°ƒï¼š
```cpp
auto filter = [](int layer) {
    return layer % 2 == 0;  // åªç¼“å­˜å¶æ•°å±‚
};
kv_cache = llama_kv_cache(model, ..., filter, nullptr);
```

**Q3: å¦‚ä½•é€‰æ‹©é‡åŒ–çº§åˆ«ï¼Ÿ**
```
Q8_0: æœ€ä½³é€‰æ‹©ï¼ŒæŸå¤±<0.5%ï¼ŒèŠ‚çœ50%å†…å­˜
Q6_K: æ¿€è¿›ä¼˜åŒ–ï¼ŒæŸå¤±~2%ï¼ŒèŠ‚çœ62.5%å†…å­˜
Q4_0: æé™å‹ç¼©ï¼ŒæŸå¤±~5%ï¼ŒèŠ‚çœ75%å†…å­˜
```

**Q4: Unified vs Independent ç¼“å­˜ï¼Ÿ**
```
Unified (n_stream=1):
- é€‚åˆï¼šå•åºåˆ—æˆ–å°‘é‡åºåˆ—
- ä¼˜ç‚¹ï¼šå†…å­˜æœ€çœ
- ç¼ºç‚¹ï¼šç®¡ç†å¤æ‚

Independent (n_stream=n_seq):
- é€‚åˆï¼šå¤§é‡å¹¶è¡Œåºåˆ—
- ä¼˜ç‚¹ï¼šç®¡ç†ç®€å•
- ç¼ºç‚¹ï¼šå†…å­˜å ç”¨å¤§
```

## 9. ä¸‹ä¸€æ­¥

**Day 10 é¢„å‘Šï¼šæ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–**
- FlashAttentionåŸç†
- åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA)
- å¤šæŸ¥è¯¢æ³¨æ„åŠ›(MQA)
- ä½ç½®ç¼–ç ä¼˜åŒ–

## ä½œä¸š

1. âœï¸ è®¡ç®—ä½ çš„æ¨¡å‹éœ€è¦å¤šå¤§KVç¼“å­˜
2. ğŸ” ä½¿ç”¨gdbæŸ¥çœ‹å®é™…çš„ç¼“å­˜å¸ƒå±€
3. ğŸ“– é˜…è¯» `src/llama-kv-cache.cpp` å®Œæ•´å®ç°
4. ğŸ’» å®ç°ä¸€ä¸ªç®€å•çš„KVç¼“å­˜å¯è§†åŒ–å·¥å…·

---

**ç»§ç»­å­¦ä¹ **: [Day 10: æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–](day10-attention-optimization.md) â†’
