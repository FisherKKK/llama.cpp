# Day 7: è®¡ç®—å›¾æ„å»ºä¸è°ƒåº¦

## è¯¾ç¨‹ç›®æ ‡

æ·±å…¥ç†è§£è®¡ç®—å›¾çš„æ„å»ºä¸æ‰§è¡Œï¼š
- llama_graph_builder å®ç°
- æ„å»ºå®Œæ•´çš„ Transformer è®¡ç®—å›¾
- åç«¯è°ƒåº¦å™¨ï¼ˆSchedulerï¼‰åŸç†
- å›¾ä¼˜åŒ–æŠ€æœ¯
- å¤šåç«¯ååŒå·¥ä½œ

## 1. è®¡ç®—å›¾å›é¡¾

### 1.1 ä¸ºä»€ä¹ˆéœ€è¦è®¡ç®—å›¾ï¼Ÿ

**ä¼ ç»Ÿç«‹å³æ‰§è¡Œ**ï¼š
```cpp
// æ¯ä¸ªæ“ä½œç«‹å³è®¡ç®—
Tensor c = add(a, b);    // ç«‹å³æ‰§è¡Œ
Tensor d = mul(c, 2);    // ç«‹å³æ‰§è¡Œ
Tensor e = relu(d);      // ç«‹å³æ‰§è¡Œ
```

**è®¡ç®—å›¾å»¶è¿Ÿæ‰§è¡Œ**ï¼š
```cpp
// åªæ„å»ºå›¾ç»“æ„ï¼Œä¸è®¡ç®—
Tensor c = ggml_add(ctx, a, b);    // åªè®°å½•æ“ä½œ
Tensor d = ggml_mul(ctx, c, 2);    // åªè®°å½•æ“ä½œ
Tensor e = ggml_relu(ctx, d);      // åªè®°å½•æ“ä½œ

// ç»Ÿä¸€æ‰§è¡Œ
ggml_graph_compute(graph);         // ä¸€æ¬¡æ€§æ‰§è¡Œæ‰€æœ‰æ“ä½œ
```

**ä¼˜åŠ¿**ï¼š
- âœ… å…¨å±€ä¼˜åŒ–ï¼šå¯ä»¥åˆ†ææ•´ä¸ªå›¾ï¼Œè¿›è¡Œèåˆä¼˜åŒ–
- âœ… å†…å­˜ç®¡ç†ï¼šå¯ä»¥é‡ç”¨ä¸­é—´ç»“æœçš„å†…å­˜
- âœ… å¹¶è¡Œè°ƒåº¦ï¼šå¯ä»¥å¹¶è¡Œæ‰§è¡Œæ— ä¾èµ–çš„æ“ä½œ
- âœ… è·¨è®¾å¤‡ï¼šå¯ä»¥æ™ºèƒ½åˆ†é…ä¸åŒè®¾å¤‡

## 2. è®¡ç®—å›¾ç»“æ„è¯¦è§£

### 2.1 å›¾çš„æ•°æ®ç»“æ„

```c
// ä½ç½®ï¼šggml/include/ggml.h:625
struct ggml_cgraph {
    int size;          // å›¾çš„æœ€å¤§å®¹é‡
    int n_nodes;       // å½“å‰èŠ‚ç‚¹æ•°
    int n_leafs;       // å¶å­èŠ‚ç‚¹æ•°ï¼ˆè¾“å…¥ï¼‰

    struct ggml_tensor ** nodes;   // æ“ä½œèŠ‚ç‚¹ï¼ˆæŒ‰æ‹“æ‰‘åºï¼‰
    struct ggml_tensor ** grads;   // æ¢¯åº¦ï¼ˆè®­ç»ƒç”¨ï¼‰
    struct ggml_tensor ** leafs;   // è¾“å…¥èŠ‚ç‚¹

    struct ggml_hash_set visited_hash_set;  // å·²è®¿é—®æ ‡è®°

    // æ‰§è¡Œé¡ºåº
    enum ggml_cgraph_eval_order order;

    // æ€§èƒ½ç»Ÿè®¡
    int perf_runs;
    int64_t perf_cycles;
    int64_t perf_time_us;
};

enum ggml_cgraph_eval_order {
    GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT = 0,
    GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT,
    GGML_CGRAPH_EVAL_ORDER_COUNT
};
```

### 2.2 å›¾çš„æ„å»ºæµç¨‹

```
ç”¨æˆ·ä»£ç ï¼š
    a = ggml_new_tensor(ctx)
    b = ggml_new_tensor(ctx)
    c = ggml_add(ctx, a, b)
    d = ggml_mul(ctx, c, 2)

    â†“ æ„å»ºå›¾

ggml_build_forward_expand(graph, d)

    â†“ ç”Ÿæˆçš„å›¾ç»“æ„

Leafs: [a, b]              # è¾“å…¥
Nodes: [c=add(a,b),        # æ“ä½œï¼ˆæ‹“æ‰‘åºï¼‰
        d=mul(c,2)]

    â†“ æ‰§è¡Œ

ggml_graph_compute(graph)
```

### 2.3 å›¾æ„å»ºä»£ç 

```c
// ä½ç½®ï¼šggml/src/ggml.c:18467
void ggml_build_forward_expand(
    struct ggml_cgraph * cgraph,
    struct ggml_tensor * tensor) {

    if (!tensor) {
        return;
    }

    // 1. æ£€æŸ¥æ˜¯å¦å·²è®¿é—®
    if (ggml_hash_contains(&cgraph->visited_hash_set, tensor)) {
        return;
    }

    // 2. æ ‡è®°ä¸ºå·²è®¿é—®
    ggml_hash_insert(&cgraph->visited_hash_set, tensor);

    // 3. é€’å½’å¤„ç†ä¾èµ–èŠ‚ç‚¹
    for (int i = 0; i < GGML_MAX_SRC; i++) {
        if (tensor->src[i]) {
            ggml_build_forward_expand(cgraph, tensor->src[i]);
        }
    }

    // 4. æ·»åŠ åˆ°å›¾ä¸­
    if (tensor->op == GGML_OP_NONE && tensor->grad == NULL) {
        // å¶å­èŠ‚ç‚¹ï¼ˆè¾“å…¥ï¼‰
        GGML_ASSERT(cgraph->n_leafs < cgraph->size);
        cgraph->leafs[cgraph->n_leafs++] = tensor;
    } else {
        // æ“ä½œèŠ‚ç‚¹
        GGML_ASSERT(cgraph->n_nodes < cgraph->size);
        cgraph->nodes[cgraph->n_nodes++] = tensor;
    }
}
```

## 3. llama_graph_builder å®ç°

### 3.1 Graph Builder ç»“æ„

```cpp
// ä½ç½®ï¼šsrc/llama-graph.cpp:146
struct llama_graph_builder {
    llama_context & lctx;
    const llama_model & model;
    const llama_ubatch & ubatch;

    // ç¼“å­˜çš„å¼ é‡
    struct ggml_tensor * inp_tokens;   // è¾“å…¥ token IDs
    struct ggml_tensor * inp_embd;     // è¾“å…¥ embeddings
    struct ggml_tensor * inp_pos;      // ä½ç½®ç´¢å¼•
    struct ggml_tensor * inp_KQ_mask;  // æ³¨æ„åŠ›æ©ç 
    struct ggml_tensor * inp_K_shift;  // KV ç¼“å­˜åç§»

    // ä¸­é—´ç»“æœ
    struct ggml_tensor * cur;          // å½“å‰æ¿€æ´»
    struct ggml_tensor * inpL;         // å±‚è¾“å…¥

    // KV ç¼“å­˜è§†å›¾
    std::vector<struct ggml_tensor *> kv_heads;
};
```

### 3.2 æ„å»º Transformer å±‚

```cpp
// ä½ç½®ï¼šsrc/llama-graph.cpp:823
struct ggml_tensor * llama_graph_builder::build_layer(
    int il,  // å±‚ç´¢å¼•
    struct ggml_tensor * cur,
    struct ggml_tensor * attn_mask) {

    const auto & model = lctx.model;
    const auto & hparams = model.hparams;
    const auto & layer = model.layers[il];

    struct ggml_context * ctx = lctx.ctx_compute.get();

    // === 1. æ³¨æ„åŠ›éƒ¨åˆ† ===

    // 1.1 LayerNorm
    struct ggml_tensor * attn_norm = ggml_rms_norm(ctx, cur, hparams.f_norm_rms_eps);
    attn_norm = ggml_mul(ctx, attn_norm, layer.attn_norm);

    // 1.2 Q, K, V æŠ•å½±
    struct ggml_tensor * Qcur = ggml_mul_mat(ctx, layer.wq, attn_norm);
    struct ggml_tensor * Kcur = ggml_mul_mat(ctx, layer.wk, attn_norm);
    struct ggml_tensor * Vcur = ggml_mul_mat(ctx, layer.wv, attn_norm);

    // 1.3 Reshape Q, K, V
    // Q: [n_embd, n_tokens] â†’ [n_head, n_embd_head, n_tokens]
    Qcur = ggml_reshape_3d(ctx, Qcur,
                           hparams.n_embd_head,
                           hparams.n_head,
                           ubatch.n_tokens);

    // 1.4 åº”ç”¨ RoPE
    Qcur = ggml_rope_ext(ctx, Qcur, inp_pos,
                         hparams.n_rot, hparams.rope_type,
                         hparams.rope_freq_base, hparams.rope_freq_scale);

    Kcur = ggml_rope_ext(ctx, Kcur, inp_pos,
                         hparams.n_rot, hparams.rope_type,
                         hparams.rope_freq_base, hparams.rope_freq_scale);

    // 1.5 å­˜å‚¨ K, V åˆ°ç¼“å­˜
    struct ggml_tensor * k_cache = kv_heads[il * 2 + 0];
    struct ggml_tensor * v_cache = kv_heads[il * 2 + 1];

    ggml_build_forward_expand(lctx.gf,
        ggml_cpy(ctx, Kcur, k_cache));
    ggml_build_forward_expand(lctx.gf,
        ggml_cpy(ctx, Vcur, v_cache));

    // 1.6 è®¡ç®—æ³¨æ„åŠ›
    // scores = Q @ K^T / sqrt(d_k)
    struct ggml_tensor * kq = ggml_mul_mat(ctx, k_cache, Qcur);
    kq = ggml_scale(ctx, kq, 1.0f / sqrtf(hparams.n_embd_head));

    // åº”ç”¨æ©ç 
    kq = ggml_add(ctx, kq, attn_mask);

    // Softmax
    kq = ggml_soft_max(ctx, kq);

    // attn_out = softmax(scores) @ V
    struct ggml_tensor * attn_out = ggml_mul_mat(ctx, v_cache, kq);

    // 1.7 Reshape å›åŸå§‹å½¢çŠ¶
    attn_out = ggml_reshape_2d(ctx, attn_out,
                               hparams.n_embd, ubatch.n_tokens);

    // 1.8 è¾“å‡ºæŠ•å½±
    attn_out = ggml_mul_mat(ctx, layer.wo, attn_out);

    // 1.9 æ®‹å·®è¿æ¥
    cur = ggml_add(ctx, cur, attn_out);

    // === 2. FFN éƒ¨åˆ† ===

    // 2.1 LayerNorm
    struct ggml_tensor * ffn_norm = ggml_rms_norm(ctx, cur, hparams.f_norm_rms_eps);
    ffn_norm = ggml_mul(ctx, ffn_norm, layer.ffn_norm);

    // 2.2 FFN (SwiGLU: gate * silu(up))
    struct ggml_tensor * ffn_gate = ggml_mul_mat(ctx, layer.ffn_gate, ffn_norm);
    struct ggml_tensor * ffn_up = ggml_mul_mat(ctx, layer.ffn_up, ffn_norm);

    ffn_gate = ggml_silu(ctx, ffn_gate);
    struct ggml_tensor * ffn_hidden = ggml_mul(ctx, ffn_gate, ffn_up);

    // 2.3 Down æŠ•å½±
    struct ggml_tensor * ffn_out = ggml_mul_mat(ctx, layer.ffn_down, ffn_hidden);

    // 2.4 æ®‹å·®è¿æ¥
    cur = ggml_add(ctx, cur, ffn_out);

    return cur;
}
```

### 3.3 æ„å»ºå®Œæ•´çš„æ¨¡å‹å›¾

```cpp
// ä½ç½®ï¼šsrc/llama-graph.cpp:1456
struct ggml_cgraph * llama_graph_builder::build_llama() {
    const auto & model = lctx.model;
    const auto & hparams = model.hparams;

    struct ggml_context * ctx = lctx.ctx_compute.get();

    // 1. åˆ›å»ºè¾“å…¥å¼ é‡
    inp_tokens = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, ubatch.n_tokens);
    ggml_set_name(inp_tokens, "inp_tokens");
    ggml_set_input(inp_tokens);

    inp_pos = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, ubatch.n_tokens);
    ggml_set_name(inp_pos, "inp_pos");
    ggml_set_input(inp_pos);

    // 2. Token åµŒå…¥
    inpL = ggml_get_rows(ctx, model.tok_embd, inp_tokens);

    // 3. æ„å»ºæ³¨æ„åŠ›æ©ç 
    struct ggml_tensor * KQ_mask = build_attention_mask(ubatch);

    // 4. é€å±‚æ„å»º
    for (int il = 0; il < hparams.n_layer; il++) {
        inpL = build_layer(il, inpL, KQ_mask);

        // å¯é€‰ï¼šä¸­é—´å±‚è¾“å‡º
        if (ubatch.output_layer[il]) {
            ggml_build_forward_expand(lctx.gf, inpL);
        }
    }

    // 5. æœ€ç»ˆ LayerNorm
    inpL = ggml_rms_norm(ctx, inpL, hparams.f_norm_rms_eps);
    inpL = ggml_mul(ctx, inpL, model.output_norm);

    // 6. è¾“å‡ºæŠ•å½±ï¼ˆlogitsï¼‰
    struct ggml_tensor * logits = ggml_mul_mat(ctx, model.output, inpL);
    ggml_set_name(logits, "logits");
    ggml_set_output(logits);

    // 7. æ„å»ºå›¾
    ggml_build_forward_expand(lctx.gf, logits);

    return lctx.gf;
}
```

## 4. åç«¯è°ƒåº¦å™¨

### 4.1 è°ƒåº¦å™¨ç»“æ„

```c
// ä½ç½®ï¼šggml/src/ggml-backend.cpp:1623
struct ggml_backend_sched {
    int n_backends;
    ggml_backend_t * backends;           // åç«¯åˆ—è¡¨

    // å¼ é‡ â†’ åç«¯æ˜ å°„
    struct ggml_hash_set hash_set;
    ggml_backend_t * tensor_backend;     // å¼ é‡æ‰€åœ¨åç«¯

    // åˆ†å‰²ä¿¡æ¯
    struct ggml_backend_sched_split * splits;
    int n_splits;
    int splits_capacity;

    // åŒæ­¥äº‹ä»¶
    struct ggml_backend_event * events;

    // å›¾å‰¯æœ¬ï¼ˆæ¯ä¸ªåç«¯ä¸€ä»½ï¼‰
    struct ggml_cgraph ** graph_copies;
};
```

### 4.2 è°ƒåº¦æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. åˆ†æå›¾ï¼Œç¡®å®šæ¯ä¸ªèŠ‚ç‚¹çš„åç«¯           â”‚
â”‚    tensor_backend[t] = best_backend(t)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. å°†å›¾åˆ†å‰²æˆå¤šä¸ª split                 â”‚
â”‚    æ¯ä¸ª split åœ¨å•ä¸ªåç«¯ä¸Šæ‰§è¡Œ          â”‚
â”‚                                         â”‚
â”‚    Split 0 (CPU):  [op0, op1]          â”‚
â”‚    Split 1 (GPU0): [op2, op3, op4]     â”‚
â”‚    Split 2 (CPU):  [op5]               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. æ’å…¥æ•°æ®ä¼ è¾“æ“ä½œ                     â”‚
â”‚    CPU â†’ GPU: copy_tensor(op1_out)     â”‚
â”‚    GPU â†’ CPU: copy_tensor(op4_out)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. é¡ºåºæ‰§è¡Œæ¯ä¸ª split                   â”‚
â”‚    for each split:                     â”‚
â”‚      backend.compute(split.ops)        â”‚
â”‚      sync_if_needed()                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 è°ƒåº¦å™¨å®ç°

```c
// ä½ç½®ï¼šggml/src/ggml-backend.cpp:2156
enum ggml_status ggml_backend_sched_graph_compute(
    struct ggml_backend_sched * sched,
    struct ggml_cgraph * graph) {

    // 1. åˆ†é…å¼ é‡åˆ°åç«¯
    for (int i = 0; i < graph->n_nodes; i++) {
        struct ggml_tensor * node = graph->nodes[i];

        // é€‰æ‹©æœ€ä½³åç«¯
        ggml_backend_t backend = ggml_backend_sched_backend_from_cur(sched, node);

        // å¦‚æœå¼ é‡è¿˜æ²¡æœ‰åç«¯ï¼Œåˆ†é…ä¸€ä¸ª
        if (node->backend == NULL || node->backend != backend) {
            ggml_backend_sched_alloc_tensor(sched, backend, node);
        }
    }

    // 2. åˆ†å‰²å›¾
    ggml_backend_sched_split_graph(sched, graph);

    // 3. æ‰§è¡Œæ¯ä¸ª split
    for (int i = 0; i < sched->n_splits; i++) {
        struct ggml_backend_sched_split * split = &sched->splits[i];
        ggml_backend_t backend = split->backend;

        // 3.1 æ‹·è´è¾“å…¥ï¼ˆå¦‚æœéœ€è¦è·¨è®¾å¤‡ä¼ è¾“ï¼‰
        for (int j = 0; j < split->n_inputs; j++) {
            struct ggml_tensor * input = split->inputs[j];
            struct ggml_tensor * input_cpy = split->inputs_cpy[j];

            if (input_cpy != NULL) {
                // è·¨è®¾å¤‡æ‹·è´
                ggml_backend_tensor_copy(input, input_cpy);
            }
        }

        // 3.2 æ‰§è¡Œè®¡ç®—
        enum ggml_status status = ggml_backend_graph_compute(
            backend, split->graph);

        if (status != GGML_STATUS_SUCCESS) {
            return status;
        }

        // 3.3 åŒæ­¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if (split->sync_needed) {
            ggml_backend_synchronize(backend);
        }
    }

    return GGML_STATUS_SUCCESS;
}
```

### 4.4 åç«¯é€‰æ‹©ç­–ç•¥

```c
// ä½ç½®ï¼šggml/src/ggml-backend.cpp:1923
static ggml_backend_t ggml_backend_sched_backend_from_cur(
    struct ggml_backend_sched * sched,
    struct ggml_tensor * tensor) {

    // 1. æ£€æŸ¥æ˜¯å¦å·²åˆ†é…
    ggml_backend_t cur_backend = ggml_get_backend(sched, tensor);
    if (cur_backend != NULL) {
        return cur_backend;
    }

    // 2. æ ¹æ®æ“ä½œç±»å‹é€‰æ‹©
    if (tensor->op == GGML_OP_NONE) {
        // è¾“å…¥å¼ é‡ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªåç«¯
        return sched->backends[0];
    }

    // 3. æ£€æŸ¥æ˜¯å¦æœ‰åç«¯æ”¯æŒæ­¤æ“ä½œ
    for (int i = 0; i < sched->n_backends; i++) {
        ggml_backend_t backend = sched->backends[i];

        if (ggml_backend_supports_op(backend, tensor)) {
            // ä¼˜å…ˆé€‰æ‹© GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if (ggml_backend_is_gpu(backend)) {
                return backend;
            }
        }
    }

    // 4. é»˜è®¤ä½¿ç”¨ CPU
    return sched->backends[0];
}
```

## 5. å›¾ä¼˜åŒ–æŠ€æœ¯

### 5.1 æ“ä½œèåˆ

å°†å¤šä¸ªæ“ä½œèåˆä¸ºä¸€ä¸ªï¼Œå‡å°‘å†…å­˜è®¿é—®ï¼š

```c
// ä¼˜åŒ–å‰ï¼š3 ä¸ªç‹¬ç«‹æ“ä½œ
y = ggml_add(ctx, x, bias);      // å†™ y
y = ggml_mul(ctx, y, scale);     // è¯» yï¼Œå†™ y
y = ggml_relu(ctx, y);           // è¯» yï¼Œå†™ y

// ä¼˜åŒ–åï¼š1 ä¸ªèåˆæ“ä½œ
y = ggml_add_mul_relu(ctx, x, bias, scale);  // ç›´æ¥è®¡ç®—
```

**å®ç°**ï¼ˆä½ç½®ï¼š`ggml/src/ggml.c:17234`ï¼‰ï¼š

```c
static void ggml_graph_optimize(struct ggml_cgraph * graph) {
    for (int i = 0; i < graph->n_nodes - 2; i++) {
        struct ggml_tensor * node0 = graph->nodes[i];
        struct ggml_tensor * node1 = graph->nodes[i + 1];
        struct ggml_tensor * node2 = graph->nodes[i + 2];

        // æ£€æµ‹æ¨¡å¼ï¼šAdd â†’ Mul â†’ ReLU
        if (node0->op == GGML_OP_ADD &&
            node1->op == GGML_OP_MUL &&
            node2->op == GGML_OP_RELU &&
            node1->src[0] == node0 &&
            node2->src[0] == node1) {

            // æ›¿æ¢ä¸ºèåˆæ“ä½œ
            node2->op = GGML_OP_ADD_MUL_RELU_FUSED;
            node2->src[0] = node0->src[0];  // x
            node2->src[1] = node0->src[1];  // bias
            node2->src[2] = node1->src[1];  // scale

            // æ ‡è®°ä¸­é—´èŠ‚ç‚¹ä¸ºæ— æ•ˆ
            node0->op = GGML_OP_NONE;
            node1->op = GGML_OP_NONE;
        }
    }
}
```

### 5.2 å†…å­˜å¤ç”¨

é‡ç”¨ä¸­é—´ç»“æœçš„å†…å­˜ï¼š

```c
// åˆ†ææ¯ä¸ªå¼ é‡çš„ç”Ÿå‘½å‘¨æœŸ
for (int i = 0; i < graph->n_nodes; i++) {
    struct ggml_tensor * node = graph->nodes[i];

    // æœ€åä½¿ç”¨ä½ç½®
    int last_use = find_last_use(graph, node);

    if (last_use < graph->n_nodes - 1) {
        // å¯ä»¥å¤ç”¨æ­¤å¼ é‡çš„å†…å­˜
        mark_for_reuse(node, last_use);
    }
}
```

### 5.3 å¸¸é‡æŠ˜å 

é¢„è®¡ç®—å¸¸é‡è¡¨è¾¾å¼ï¼š

```c
// ä¼˜åŒ–å‰
struct ggml_tensor * scale = ggml_new_f32(ctx, 0.5f);
struct ggml_tensor * two = ggml_new_f32(ctx, 2.0f);
struct ggml_tensor * result = ggml_mul(ctx, scale, two);  // 0.5 * 2.0

// ä¼˜åŒ–åï¼ˆç¼–è¯‘æ—¶è®¡ç®—ï¼‰
struct ggml_tensor * result = ggml_new_f32(ctx, 1.0f);
```

## 6. å®æˆ˜ï¼šæ„å»ºè‡ªå®šä¹‰è®¡ç®—å›¾

### 6.1 ç®€å•çš„ MLP

```cpp
struct ggml_cgraph * build_mlp_graph(
    struct ggml_context * ctx,
    struct ggml_tensor * input,    // [batch, input_dim]
    struct ggml_tensor * w1,       // [input_dim, hidden_dim]
    struct ggml_tensor * b1,       // [hidden_dim]
    struct ggml_tensor * w2,       // [hidden_dim, output_dim]
    struct ggml_tensor * b2) {     // [output_dim]

    // å±‚ 1: hidden = relu(input @ w1 + b1)
    struct ggml_tensor * hidden = ggml_mul_mat(ctx, w1, input);
    hidden = ggml_add(ctx, hidden, b1);
    hidden = ggml_relu(ctx, hidden);

    // å±‚ 2: output = hidden @ w2 + b2
    struct ggml_tensor * output = ggml_mul_mat(ctx, w2, hidden);
    output = ggml_add(ctx, output, b2);

    // æ„å»ºå›¾
    struct ggml_cgraph * gf = ggml_new_graph(ctx);
    ggml_build_forward_expand(gf, output);

    return gf;
}
```

### 6.2 å¸¦æ®‹å·®è¿æ¥çš„å—

```cpp
struct ggml_tensor * build_residual_block(
    struct ggml_context * ctx,
    struct ggml_tensor * input,
    struct ggml_tensor * w1,
    struct ggml_tensor * w2) {

    // ä¸»è·¯å¾„
    struct ggml_tensor * x = ggml_mul_mat(ctx, w1, input);
    x = ggml_relu(ctx, x);
    x = ggml_mul_mat(ctx, w2, x);

    // æ®‹å·®è¿æ¥
    x = ggml_add(ctx, x, input);

    return x;
}
```

### 6.3 å¤šå¤´æ³¨æ„åŠ›

```cpp
struct ggml_tensor * build_multi_head_attention(
    struct ggml_context * ctx,
    struct ggml_tensor * input,      // [n_tokens, n_embd]
    struct ggml_tensor * wq,         // [n_embd, n_embd]
    struct ggml_tensor * wk,
    struct ggml_tensor * wv,
    struct ggml_tensor * wo,
    int n_head,
    int n_embd_head) {

    // Q, K, V æŠ•å½±
    struct ggml_tensor * Q = ggml_mul_mat(ctx, wq, input);
    struct ggml_tensor * K = ggml_mul_mat(ctx, wk, input);
    struct ggml_tensor * V = ggml_mul_mat(ctx, wv, input);

    // Reshape ä¸ºå¤šå¤´
    Q = ggml_reshape_3d(ctx, Q, n_embd_head, n_head, -1);
    K = ggml_reshape_3d(ctx, K, n_embd_head, n_head, -1);
    V = ggml_reshape_3d(ctx, V, n_embd_head, n_head, -1);

    // ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
    struct ggml_tensor * KQ = ggml_mul_mat(ctx, K, Q);
    KQ = ggml_scale(ctx, KQ, 1.0f / sqrtf(n_embd_head));
    KQ = ggml_soft_max(ctx, KQ);

    struct ggml_tensor * KQV = ggml_mul_mat(ctx, V, KQ);

    // Reshape å›åŸå§‹å½¢çŠ¶
    KQV = ggml_reshape_2d(ctx, KQV, n_head * n_embd_head, -1);

    // è¾“å‡ºæŠ•å½±
    struct ggml_tensor * output = ggml_mul_mat(ctx, wo, KQV);

    return output;
}
```

## 7. è°ƒè¯•å·¥å…·

### 7.1 å›¾å¯è§†åŒ–

```cpp
void dump_graph_to_dot(struct ggml_cgraph * graph, const char * filename) {
    FILE * fp = fopen(filename, "w");
    fprintf(fp, "digraph G {\n");
    fprintf(fp, "  rankdir=TB;\n");

    // è¾“å‡ºèŠ‚ç‚¹
    for (int i = 0; i < graph->n_nodes; i++) {
        struct ggml_tensor * node = graph->nodes[i];

        fprintf(fp, "  node%d [label=\"%s\\n%s\\n[%lld,%lld,%lld,%lld]\"];\n",
                i, node->name,
                ggml_op_name(node->op),
                node->ne[0], node->ne[1], node->ne[2], node->ne[3]);

        // è¾“å‡ºè¾¹
        for (int j = 0; j < GGML_MAX_SRC; j++) {
            if (node->src[j]) {
                int src_idx = find_node_index(graph, node->src[j]);
                fprintf(fp, "  node%d -> node%d;\n", src_idx, i);
            }
        }
    }

    fprintf(fp, "}\n");
    fclose(fp);

    printf("Graph dumped to %s\n", filename);
    printf("View with: dot -Tpng %s -o graph.png\n", filename);
}
```

### 7.2 æ€§èƒ½åˆ†æ

```cpp
void profile_graph_execution(struct ggml_cgraph * graph) {
    printf("Graph profiling:\n");
    printf("%-40s %12s %12s\n", "Operation", "Time (ms)", "Percentage");
    printf("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");

    int64_t total_time = graph->perf_time_us;

    for (int i = 0; i < graph->n_nodes; i++) {
        struct ggml_tensor * node = graph->nodes[i];

        float time_ms = node->perf_time_us / 1000.0f;
        float percentage = 100.0f * node->perf_time_us / total_time;

        printf("%-40s %12.3f %11.1f%%\n",
               node->name, time_ms, percentage);
    }

    printf("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n");
    printf("%-40s %12.3f\n", "Total", total_time / 1000.0f);
}
```

## 8. æ€»ç»“

ä»Šå¤©æˆ‘ä»¬æ·±å…¥å­¦ä¹ äº†è®¡ç®—å›¾çš„æ„å»ºä¸è°ƒåº¦ï¼š

âœ… **å›¾æ„å»º**ï¼šæ‹“æ‰‘æ’åºã€ä¾èµ–è¿½è¸ª
âœ… **llama_graph_builder**ï¼šæ„å»ºå®Œæ•´çš„ Transformer å›¾
âœ… **åç«¯è°ƒåº¦**ï¼šå¤šåç«¯ååŒã€è‡ªåŠ¨åˆ†å‰²
âœ… **å›¾ä¼˜åŒ–**ï¼šæ“ä½œèåˆã€å†…å­˜å¤ç”¨ã€å¸¸é‡æŠ˜å 
âœ… **è°ƒè¯•å·¥å…·**ï¼šå¯è§†åŒ–ã€æ€§èƒ½åˆ†æ

### å…³é”®è¦ç‚¹

1. **å»¶è¿Ÿæ‰§è¡Œ**ï¼šæ„å»ºæ—¶ä¸è®¡ç®—ï¼Œæ‰§è¡Œæ—¶ç»Ÿä¸€ä¼˜åŒ–
2. **è‡ªåŠ¨è°ƒåº¦**ï¼šè°ƒåº¦å™¨æ™ºèƒ½åˆ†é…æ“ä½œåˆ°ä¸åŒåç«¯
3. **å›¾ä¼˜åŒ–**ï¼šå…¨å±€è§†è§’ä¼˜åŒ–æ€§èƒ½
4. **å¯æ‰©å±•**ï¼šæ˜“äºæ·»åŠ æ–°æ“ä½œå’Œåç«¯

## ä¸‹ä¸€æ­¥

æ˜å¤©æˆ‘ä»¬å°†å­¦ä¹  **Day 8: Transformer æ¶æ„å®ç°**ï¼š
- LLaMA æ¶æ„è¯¦è§£
- æ³¨æ„åŠ›å±‚å®ç°
- FFN å±‚å®ç°
- ä¸åŒæ¶æ„çš„å˜ä½“

---

**ç»ƒä¹ **ï¼š
1. æ„å»ºä¸€ä¸ªç®€å•çš„ 3 å±‚ MLP å›¾
2. åˆ†æ llama-7b çš„è®¡ç®—å›¾ç»“æ„
3. ä½¿ç”¨ DOT å¯è§†åŒ–ä½ çš„è‡ªå®šä¹‰å›¾

ğŸ“š [Day 8: Transformer æ¶æ„å®ç°](day08-transformer-impl.md)
